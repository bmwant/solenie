<?xml version="1.0" encoding="ascii" ?>

<script language="javascript" type="text/javascript">

function astext(node)
{
    return node.innerHTML.replace(/(<([^>]+)>)/ig,"")
                         .replace(/&gt;/ig, ">")
                         .replace(/&lt;/ig, "<")
                         .replace(/&quot;/ig, '"')
                         .replace(/&amp;/ig, "&");
}

function copy_notify(node, bar_color, data)
{
    // The outer box: relative + inline positioning.
    var box1 = document.createElement("div");
    box1.style.position = "relative";
    box1.style.display = "inline";
    box1.style.top = "2em";
    box1.style.left = "1em";
  
    // A shadow for fun
    var shadow = document.createElement("div");
    shadow.style.position = "absolute";
    shadow.style.left = "-1.3em";
    shadow.style.top = "-1.3em";
    shadow.style.background = "#404040";
    
    // The inner box: absolute positioning.
    var box2 = document.createElement("div");
    box2.style.position = "relative";
    box2.style.border = "1px solid #a0a0a0";
    box2.style.left = "-.2em";
    box2.style.top = "-.2em";
    box2.style.background = "white";
    box2.style.padding = ".3em .4em .3em .4em";
    box2.style.fontStyle = "normal";
    box2.style.background = "#f0e0e0";

    node.insertBefore(box1, node.childNodes.item(0));
    box1.appendChild(shadow);
    shadow.appendChild(box2);
    box2.innerHTML="Copied&nbsp;to&nbsp;the&nbsp;clipboard: " +
                   "<pre class='copy-notify'>"+
                   data+"</pre>";
    setTimeout(function() { node.removeChild(box1); }, 1000);

    var elt = node.parentNode.firstChild;
    elt.style.background = "#ffc0c0";
    setTimeout(function() { elt.style.background = bar_color; }, 200);
}

function copy_codeblock_to_clipboard(node)
{
    var data = astext(node)+"\n";
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#40a060", data);
    }
}

function copy_doctest_to_clipboard(node)
{
    var s = astext(node)+"\n   ";
    var data = "";

    var start = 0;
    var end = s.indexOf("\n");
    while (end >= 0) {
        if (s.substring(start, start+4) == ">>> ") {
            data += s.substring(start+4, end+1);
        }
        else if (s.substring(start, start+4) == "... ") {
            data += s.substring(start+4, end+1);
        }
        /*
        else if (end-start > 1) {
            data += "# " + s.substring(start, end+1);
        }*/
        // Grab the next line.
        start = end+1;
        end = s.indexOf("\n", start);
    }
    
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#4060a0", data);
    }
}
    
function copy_text_to_clipboard(data)
{
    if (window.clipboardData) {
        window.clipboardData.setData("Text", data);
        return true;
     }
    else if (window.netscape) {
        // w/ default firefox settings, permission will be denied for this:
        netscape.security.PrivilegeManager
                      .enablePrivilege("UniversalXPConnect");
    
        var clip = Components.classes["@mozilla.org/widget/clipboard;1"]
                      .createInstance(Components.interfaces.nsIClipboard);
        if (!clip) return;
    
        var trans = Components.classes["@mozilla.org/widget/transferable;1"]
                       .createInstance(Components.interfaces.nsITransferable);
        if (!trans) return;
    
        trans.addDataFlavor("text/unicode");
    
        var str = new Object();
        var len = new Object();
    
        var str = Components.classes["@mozilla.org/supports-string;1"]
                     .createInstance(Components.interfaces.nsISupportsString);
        var datacopy=data;
        str.data=datacopy;
        trans.setTransferData("text/unicode",str,datacopy.length*2);
        var clipid=Components.interfaces.nsIClipboard;
    
        if (!clip) return false;
    
        clip.setData(trans,null,clipid.kGlobalClipboard);
        return true;
    }
    return false;
}
//-->
</script>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ascii" />
<meta name="generator" content="Docutils 0.14: http://docutils.sourceforge.net/" />
<title>6. Learning to Classify Text</title>
<--- Cannot embed stylesheet '../nltkdoc.css': No such file or directory. --->
</head>
<body>
<div class="document" id="learning-to-classify-text">
<h1 class="title">6. Learning to Classify Text</h1>

<!-- -*- mode: rst -*- -->
<!-- -*- mode: rst -*- -->
<!-- CAP abbreviations (map to small caps in LaTeX) -->
<!-- Other candidates for global consistency -->
<!-- PTB removed since it must be indexed -->
<!-- WN removed since it must be indexed -->
<!-- misc & punctuation -->
<!-- cdots was unicode U+22EF but not working -->
<!-- exercise meta-tags -->
<!-- Unicode tests -->
<!-- phonetic -->
<!-- misc -->
<!-- used in Unicode section -->
<!-- arrows -->
<!-- unification stuff -->
<!-- Math & Logic -->
<!-- sets -->
<!-- Greek -->
<!-- Chinese -->
<!-- URLs -->
<!-- Python example - a snippet of code in running text -->
<!-- PlaceHolder example -  something that should be replaced by actual code -->
<!-- Linguistic eXample - cited form in running text -->
<!-- Emphasized (more declarative than just using *) -->
<!-- Grammatical Category - e.g. NP and verb as technical terms
.. role:: gc
   :class: category -->
<!-- Math expression - e.g. especially for variables -->
<!-- Textual Math expression - for words 'inside' a math environment -->
<!-- Feature (or attribute) -->
<!-- Raw LaTeX -->
<!-- Raw HTML -->
<!-- Feature-value -->
<!-- Lexemes -->
<!-- Replacements that rely on previous definitions :-) -->
<!-- standard global imports

>>> from __future__ import division
>>> import nltk, re, pprint -->
<!-- Organization:
- Supervised Classification - - this introduces basic concepts, and
  runs through lots of interesting examples
- Evaluation - - talks about evaluation :)
- Classification Methods (x3)
- Generative vs Conditional
- Joint Classification
- Data modeling - - this talks about abstractly what we can learn
  about language as linguists
- ML in Python -->
<span class="target" id="chap-data-intensive"></span><!-- XXX "test set" or "evaluation set"- - and what about "gold standard"? -->
<!-- XXX globally, "<word>"(ie word enclosed by double quotes) needs to be replaced by -->
<!-- `<word>`:lx: -->
<!-- nomenclature note: training corpus/test corpus or training set/test set?? -->
<!-- discuss qc corpus -->
<!-- explain that segmentation (e.g. tokenization, sentence segmentation) can
be viewed as a classification task -->
<!-- Determinize, for the sake of doctest:

>>> import random; random.seed(12345) -->
<p>Detecting patterns is a central part of Natural Language Processing.  Words ending in
<span class="example">-ed</span> tend to be past tense verbs (<a href="#id19"><span class="problematic" id="id20">chap-tag_</span></a>).  Frequent use of
<span class="example">will</span> is indicative of news text (<a href="#id21"><span class="problematic" id="id22">chap-words_</span></a>).  These observable
patterns &#8212; word structure and word frequency &#8212; happen to
correlate with particular aspects of meaning, such as tense and topic.
But how did we know where to start looking, which aspects of form to
associate with which aspects of meaning?</p>
<p>The goal of this chapter is to answer the following questions:</p>
<ol class="arabic simple">
<li>How can we identify particular features of language data that
are salient for classifying it?</li>
<li>How can we construct models of language that can
be used to perform language processing tasks automatically?</li>
<li>What can we learn about language from these models?</li>
</ol>
<p>Along the way we will study some important machine learning
techniques, including decision trees, naive Bayes' classifiers,
and maximum entropy classifiers.  We will gloss over the mathematical and
statistical underpinnings of these techniques, focusing instead on how
and when to use them (see the Further Readings section for more
technical background).  Before looking at these methods, we first need
to appreciate the broad scope of this topic.</p>
<div class="section" id="supervised-classification">
<span id="sec-supervised-classification"></span><h1>1&nbsp;&nbsp;&nbsp;Supervised Classification</h1>
<p><a name="classification_index_term" /><span class="termdef">Classification</span> is the task of choosing the correct <a name="class_label_index_term" /><span class="termdef">class
label</span> for a given input.  In basic classification tasks, each
input is considered in isolation from all other inputs, and the set of
labels is defined in advance.  Some examples of classification tasks
are:</p>
<ul class="simple">
<li>Deciding whether an email is spam or not.</li>
<li>Deciding what the topic of a news article is, from a fixed list of
topic areas such as &quot;sports,&quot; &quot;technology,&quot; and &quot;politics.&quot;</li>
<li>Deciding whether a given occurrence of the word <span class="example">bank</span> is used to
refer to a river bank, a financial institution, the act of tilting
to the side, or the act of depositing something in a financial
institution.</li>
</ul>
<p>The basic classification task has a number of interesting variants.
For example, in multi-class classification, each instance may be
assigned multiple labels; in open-class classification, the set of
labels is not defined in advance; and in sequence classification, a
list of inputs are jointly classified.</p>
<p>A classifier is called <a name="supervised_index_term" /><span class="termdef">supervised</span> if it is built based on
training corpora containing the correct label for each input.  The
framework used by supervised classification is shown in
<a class="reference internal" href="#fig-supervised-classification">fig-supervised-classification</a>.</p>
<div class="system-message" id="fig-supervised-classification">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 97)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '50:150:60')
invalid literal for int() with base 10: '50:150:60'.</p>
<pre class="literal-block">
.. figure:: ../images/supervised-classification.png
   :scale: 50:150:60

   Supervised Classification.  (a) During training, a feature
   extractor is used to convert each input value to a feature set.
   These feature sets, which capture the basic information about
   each input that should be used to classify it, are discussed in
   the next section.
   Pairs of feature sets and labels are fed into the machine learning
   algorithm to generate a model.  (b) During prediction, the same
   feature extractor is used to convert unseen inputs to feature sets.
   These feature sets are then fed into the model, which generates
   predicted labels.

</pre>
</div>
<p>In the rest of this section, we will look at how classifiers can be
employed to solve a wide variety of tasks.  Our discussion is not intended
to be comprehensive, but to give a representative sample of tasks that
can be performed with the help of text classifiers.</p>
<div class="section" id="gender-identification">
<h2>1.1&nbsp;&nbsp;&nbsp;Gender Identification</h2>
<p>In <a href="#id23"><span class="problematic" id="id24">sec-lexical-resources_</span></a> we saw that male and female names
have some distinctive characteristics.  Names ending in <span class="example">a</span>,
<span class="example">e</span> and <span class="example">i</span> are likely to be female, while names ending in
<span class="example">k</span>, <span class="example">o</span>, <span class="example">r</span>, <span class="example">s</span> and <span class="example">t</span> are likely to be male.
Let's build a classifier to model these differences more precisely.</p>
<p>The first step in creating a classifier is deciding what
<a name="features_index_term" /><span class="termdef">features</span> of the input are relevant, and how to <a name="encode_index_term" /><span class="termdef">encode</span>
those features.  For this example, we'll start by just looking at the
final letter of a given name.  The following <a name="feature_extractor_index_term" /><span class="termdef">feature extractor</span>
function builds a dictionary containing relevant information about a
given name:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def gender_features(word):
...     return {'last_letter': word[-1]}
>>> gender_features('Shrek')
{'last_letter': 'k'}</td>
</tr></table></td></tr>
</table></div>
<p>The returned dictionary, known as a <a name="feature_set_index_term" /><span class="termdef">feature set</span>, maps from
feature names to their values.  Feature names are case-sensitive
strings that typically provide a short human-readable description of
the feature, as in the example <tt class="doctest"><span class="pre">'last_letter'</span></tt>.  Feature values are values with simple types, such as
booleans, numbers, and strings.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Most classification methods require that features be encoded using
simple value types, such as booleans, numbers, and strings.  But
note that just because a feature has a simple type, this does not
necessarily mean that the feature's value is simple to express or
compute. Indeed, it is even possible to use very complex and
informative values, such as the output of a second supervised
classifier, as features.</p>
</div>
<p>Now that we've defined a feature extractor, we need to prepare
a list of examples and corresponding
class labels.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import names
>>> labeled_names = ([(name, 'male') for name in names.words('male.txt')] +
... [(name, 'female') for name in names.words('female.txt')])
>>> import random
>>> random.shuffle(labeled_names)</td>
</tr></table></td></tr>
</table></div>
<!-- XXX say something about what featuresets are -->
<p>Next, we use the feature extractor to process the <tt class="doctest"><span class="pre">names</span></tt> data, and
divide the resulting list of feature sets into a <a name="training_set_index_term" /><span class="termdef">training set</span>
and a <a name="test_set_index_term" /><span class="termdef">test set</span>.  The training set is used to train a new
&quot;naive Bayes&quot; classifier.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]
>>> train_set, test_set = featuresets[500:], featuresets[:500]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)</td>
</tr></table></td></tr>
</table></div>
<p>We will learn more about the naive Bayes classifier later in the
chapter.  For now, let's just test it out on some names that did not
appear in its training data:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> classifier.classify(gender_features('Neo'))
'male'
>>> classifier.classify(gender_features('Trinity'))
'female'</td>
</tr></table></td></tr>
</table></div>
<p>Observe that these character names from <em>The Matrix</em> are correctly
classified.  Although this science fiction movie is set in 2199, it
still conforms with our expectations about names and genders.  We can
systematically evaluate the classifier on a much larger quantity of
unseen data:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(nltk.classify.accuracy(classifier, test_set))
0.77</td>
</tr></table></td></tr>
</table></div>
<p>Finally, we can examine the classifier to determine which features it
found most effective for distinguishing the names' genders:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> classifier.show_most_informative_features(5)
Most Informative Features
             last_letter = 'a'            female : male   =     33.2 : 1.0
             last_letter = 'k'              male : female =     32.6 : 1.0
             last_letter = 'p'              male : female =     19.7 : 1.0
             last_letter = 'v'              male : female =     18.6 : 1.0
             last_letter = 'f'              male : female =     17.3 : 1.0</td>
</tr></table></td></tr>
</table></div>
<p>This listing shows that the names in the training set that end in &quot;a&quot;
are female 33 times more often than they are male, but names that end
in &quot;k&quot; are male 32 times more often than they are female.  These
ratios are known as <a name="likelihood_ratios_index_term" /><span class="termdef">likelihood ratios</span>, and can be useful for
comparing different feature-outcome relationships.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Modify the <tt class="doctest"><span class="pre">gender_features()</span></tt> function to provide the
classifier with features encoding the length of the name, its first
letter, and any other features that seem like they might be
informative.  Retrain the classifier with these new features, and
test its accuracy.</p>
</div>
<p>When working with large corpora, constructing a single list
that contains the features of every instance can use up a large
amount of memory.  In these cases, use the function
<tt class="doctest"><span class="pre">nltk.classify.apply_features</span></tt>, which returns an object that acts
like a list but does not store all the feature sets in memory:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.classify import apply_features
>>> train_set = apply_features(gender_features, labeled_names[500:])
>>> test_set = apply_features(gender_features, labeled_names[:500])</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="choosing-the-right-features">
<h2>1.2&nbsp;&nbsp;&nbsp;Choosing The Right Features</h2>
<p>Selecting relevant features and deciding how to encode them for a
learning method can have an enormous impact on the learning method's
ability to extract a good model.  Much of the interesting work in
building a classifier is deciding what features might be relevant, and
how we can represent them.  Although it's often possible to get decent
performance by using a fairly simple and obvious set of features,
there are usually significant gains to be had by using carefully
constructed features based on a thorough understanding of the task at
hand.</p>
<p>Typically, feature extractors are built through a process of
trial-and-error, guided by intuitions about what information is
relevant to the problem.  It's common to start with a
&quot;kitchen sink&quot; approach, including all the features that you can think
of, and then checking to see which features actually are
helpful.  We take this approach for name gender features in
<a class="reference internal" href="#code-gender-features-overfitting">1.1</a>.</p>
<!-- XXX is it worth telling readers not to confuse feature names like "count(j)"
with functions? -->
<!-- XXX how about mentioning the term "presence feature" as the name
for boolean-valued features? -->
<!-- XXX briefly explain what's being done in
features["count(%s)" % letter] = name.lower().count(letter) -->
<span class="target" id="code-gender-features-overfitting"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def gender_features2(name):
    features = {}
    features["first_letter"] = name[0].lower()
    features["last_letter"] = name[-1].lower()
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        features["count({})".format(letter)] = name.lower().count(letter)
        features["has({})".format(letter)] = (letter in name.lower())
    return features</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> gender_features2('John') 
{'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">A Feature Extractor that Overfits Gender Features.
The feature sets returned by this feature extractor contain a
large number of specific features, leading to overfitting for
the relatively small Names Corpus.</td></tr></p>
</table></div>
<p>However, there are usually limits to the number of features
that you should use with a given learning algorithm &#8212; if you provide
too many features, then the algorithm will have a higher chance of
relying on idiosyncrasies of your training data that don't generalize
well to new examples.  This problem is known as <a name="overfitting_index_term" /><span class="termdef">overfitting</span>, and
can be especially problematic when working with small training sets.  For
example, if we train a naive Bayes classifier using the feature
extractor shown in <a class="reference internal" href="#code-gender-features-overfitting">1.1</a>, it will overfit
the relatively small training set, resulting in a system whose accuracy
is about 1% lower than the accuracy of a classifier that only
pays attention to the final letter of each name:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]
>>> train_set, test_set = featuresets[500:], featuresets[:500]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> print(nltk.classify.accuracy(classifier, test_set))
0.768</td>
</tr></table></td></tr>
</table></div>
<!-- XXX How common is it to call the training + dev-test data a 'development set'? -->
<!-- I've not come across it. In -->
<!-- any case, the extra label seems unnecessary. J&M seem to be more standard; they -->
<!-- call partition into "training", "dev-test" and "test" (p187) -->
<p>Once an initial set of features has been chosen, a very productive
method for refining the feature set is <a name="error_analysis_index_term" /><span class="termdef">error analysis</span>.  First,
we select a <a name="development_set_index_term" /><span class="termdef">development set</span>, containing the corpus data for
creating the model.  This development set is then subdivided
into the <a name="training_set_index_term_2" /><span class="termdef">training set</span> and the <a name="dev_test_index_term" /><span class="termdef">dev-test</span> set.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> train_names = labeled_names[1500:]
>>> devtest_names = labeled_names[500:1500]
>>> test_names = labeled_names[:500]</td>
</tr></table></td></tr>
</table></div>
<p>The training set is used to train the model, and the dev-test set is
used to perform error analysis.  The test set serves in our final
evaluation of the system.  For reasons discussed below, it is
important that we employ a separate dev-test set for error analysis,
rather than just using the test set.  The division of the corpus data
into different subsets is shown in <a class="reference internal" href="#fig-corpus-org">fig-corpus-org</a>.</p>
<div class="system-message" id="fig-corpus-org">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 318)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '50:150:75')
invalid literal for int() with base 10: '50:150:75'.</p>
<pre class="literal-block">
.. figure:: ../images/corpus-org.png
   :scale: 50:150:75

   Organization of corpus data for training supervised classifiers.
   The corpus data is divided into two sets: the development set,
   and the test set.  The development set is often further subdivided
   into a training set and a dev-test set.

</pre>
</div>
<p>Having divided the corpus into appropriate datasets, we train a model
using the training set <a class="reference internal" href="#err-analysis-train"><span id="ref-err-analysis-train"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>, and then run it on the
dev-test set <a class="reference internal" href="#err-analysis-run"><span id="ref-err-analysis-run"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> train_set = [(gender_features(n), gender) for (n, gender) in train_names]
>>> devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]
>>> test_set = [(gender_features(n), gender) for (n, gender) in test_names]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set) # [_err-analysis-train]
>>> print(nltk.classify.accuracy(classifier, devtest_set)) # [_err-analysis-run]
0.75</td>
</tr></table></td></tr>
</table></div>
<p>Using the dev-test set, we can generate a list of the errors that the
classifier makes when predicting name genders:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> errors = []
>>> for (name, tag) in devtest_names:
...     guess = classifier.classify(gender_features(name))
...     if guess != tag:
...         errors.append( (tag, guess, name) )</td>
</tr></table></td></tr>
</table></div>
<p>We can then examine individual error cases where the model predicted
the wrong label, and try to determine what additional pieces of
information would allow it to make the right decision (or which
existing pieces of information are tricking it into making the wrong
decision).  The feature set can then be adjusted accordingly.  The
names classifier that we have built generates about 100 errors on the
dev-test corpus:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> for (tag, guess, name) in sorted(errors):
...     print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))
correct=female   guess=male     name=Abigail
  ...
correct=female   guess=male     name=Cindelyn
  ...
correct=female   guess=male     name=Katheryn
correct=female   guess=male     name=Kathryn
  ...
correct=male     guess=female   name=Aldrich
  ...
correct=male     guess=female   name=Mitch
  ...
correct=male     guess=female   name=Rich
  ...</td>
</tr></table></td></tr>
</table></div>
<p>Looking through this list of errors makes it clear that some suffixes
that are more than one letter can be indicative of name genders.  For
example, names ending in <span class="example">yn</span> appear to be predominantly female,
despite the fact that names ending in <span class="example">n</span> tend to be male; and names
ending in <span class="example">ch</span> are usually male, even though names that end in <span class="example">h</span>
tend to be female.  We therefore
adjust our feature extractor to include features for two-letter
suffixes:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def gender_features(word):
...     return {'suffix1': word[-1:],
...             'suffix2': word[-2:]}</td>
</tr></table></td></tr>
</table></div>
<p>Rebuilding the classifier with the new feature extractor, we see that
the performance on the dev-test dataset improves by almost 2
percentage points (from 76.5% to 78.2%):</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> train_set = [(gender_features(n), gender) for (n, gender) in train_names]
>>> devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> print(nltk.classify.accuracy(classifier, devtest_set))
0.782</td>
</tr></table></td></tr>
</table></div>
<p>This error analysis procedure can then be repeated, checking for
patterns in the errors that are made by the newly improved classifier.
Each time the error analysis procedure is repeated, we should select a
different dev-test/training split, to ensure that the classifier
does not start to reflect idiosyncrasies in the dev-test set.</p>
<p>But once we've used the dev-test set to help us develop the
model, we can no longer trust that it will give us an accurate idea of
how well the model would perform on new data.  It is therefore
important to keep the test set separate, and unused, until our model
development is complete.  At that point, we can use the test set to
evaluate how well our model will perform on new input values.</p>
</div>
<div class="section" id="document-classification">
<h2>1.3&nbsp;&nbsp;&nbsp;Document Classification</h2>
<!-- Determinize, for the sake of doctest:

>>> import random; random.seed(12345) -->
<p>In <a href="#id25"><span class="problematic" id="id26">sec-extracting-text-from-corpora_</span></a>, we saw several examples of
corpora where documents have been labeled with categories.  Using
these corpora, we can build classifiers that will automatically tag
new documents with appropriate category labels.  First, we
construct a list of documents, labeled with the appropriate
categories.  For this example, we've chosen the Movie Reviews Corpus,
which categorizes each review as positive or negative.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import movie_reviews
>>> documents = [(list(movie_reviews.words(fileid)), category)
...              for category in movie_reviews.categories()
...              for fileid in movie_reviews.fileids(category)]
>>> random.shuffle(documents)</td>
</tr></table></td></tr>
</table></div>
<p>Next, we define a feature extractor for documents, so the classifier
will know which aspects of the data it should pay attention to
(<a class="reference internal" href="#code-document-classify-fd">1.2</a>).  For document topic identification, we can
define a feature for each word, indicating whether the document
contains that word.  To limit the number of features that the
classifier needs to process, we begin by constructing a list of the
2000 most frequent words in the overall corpus
<a class="reference internal" href="#document-classify-all-words"><span id="ref-document-classify-all-words"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.  We can then define a feature extractor
<a class="reference internal" href="#document-classify-extractor"><span id="ref-document-classify-extractor"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a> that simply checks whether each of these
words is present in a given document.</p>
<span class="target" id="code-document-classify-fd"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000] # [_document-classify-all-words]

def document_features(document): # [_document-classify-extractor]
    document_words = set(document) # [_document-classify-set]
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) 
{'contains(waste)': False, 'contains(lot)': False, ...}</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">A feature extractor for document classification, whose
features indicate whether or not individual words are present
in a given document.</td></tr></p>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The reason that we compute the set of all words in a document in
<a class="reference internal" href="#document-classify-set"><span id="ref-document-classify-set"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>, rather than just checking if
<tt class="doctest"><span class="pre">word in document</span></tt>, is
that checking whether a word occurs in a set is much faster than
checking whether it occurs in a list (<a href="#id27"><span class="problematic" id="id28">sec-algorithm-design_</span></a>).</p>
</div>
<p>Now that we've defined our feature extractor, we can use it to train a
classifier to label new movie reviews (<a class="reference internal" href="#code-document-classify-use">1.3</a>).  To
check how reliable the resulting classifier is, we compute its
accuracy on the test set <a class="reference internal" href="#document-classify-test"><span id="ref-document-classify-test"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.  And once again,
we can use <tt class="doctest"><span class="pre">show_most_informative_features()</span></tt> to find out which
features the classifier found to be most informative
<a class="reference internal" href="#document-classify-smif"><span id="ref-document-classify-smif"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.</p>
<span class="target" id="code-document-classify-use"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = nltk.NaiveBayesClassifier.train(train_set)</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(nltk.classify.accuracy(classifier, test_set)) # [_document-classify-test]
0.81
>>> classifier.show_most_informative_features(5) # [_document-classify-smif]
Most Informative Features
   contains(outstanding) = True              pos : neg    =     11.1 : 1.0
        contains(seagal) = True              neg : pos    =      7.7 : 1.0
   contains(wonderfully) = True              pos : neg    =      6.8 : 1.0
         contains(damon) = True              pos : neg    =      5.9 : 1.0
        contains(wasted) = True              neg : pos    =      5.8 : 1.0</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">Training and testing a classifier for document classification.</td></tr></p>
</table></div>
<p>Apparently in this corpus, a review that mentions &quot;Seagal&quot; is almost 8
times more likely to be negative than positive, while a review that
mentions &quot;Damon&quot; is about 6 times more likely to be positive.</p>
<!-- I tried adding simple frequencies w/ binning, but it didn't improve
performance.  Using something more advanced like tf-idf probably
would, but I don't want to take up the space here to explain all
that. -->
<!-- This classifier gives different results for most_informative_features on
successive runs. [EK] -->
</div>
<div class="section" id="part-of-speech-tagging">
<h2>1.4&nbsp;&nbsp;&nbsp;Part-of-Speech Tagging</h2>
<!-- The decision tree classifier is currently RIDICULOUSLY SLOW. :-/ -->
<p>In <a href="#id29"><span class="problematic" id="id30">chap-tag_</span></a> we built a regular expression tagger that chooses a
part-of-speech tag for a word by looking at the internal make-up of
the word.  However, this regular expression tagger had to be
hand-crafted.  Instead, we can train a classifier to work out which
suffixes are most informative.  Let's begin by finding out what the
most common suffixes are:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import brown
>>> suffix_fdist = nltk.FreqDist()
>>> for word in brown.words():
...     word = word.lower()
...     suffix_fdist[word[-1:]] += 1
...     suffix_fdist[word[-2:]] += 1
...     suffix_fdist[word[-3:]] += 1</td>
</tr></table></td></tr>
</table></div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]
>>> print(common_suffixes)
['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the',
 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l',
 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or',
 're', 'it', '``', 'an', "''", 'm', ';', 'i', 'ly', 'ion', ...]</td>
</tr></table></td></tr>
</table></div>
<!-- To make the emacs mode less confused: `` -->
<p>Next, we'll define a feature extractor function which checks a given
word for these suffixes:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def pos_features(word):
...     features = {}
...     for suffix in common_suffixes:
...         features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)
...     return features</td>
</tr></table></td></tr>
</table></div>
<p>Feature extraction functions behave like tinted glasses, highlighting
some of the properties (colors) in our data and making it impossible
to see other properties.  The classifier will rely exclusively on
these highlighted properties when determining how to label inputs.  In
this case, the classifier will make its decisions based only on
information about which of the common suffixes (if any) a given word
has.</p>
<!-- XXX I like the use of spaces in the doctest block, but I think this the only time -->
<!-- they are used. -->
<p>Now that we've defined our feature extractor, we can use it to
train a new &quot;decision tree&quot; classifier (to be discussed in
<a class="reference internal" href="#sec-decision-trees">4</a>):</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tagged_words = brown.tagged_words(categories='news')
>>> featuresets = [(pos_features(n), g) for (n,g) in tagged_words]</td>
</tr></table></td></tr>
</table></div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> size = int(len(featuresets) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]</td>
</tr></table></td></tr>
</table></div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> classifier = nltk.DecisionTreeClassifier.train(train_set)
>>> nltk.classify.accuracy(classifier, test_set)
0.62705121829935351</td>
</tr></table></td></tr>
</table></div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> classifier.classify(pos_features('cats'))
'NNS'</td>
</tr></table></td></tr>
</table></div>
<!-- XXX raise the issue of interpretability of models earlier?
(E.g. deliberately use the word "interpret" in the opening of the chapter?) -->
<p>One nice feature of decision tree models is that they are often fairly
easy to interpret &#8212; we can even instruct NLTK to print them
out as pseudocode:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(classifier.pseudocode(depth=4))
if endswith(,) == True: return ','
if endswith(,) == False:
  if endswith(the) == True: return 'AT'
  if endswith(the) == False:
    if endswith(s) == True:
      if endswith(is) == True: return 'BEZ'
      if endswith(is) == False: return 'VBZ'
    if endswith(s) == False:
      if endswith(.) == True: return '.'
      if endswith(.) == False: return 'NN'</td>
</tr></table></td></tr>
</table></div>
<!-- XXX NOTE: Last time I ran the doctests, I got a slightly different
decision tree.  Is this nondeterministic?  It appeared that the
decision tree was making *mostly* the same decisions.  We can
update this, but it will require updating the following text too.
Here's the decision tree I got on the most recent run:

 if endswith(the) == False:
   if endswith(,) == False:
     if endswith(s) == False:
       if endswith(.) == False: return '.'
       if endswith(.) == True: return '.'
     if endswith(s) == True:
       if endswith(is) == False: return 'PP$'
       if endswith(is) == True: return 'BEZ'
   if endswith(,) == True: return ','
 if endswith(the) == True: return 'AT' -->
<p>Here, we can see that the classifier begins by checking whether a word
ends with a comma &#8212; if so, then it will receive the special tag
<tt class="doctest"><span class="pre">","</span></tt>.  Next, the classifier checks if the word ends in <tt class="doctest"><span class="pre">"the"</span></tt>,
in which case it's almost certainly a determiner.  This &quot;suffix&quot; gets
used early by the decision tree because the word &quot;the&quot; is so common.
Continuing on, the classifier checks if the word ends in &quot;s&quot;.  If so,
then it's most likely to receive the verb tag <tt class="doctest"><span class="pre">VBZ</span></tt> (unless it's
the word &quot;is&quot;, which has a special tag <tt class="doctest"><span class="pre">BEZ</span></tt>), and if not,
then it's most likely a noun (unless it's the punctuation mark &quot;.&quot;).
The actual classifier contains further nested if-then statements below
the ones shown here, but the <tt class="doctest"><span class="pre">depth=4</span></tt> argument just displays the
top portion of the decision tree.</p>
<!-- XXX How about some features that match word patterns more generally,
linking back to the knowledge represented in a regular expression
tagger, such as: (r'^-?[0-9]+(.[0-9]+)?$', 'CD')
-> response: this is a very large search space, though we could
manually throw some of them in. -->
</div>
<div class="section" id="exploiting-context">
<h2>1.5&nbsp;&nbsp;&nbsp;Exploiting Context</h2>
<!-- Change this to use DecisionTree, to match prev section? -->
<p>By augmenting the feature extraction function, we could modify this
part-of-speech tagger to leverage a variety of other word-internal
features, such as the length of the word, the number of syllables it
contains, or its prefix.  However, as long as the feature extractor
just looks at the target word, we have no way to add features that
depend on the <em>context</em> that the word appears in.  But contextual
features often provide powerful clues about the correct tag &#8212; for
example, when tagging the word &quot;fly,&quot; knowing that the previous word
is &quot;a&quot; will allow us to determine that it is functioning as a noun, not
a verb.</p>
<p>In order to accommodate features that depend on a word's context, we
must revise the pattern that we used to define our feature extractor.
Instead of just passing in the word to be tagged, we will pass in a
complete (untagged) sentence, along with the index of the target word.
This approach is demonstrated in <a class="reference internal" href="#code-suffix-pos-tag">1.4</a>, which employs a
context-dependent feature extractor to define a part of speech tag
classifier.</p>
<span class="target" id="code-suffix-pos-tag"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def pos_features(sentence, i): # [_suffix-pos-tag-fd]
    features = {"suffix(1)": sentence[i][-1:],
                "suffix(2)": sentence[i][-2:],
                "suffix(3)": sentence[i][-3:]}
    if i == 0:
        features["prev-word"] = "<START>"
    else:
        features["prev-word"] = sentence[i-1]
    return features</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> pos_features(brown.sents()[0], 8)
{'suffix(3)': 'ion', 'prev-word': 'an', 'suffix(2)': 'on', 'suffix(1)': 'n'}

>>> tagged_sents = brown.tagged_sents(categories='news')
>>> featuresets = []
>>> for tagged_sent in tagged_sents:
...     untagged_sent = nltk.tag.untag(tagged_sent)
...     for i, (word, tag) in enumerate(tagged_sent):
...         featuresets.append( (pos_features(untagged_sent, i), tag) )

>>> size = int(len(featuresets) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)

>>> nltk.classify.accuracy(classifier, test_set)
0.78915962207856782</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">A part-of-speech classifier whose feature detector
examines the context in which a word appears in order to
determine which part of speech tag should be assigned.  In
particular, the identity of the previous word is included as a
feature.</td></tr></p>
</table></div>
<!-- This needs a comparison point from prev classifier!! -->
<!-- It would be nice to actually show this (using
show_most_informative_features or something): -->
<p>It is clear that exploiting contextual features improves the performance
of our part-of-speech tagger.  For example, the classifier learns
that a word is likely to be a noun if it comes immediately after the
word &quot;large&quot; or the word &quot;gubernatorial&quot;.  However, it is unable to
learn the generalization that a word is probably a noun if it follows
an adjective, because it doesn't have access to the previous word's
part-of-speech tag.  In general, simple classifiers always treat each
input as independent from all other inputs.  In many contexts, this
makes perfect sense.  For example, decisions about whether names tend
to be male or female can be made on a case-by-case basis.  However,
there are often cases, such as part-of-speech tagging, where we are
interested in solving classification problems that are closely related
to one another.</p>
</div>
<div class="section" id="sequence-classification">
<h2>1.6&nbsp;&nbsp;&nbsp;Sequence Classification</h2>
<p>In order to capture the dependencies between related classification
tasks, we can use <a name="joint_classifier_index_term" /><span class="termdef">joint classifier</span> models, which choose an
appropriate labeling for a collection of related inputs.  In the case
of part-of-speech tagging, a variety of different <a name="sequence_classifier_index_term" /><span class="termdef">sequence
classifier</span> models can be used to jointly choose part-of-speech
tags for all the words in a given sentence.</p>
<!-- note that consecutive classification isn't a widely-used term,
but it seems reasonably good to me. -->
<p>One sequence classification strategy, known as <a name="consecutive_classification_index_term" /><span class="termdef">consecutive
classification</span> or <a name="greedy_sequence_classification_index_term" /><span class="termdef">greedy sequence classification</span>, is to
find the most likely class label for the first input,
then to use that answer to help find the best label for the next
input.  The process can then be repeated until all of the inputs have
been labeled.  This is the approach that was taken by the bigram
tagger from <a href="#id31"><span class="problematic" id="id32">sec-n-gram-tagging_</span></a>, which began by choosing a
part-of-speech tag for the first word in the sentence, and then chose
the tag for each subsequent word based on the word itself and the
predicted tag for the previous word.</p>
<p>This strategy is demonstrated in <a class="reference internal" href="#code-consecutive-pos-tagger">1.5</a>.
First, we must
augment our feature extractor function to take a <tt class="doctest"><span class="pre">history</span></tt>
argument, which provides a list of the tags that we've predicted for
the sentence so far <a class="reference internal" href="#consec-pos-tag-features"><span id="ref-consec-pos-tag-features"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.
Each tag in <tt class="doctest"><span class="pre">history</span></tt> corresponds with a word in <tt class="doctest"><span class="pre">sentence</span></tt>.  But
note that <tt class="doctest"><span class="pre">history</span></tt> will only contain tags for words we've already
classified, that is, words to the left of the target word.  Thus, while it is
possible to look at some features of words to the right
of the target word, it is not possible to look at the tags for those
words (since we haven't generated them yet).</p>
<p>Having defined a feature extractor, we can proceed to build our
sequence classifier <a class="reference internal" href="#consec-pos-tagger"><span id="ref-consec-pos-tagger"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.  During training, we use
the annotated tags to
provide the appropriate history to the feature extractor, but when
tagging new sentences, we generate the history list based on the
output of the tagger itself.</p>
<span class="target" id="code-consecutive-pos-tagger"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"> def pos_features(sentence, i, history): # [_consec-pos-tag-features]
     features = {"suffix(1)": sentence[i][-1:],
                 "suffix(2)": sentence[i][-2:],
                 "suffix(3)": sentence[i][-3:]}
     if i == 0:
         features["prev-word"] = "<START>"
         features["prev-tag"] = "<START>"
     else:
         features["prev-word"] = sentence[i-1]
         features["prev-tag"] = history[i-1]
     return features

class ConsecutivePosTagger(nltk.TaggerI): # [_consec-pos-tagger]

    def __init__(self, train_sents):
        train_set = []
        for tagged_sent in train_sents:
            untagged_sent = nltk.tag.untag(tagged_sent)
            history = []
            for i, (word, tag) in enumerate(tagged_sent):
                featureset = pos_features(untagged_sent, i, history)
                train_set.append( (featureset, tag) )
                history.append(tag)
        self.classifier = nltk.NaiveBayesClassifier.train(train_set)

    def tag(self, sentence):
        history = []
        for i, word in enumerate(sentence):
            featureset = pos_features(sentence, i, history)
            tag = self.classifier.classify(featureset)
            history.append(tag)
        return zip(sentence, history)</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tagged_sents = brown.tagged_sents(categories='news')
>>> size = int(len(tagged_sents) * 0.1)
>>> train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]
>>> tagger = ConsecutivePosTagger(train_sents)
>>> print(tagger.evaluate(test_sents))
0.79796012981</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">Part of Speech Tagging with a Consecutive Classifier</td></tr></p>
</table></div>
</div>
<div class="section" id="other-methods-for-sequence-classification">
<h2>1.7&nbsp;&nbsp;&nbsp;Other Methods for Sequence Classification</h2>
<p>One shortcoming of this approach is that we commit to every decision
that we make.  For example, if we decide to label a word as a noun,
but later find evidence that it should have been a verb, there's no
way to go back and fix our mistake.  One solution to this problem is
to adopt a transformational strategy instead.  Transformational joint
classifiers work by creating an initial assignment of labels for the
inputs, and then iteratively refining that assignment in an attempt to
repair inconsistencies between related inputs.  The Brill tagger,
described in <a href="#id33"><span class="problematic" id="id34">sec-transformation-based-tagging_</span></a>, is a good example of this strategy.</p>
<p>Another solution is to assign scores to all of the possible
sequences of part-of-speech tags, and to choose the sequence
whose overall score is highest. This is the approach taken by
<a name="hidden_markov_models_index_term" /><span class="termdef">Hidden Markov Models</span>.
Hidden Markov Models are similar to consecutive classifiers in
that they look at both the inputs and the history of predicted
tags. However, rather than simply finding the single best tag
for a given word, they generate a probability distribution over
tags. These probabilities are then combined to calculate
probability scores for tag sequences, and the tag sequence with
the highest probability is chosen. Unfortunately, the number of
possible tag sequences is quite large. Given a tag set with 30
tags, there are about 600 trillion (30<sup>10</sup>) ways to label
a 10-word sentence.  In order to avoid considering all these possible
sequences separately, Hidden Markov Models require that the
feature extractor only look at the most recent tag (or the most
recent <span class="math">n</span> tags, where <span class="math">n</span> is fairly small). Given that
restriction, it is possible to use dynamic programming (<a href="#id35"><span class="problematic" id="id36">sec-algorithm-design_</span></a>)
to efficiently find the most likely tag sequence. In particular,
for each consecutive word index <span class="math">i</span>,
a score is computed for each possible current and previous tag.
This same basic approach is taken by
two more advanced models, called <a name="maximum_entropy_markov_models_index_term" /><span class="termdef">Maximum Entropy Markov Models</span> and
<a name="linear_chain_conditional_random_field_models_index_term" /><span class="termdef">Linear-Chain Conditional Random Field Models</span>;
but different algorithms are used to find scores for tag sequences.</p>
</div>
</div>
<div class="section" id="further-examples-of-supervised-classification">
<span id="sec-further-examples-of-supervised-classification"></span><h1>2&nbsp;&nbsp;&nbsp;Further Examples of Supervised Classification</h1>
<div class="section" id="sentence-segmentation">
<h2>2.1&nbsp;&nbsp;&nbsp;Sentence Segmentation</h2>
<p>Sentence segmentation can be viewed as a classification task for
punctuation: whenever we encounter a symbol that could possibly end a
sentence, such as a period or a question mark, we have to decide
whether it terminates the preceding sentence.</p>
<p>The first step is to obtain some data that has already been segmented
into sentences and convert it into a form that is suitable for
extracting features:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> sents = nltk.corpus.treebank_raw.sents()
>>> tokens = []
>>> boundaries = set()
>>> offset = 0
>>> for sent in sents:
...     tokens.extend(sent)
...     offset += len(sent)
...     boundaries.add(offset-1)</td>
</tr></table></td></tr>
</table></div>
<p>Here, <tt class="doctest"><span class="pre">tokens</span></tt> is a merged list of tokens from the individual
sentences, and <tt class="doctest"><span class="pre">boundaries</span></tt> is a set containing the indexes of all
sentence-boundary tokens.  Next, we need to specify the features of
the data that will be used in order to decide whether punctuation
indicates a sentence-boundary:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def punct_features(tokens, i):
...     return {'next-word-capitalized': tokens[i+1][0].isupper(),
...             'prev-word': tokens[i-1].lower(),
...             'punct': tokens[i],
...             'prev-word-is-one-char': len(tokens[i-1]) == 1}</td>
</tr></table></td></tr>
</table></div>
<p>Based on this feature extractor, we can create a list of labeled
featuresets by selecting all the punctuation tokens, and tagging
whether they are boundary tokens or not:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> featuresets = [(punct_features(tokens, i), (i in boundaries))
...                for i in range(1, len(tokens)-1)
...                if tokens[i] in '.?!']</td>
</tr></table></td></tr>
</table></div>
<p>Using these featuresets, we can train and evaluate a
punctuation classifier:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> size = int(len(featuresets) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> nltk.classify.accuracy(classifier, test_set)
0.936026936026936</td>
</tr></table></td></tr>
</table></div>
<p>To use this classifier to perform sentence segmentation, we simply
check each punctuation mark to see whether it's labeled as a boundary;
and divide the list of words at the boundary marks.  The listing
in <a class="reference internal" href="#code-classification-based-segmenter">2.1</a> shows how this can be done.</p>
<span class="target" id="code-classification-based-segmenter"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def segment_sentences(words):
    start = 0
    sents = []
    for i, word in enumerate(words):
        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:
            sents.append(words[start:i+1])
            start = i+1
    if start < len(words):
        sents.append(words[start:])
    return sents</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">Classification Based Sentence Segmenter</td></tr></p>
</table></div>
</div>
<div class="section" id="identifying-dialogue-act-types">
<h2>2.2&nbsp;&nbsp;&nbsp;Identifying Dialogue Act Types</h2>
<p>When processing dialogue, it can be useful to think of
utterances as a type of <em>action</em> performed by the speaker.  This
interpretation is most straightforward for performative statements
such as &quot;I forgive you&quot; or &quot;I bet you can't climb that hill.&quot;  But
greetings, questions, answers, assertions, and clarifications can all
be thought of as types of speech-based actions.  Recognizing the
<a name="dialogue_acts_index_term" /><span class="termdef">dialogue acts</span> underlying the utterances in a dialogue can be an
important first step in understanding the conversation.</p>
<p>The NPS Chat Corpus, which was demonstrated in
<a href="#id37"><span class="problematic" id="id38">sec-extracting-text-from-corpora_</span></a>, consists of over 10,000 posts from
instant messaging sessions.  These posts have all been labeled with
one of 15 dialogue act types, such as &quot;Statement,&quot; &quot;Emotion,&quot;
&quot;ynQuestion&quot;, and &quot;Continuer.&quot;  We can therefore use this data to
build a classifier that can identify the dialogue act types for new
instant messaging posts.  The first step is to extract the basic
messaging data.  We will call <tt class="doctest"><span class="pre">xml_posts()</span></tt> to get a data structure
representing the XML annotation for each post:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> posts = nltk.corpus.nps_chat.xml_posts()[:10000]</td>
</tr></table></td></tr>
</table></div>
<p>Next, we'll define a simple feature extractor that checks what words
the post contains:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def dialogue_act_features(post):
...     features = {}
...     for word in nltk.word_tokenize(post):
...         features['contains({})'.format(word.lower())] = True
...     return features</td>
</tr></table></td></tr>
</table></div>
<p>Finally, we construct the training and testing data by applying the
feature extractor to each post (using <tt class="doctest"><span class="pre">post.get('class')</span></tt> to get
a post's dialogue act type), and create a new classifier:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> featuresets = [(dialogue_act_features(post.text), post.get('class'))
...                for post in posts]
>>> size = int(len(featuresets) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> print(nltk.classify.accuracy(classifier, test_set))
0.67</td>
</tr></table></td></tr>
</table></div>
<!-- XXX delete note -->
<!-- *We had planned to extend this example by showing how a sequence
classifier can be used to take advantage of the fact that
some sequences (eg ynquestion->yanswer) are more likely than
others.* -->
</div>
<div class="section" id="recognizing-textual-entailment">
<h2>2.3&nbsp;&nbsp;&nbsp;Recognizing Textual Entailment</h2>
<p>Recognizing textual entailment (RTE) is the task of determining
whether a given piece of text <em>T</em> entails another text called the
&quot;hypothesis&quot; (as already discussed in <a href="#id39"><span class="problematic" id="id40">sec-automatic-natural-language-understanding_</span></a>).
To date, there have been four RTE Challenges, where
shared development and test data is made available to competing teams.
Here are a couple of examples of text/hypothesis pairs from the
Challenge 3 development dataset. The label <em>True</em> indicates that the
entailment holds, and <em>False</em>, that it fails to hold.</p>
<blockquote>
<p>Challenge 3, Pair 34 (True)</p>
<blockquote>
<p><strong>T</strong>: Parviz Davudi was representing Iran at a meeting of the Shanghai
Co-operation Organisation (SCO), the fledgling association that
binds Russia, China and four former Soviet republics of central
Asia together to fight terrorism.</p>
<p><strong>H</strong>: China is a member of SCO.</p>
</blockquote>
<p>Challenge 3, Pair 81 (False)</p>
<blockquote>
<p><strong>T</strong>: According to NC Articles of Organization, the members of LLC
company are H. Nelson Beavers, III, H. Chester Beavers and Jennie
Beavers Stewart.</p>
<p><strong>H</strong>: Jennie Beavers Stewart is a share-holder of Carolina Analytical
Laboratory.</p>
</blockquote>
</blockquote>
<p>It should be emphasized that the relationship between text and
hypothesis is not intended to be logical entailment, but rather
whether a human would conclude that the text provides reasonable
evidence for taking the hypothesis to be true.</p>
<p>We can treat RTE as a classification task, in which we try to
predict the <em>True</em>/<em>False</em> label for each pair. Although it seems
likely that successful approaches to this task will involve a
combination of parsing, semantics and real world
knowledge, many early attempts at RTE achieved reasonably good results
with shallow analysis, based on similarity between the text and
hypothesis at the word level. In the ideal case, we would expect that if
there is an entailment, then all the information expressed by the hypothesis
should also be present in the text. Conversely, if there is information
found in the hypothesis that is absent from the text, then there
will be no entailment.</p>
<p>In our RTE feature detector (<a class="reference internal" href="#code-rte-features">2.2</a>), we let words
(i.e., word types) serve as proxies for information, and
our features count the degree of word overlap, and the degree to which
there are words in the hypothesis but not in the text (captured by the
method <tt class="doctest"><span class="pre">hyp_extra()</span></tt>). Not all words are equally important &#8212;
Named Entity mentions such as the names of people, organizations and
places are likely to be more significant, which motivates us to
extract distinct information for <tt class="doctest"><span class="pre">word</span></tt>s  and <tt class="doctest"><span class="pre">ne</span></tt>s (Named
Entities). In addition, some high frequency function words are
filtered out as &quot;stopwords&quot;.</p>
<!-- XXX following pylisting is not referenced from the text -->
<table class="docutils citation" frame="void" id="xx" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[xx]</td><td>give some intro to RTEFeatureExtractor??</td></tr>
</tbody>
</table>
<span class="target" id="code-rte-features"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def rte_features(rtepair):
    extractor = nltk.RTEFeatureExtractor(rtepair)
    features = {}
    features['word_overlap'] = len(extractor.overlap('word'))
    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))
    features['ne_overlap'] = len(extractor.overlap('ne'))
    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))
    return features</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">&quot;Recognizing Text Entailment&quot; Feature Extractor.  The
<tt class="doctest"><span class="pre">RTEFeatureExtractor</span></tt> class builds a bag
of words for both the text and the hypothesis after throwing
away some stopwords, then calculates overlap and difference.</td></tr></p>
</table></div>
<p>To illustrate the content of these features, we examine some
attributes of the text/hypothesis Pair 34 shown earlier:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]
>>> extractor = nltk.RTEFeatureExtractor(rtepair)
>>> print(extractor.text_words)
{'Russia', 'Organisation', 'Shanghai', 'Asia', 'four', 'at',
'operation', 'SCO', ...}
>>> print(extractor.hyp_words)
{'member', 'SCO', 'China'}
>>> print(extractor.overlap('word'))
set()
>>> print(extractor.overlap('ne'))
{'SCO', 'China'}
>>> print(extractor.hyp_extra('word'))
{'member'}</td>
</tr></table></td></tr>
</table></div>
<p>These features indicate that all important words in the hypothesis are
contained in the text, and thus there is some evidence for labeling this
as <em>True</em>.</p>
<p>The module <tt class="doctest"><span class="pre">nltk.classify.rte_classify</span></tt> reaches just over 58%
accuracy on the combined RTE test data using methods like these. Although
this figure is not very impressive, it requires significant effort, and more
linguistic processing, to achieve much better results.</p>
</div>
<div class="section" id="scaling-up-to-large-datasets">
<h2>2.4&nbsp;&nbsp;&nbsp;Scaling Up to Large Datasets</h2>
<p>Python provides an excellent environment for performing basic text
processing and feature extraction.  However, it is not able to perform
the numerically intensive calculations required by machine learning
methods nearly as quickly as lower-level languages such as C.  Thus,
if you attempt to use the pure-Python machine learning implementations
(such as <tt class="doctest"><span class="pre">nltk.NaiveBayesClassifier</span></tt>) on large datasets, you may
find that the learning algorithm takes an unreasonable amount of time
and memory to complete.</p>
<p>If you plan to train classifiers with large amounts of training data
or a large number of features, we recommend that you explore
NLTK's facilities for interfacing with external machine learning
packages.  Once these packages have been installed, NLTK can
transparently invoke them (via system calls) to train classifier
models significantly faster than the pure-Python classifier
implementations.  See the NLTK webpage for a list of recommended
machine learning packages that are supported by NLTK.</p>
<!-- SB: I think numpy and other numerical libraries do their work in C, and
are probably quite efficient.  Not clear about support for sparse arrays. -->
<!-- EL: Yes, but even with numpy etc, python is *slow* for this stuff.
I speak from experience. :) -->
<!-- XXX Note that the additive nature of a lot of training tasks mean
that parallelization will work fine.  If those can be done using
MapReduce we'll be able to use NLTK for some non-toy problems here. -->
</div>
</div>
<div class="section" id="evaluation">
<span id="sec-evaluation"></span><h1>3&nbsp;&nbsp;&nbsp;Evaluation</h1>
<p>In order to decide whether a classification model is accurately
capturing a pattern, we must evaluate that model.  The result of this
evaluation is important for deciding how trustworthy the model is, and
for what purposes we can use it.  Evaluation can also be an effective tool
for guiding us in making future improvements to the model.</p>
<div class="section" id="the-test-set">
<h2>3.1&nbsp;&nbsp;&nbsp;The Test Set</h2>
<p>Most evaluation techniques calculate a score for a model by comparing
the labels that it generates for the inputs in a <a name="test_set_index_term_2" /><span class="termdef">test set</span>
(or <a name="evaluation_set_index_term" /><span class="termdef">evaluation set</span>)
with the correct labels for those inputs.  This test set
typically has the same format as the training set.  However, it is
very important that the test set be distinct from the training
corpus: if we simply re-used the training set as the test
set, then a model that simply memorized its input, without learning
how to generalize to new examples, would receive misleadingly high
scores.</p>
<p>When building the test set, there is often a trade-off between
the amount of data available for testing and the amount available
for training.  For classification tasks that have a small
number of well-balanced labels and a diverse test set, a
meaningful evaluation can be performed with as few as 100 evaluation
instances.  But if a classification task has a large number of labels,
or includes very infrequent labels, then the size of the test
set should be chosen to ensure that the least frequent label occurs at
least 50 times.  Additionally, if the test set contains many
closely related instances &#8212; such as instances drawn from a single
document &#8212; then the size of the test set should be increased to
ensure that this lack of diversity does not skew the evaluation
results.  When large amounts of annotated data are available, it is
common to err on the side of safety by using 10% of the overall data
for evaluation.</p>
<p>Another consideration when choosing the test set is the degree
of similarity between instances in the test set and those in the
development set.  The more similar these two datasets are, the less
confident we can be that evaluation results will generalize to other
datasets.  For example, consider the part-of-speech tagging task.  At
one extreme, we could create the training set and test set by
randomly assigning sentences from a data source that reflects a single
genre (news):</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> import random
>>> from nltk.corpus import brown
>>> tagged_sents = list(brown.tagged_sents(categories='news'))
>>> random.shuffle(tagged_sents)
>>> size = int(len(tagged_sents) * 0.1)
>>> train_set, test_set = tagged_sents[size:], tagged_sents[:size]</td>
</tr></table></td></tr>
</table></div>
<p>In this case, our test set will be <em>very</em> similar to our training
set.  The training set and test set are taken from the same
genre, and so we cannot be confident that evaluation results would
generalize to other genres.  What's worse, because of the call to
<tt class="doctest"><span class="pre">random.shuffle()</span></tt>, the test set contains sentences that are
taken from the same documents that were used for training.  If there
is any consistent pattern within a document &#8212; say, if a given word
appears with a particular part-of-speech tag especially frequently &#8212; then
that difference will be reflected in both the development set and the
test set.  A somewhat better approach is to ensure that
the training set and test set are taken from different documents:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> file_ids = brown.fileids(categories='news')
>>> size = int(len(file_ids) * 0.1)
>>> train_set = brown.tagged_sents(file_ids[size:])
>>> test_set = brown.tagged_sents(file_ids[:size])</td>
</tr></table></td></tr>
</table></div>
<p>If we want to perform a more stringent evaluation, we can draw the
test set from documents that are less closely related to those
in the training set:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> train_set = brown.tagged_sents(categories='news')
>>> test_set = brown.tagged_sents(categories='fiction')</td>
</tr></table></td></tr>
</table></div>
<p>If we build a classifier that performs well on this test set,
then we can be confident that it has the power to generalize well
beyond the data that it was trained on.</p>
</div>
<div class="section" id="accuracy">
<h2>3.2&nbsp;&nbsp;&nbsp;Accuracy</h2>
<p>The simplest metric that can be used to evaluate a classifier,
<a name="accuracy_index_term" /><span class="termdef">accuracy</span>, measures the percentage of inputs in the test
set that the classifier correctly labeled.  For example, a name gender
classifier that predicts the correct name 60 times in a test
set containing 80 names would have an accuracy of 60/80 = 75%.  The
function <tt class="doctest"><span class="pre">nltk.classify.accuracy()</span></tt> will calculate the
accuracy of a classifier model on a given test set:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> classifier = nltk.NaiveBayesClassifier.train(train_set) 
>>> print('Accuracy: {:4.2f}'.format(nltk.classify.accuracy(classifier, test_set))) 
0.75</td>
</tr></table></td></tr>
</table></div>
<p>When interpreting the accuracy score of a classifier, it is important
to take into consideration the frequencies of the individual class
labels in the test set.  For example, consider a classifier that
determines the correct word sense for each occurrence of the word
<span class="example">bank</span>.  If we evaluate this classifier on financial newswire text,
then we may find that the <tt class="doctest"><span class="pre">financial-institution</span></tt> sense appears 19
times out of 20.  In that case, an accuracy of 95% would hardly be
impressive, since we could achieve that accuracy with a model that
always returns the <tt class="doctest"><span class="pre">financial-institution</span></tt> sense.  However, if we
instead evaluate the classifier on a more balanced corpus, where the
most frequent word sense has a frequency of 40%, then a 95% accuracy
score would be a much more positive result.  (A similar issue arises
when measuring inter-annotator agreement in
<a href="#id41"><span class="problematic" id="id42">sec-life-cycle-of-a-corpus_</span></a>.)</p>
</div>
<div class="section" id="precision-and-recall">
<h2>3.3&nbsp;&nbsp;&nbsp;Precision and Recall</h2>
<p>Another instance where accuracy scores can be misleading is in
&quot;search&quot; tasks, such as information retrieval, where we are attempting
to find documents that are relevant to a particular task.  Since the
number of irrelevant documents far outweighs the number of relevant
documents, the accuracy score for a model that labels every document
as irrelevant would be very close to 100%.</p>
<div class="system-message" id="fig-precision-recall">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1216)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '25:120:30')
invalid literal for int() with base 10: '25:120:30'.</p>
<pre class="literal-block">
.. figure:: ../images/precision-recall.png
   :scale: 25:120:30

   True and False Positives and Negatives

</pre>
</div>
<p>It is therefore conventional to employ a different set of measures for
search tasks, based on the number of items in each of the four
categories shown in <a class="reference internal" href="#fig-precision-recall">fig-precision-recall</a>:</p>
<ul class="simple">
<li><a name="true_positives_index_term" /><span class="termdef">True positives</span> are relevant items that we correctly identified
as relevant.</li>
<li><a name="true_negatives_index_term" /><span class="termdef">True negatives</span> are irrelevant items that we correctly identified
as irrelevant.</li>
<li><a name="false_positives_index_term" /><span class="termdef">False positives</span> (or <a name="type_i_errors_index_term" /><span class="termdef">Type I errors</span>) are irrelevant items
that we incorrectly identified as relevant.</li>
<li><a name="false_negatives_index_term" /><span class="termdef">False negatives</span> (or <a name="type_ii_errors_index_term" /><span class="termdef">Type II errors</span>) are relevant items
that we incorrectly identified as irrelevant.</li>
</ul>
<p>Given these four numbers, we can define the following metrics:</p>
<ul class="simple">
<li><a name="precision_index_term" /><span class="termdef">Precision</span>, which indicates how many of the items that we
identified were relevant, is <span class="math">TP/(TP+FP)</span>.</li>
<li><a name="recall_index_term" /><span class="termdef">Recall</span>, which indicates how many of the relevant items that we
identified, is <span class="math">TP/(TP+FN)</span>.</li>
<li>The <a name="f_measure_index_term" /><span class="termdef">F-Measure</span> (or <a name="f_score_index_term" /><span class="termdef">F-Score</span>), which combines the precision
and recall to give a single score, is defined to be the harmonic
mean of the precision and recall:
(2 &#215; <em>Precision</em> &#215; <em>Recall</em>) / (<em>Precision</em> + <em>Recall</em>).</li>
</ul>
<!-- alternative def for f-score: 2/(1/Precision+1/Recall) -->
</div>
<div class="section" id="confusion-matrices">
<h2>3.4&nbsp;&nbsp;&nbsp;Confusion Matrices</h2>
<!-- Repeat some code from chapter 5, which recreates the tagger t2.
This tagger is used to illustrate the confusion matrix:

 >>> from nltk.corpus import brown
 >>> brown_tagged_sents = brown.tagged_sents(categories='news')
 >>> size = int(len(brown_tagged_sents) * 0.9)
 >>> train_sents = brown_tagged_sents[:size]
 >>> test_sents = brown_tagged_sents[size:]
 >>> t0 = nltk.DefaultTagger('NN')
 >>> t1 = nltk.UnigramTagger(train_sents, backoff=t0)
 >>> t2 = nltk.BigramTagger(train_sents, backoff=t1) -->
<p>When performing classification tasks with three or more labels, it can
be informative to subdivide the errors made by the model based on
which types of mistake it made.  A <a name="confusion_matrix_index_term" /><span class="termdef">confusion matrix</span> is a table
where each cell [<span class="math">i</span>,<span class="math">j</span>] indicates how often label <span class="math">j</span> was
predicted when the correct label was <span class="math">i</span>.  Thus, the diagonal
entries (i.e., cells <a href="#id17"><span class="problematic" id="id18">|ii|</span></a>) indicate labels that were
correctly predicted, and the off-diagonal entries indicate errors.  In
the following example, we generate a confusion matrix for the bigram
tagger developed in <a href="#id43"><span class="problematic" id="id44">sec-automatic-tagging_</span></a>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def tag_list(tagged_sents):
...     return [tag for sent in tagged_sents for (word, tag) in sent]
>>> def apply_tagger(tagger, corpus):
...     return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]
>>> gold = tag_list(brown.tagged_sents(categories='editorial'))
>>> test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))
>>> cm = nltk.ConfusionMatrix(gold, test)
>>> print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))
    |                                         N                      |
    |      N      I      A      J             N             V      N |
    |      N      N      T      J      .      S      ,      B      P |
----+----------------------------------------------------------------+
 NN | <11.8%>  0.0%      .   0.2%      .   0.0%      .   0.3%   0.0% |
 IN |   0.0%  <9.0%>     .      .      .   0.0%      .      .      . |
 AT |      .      .  <8.6%>     .      .      .      .      .      . |
 JJ |   1.7%      .      .  <3.9%>     .      .      .   0.0%   0.0% |
  . |      .      .      .      .  <4.8%>     .      .      .      . |
NNS |   1.5%      .      .      .      .  <3.2%>     .      .   0.0% |
  , |      .      .      .      .      .      .  <4.4%>     .      . |
 VB |   0.9%      .      .   0.0%      .      .      .  <2.4%>     . |
 NP |   1.0%      .      .   0.0%      .      .      .      .  <1.8%>|
----+----------------------------------------------------------------+
(row = reference; col = test)
<BLANKLINE></td>
</tr></table></td></tr>
</table></div>
<p>The confusion matrix indicates that common errors include a
substitution of <tt class="doctest"><span class="pre">NN</span></tt> for <tt class="doctest"><span class="pre">JJ</span></tt> (for 1.6% of words), and of <tt class="doctest"><span class="pre">NN</span></tt> for <tt class="doctest"><span class="pre">NNS</span></tt> (for
1.5% of words).  Note that periods (<tt class="doctest"><span class="pre">.</span></tt>) indicate cells
whose value is 0, and that the diagonal entries &#8212; which correspond to
correct classifications &#8212; are marked with angle brackets.
.. XXX explain use of &quot;reference&quot; in the legend above.</p>
</div>
<div class="section" id="cross-validation">
<h2>3.5&nbsp;&nbsp;&nbsp;Cross-Validation</h2>
<p>In order to evaluate our models, we must reserve a portion of the
annotated data for the test set.  As we already mentioned,
if the test set is too small, then
our evaluation may not be accurate.  However, making the test set
larger usually means making the training set smaller, which can have a
significant impact on performance if a limited amount of annotated
data is available.</p>
<p>One solution to this problem is to perform multiple evaluations on
different test sets, then to combine the scores from those
evaluations, a technique known as <a name="cross_validation_index_term" /><span class="termdef">cross-validation</span>.  In
particular, we subdivide the original corpus into <span class="math">N</span>
subsets called <a name="folds_index_term" /><span class="termdef">folds</span>.  For each of these folds, we train a model using all
of the data <em>except</em> the data in that fold, and then test that
model on the fold.  Even though the individual folds might
be too small to give accurate evaluation scores on their own, the
combined evaluation score is based on a large amount of data, and is
therefore quite reliable.</p>
<p>A second, and equally important, advantage of using cross-validation
is that it allows us to examine how widely the performance varies
across different training sets.  If we get very similar scores for all
<span class="math">N</span> training sets, then we can be fairly confident that the
score is accurate.  On the other hand, if scores vary widely across
the <span class="math">N</span> training sets, then we should probably be skeptical
about the accuracy of the evaluation score.</p>
<!-- XXX give a code snippet for dividing corpus up for cross-validation -->
<!-- mention statistical significance tests explicitly here? -->
</div>
</div>
<div class="section" id="decision-trees">
<span id="sec-decision-trees"></span><h1>4&nbsp;&nbsp;&nbsp;Decision Trees</h1>
<p>In the next three sections, we'll take a closer look at three machine
learning methods that can be used to automatically build
classification models: decision trees, naive Bayes classifiers, and
Maximum Entropy classifiers.  As we've seen, it's possible to treat these
learning methods as black boxes, simply training models and using them
for prediction without understanding how they work.  But there's a lot
to be learned from taking a closer look at how these learning methods
select models based on the data in a training set.  An
understanding of these methods can help guide our selection of
appropriate features, and especially our decisions about how those
features should be encoded.  And an understanding of the generated
models can allow us to extract information about which features
are most informative, and how those features relate to one another.</p>
<!-- Note that they haven't necessarily seen syntax trees before this, so
it may seem odd to them (or at least not obvious) that these "trees"
are upside down. -->
<!-- XXX some people will know the concept of a "dichotomous key".
("tree-structured flowchart" assumes domain knowledge so doesn't
communicate as widely) -->
<p>A <a name="decision_tree_index_term" /><span class="termdef">decision tree</span> is a simple flowchart that selects
labels for input values.  This flowchart consists of <a name="decision_nodes_index_term" /><span class="termdef">decision
nodes</span>, which check feature values, and <a name="leaf_nodes_index_term" /><span class="termdef">leaf nodes</span>, which
assign labels.  To choose the label for an input value, we begin at
the flowchart's initial decision node, known as its <a name="root_node_index_term" /><span class="termdef">root node</span>.
This node contains a condition that checks one of the input value's
features, and selects a branch based on that feature's value.
Following the branch that describes our input value, we arrive at a
new decision node, with a new condition on the input value's features.
We continue following the branch selected by each node's condition,
until we arrive at a leaf node which provides a label for the input
value.  <a class="reference internal" href="#fig-decision-tree">fig-decision-tree</a> shows an example decision tree model for
the name gender task.</p>
<div class="system-message" id="fig-decision-tree">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1383)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '50:120:130')
invalid literal for int() with base 10: '50:120:130'.</p>
<pre class="literal-block">
.. figure:: ../images/decision-tree.png
   :scale: 50:120:130

   Decision Tree model for the name gender task.  Note that tree
   diagrams are conventionally drawn &quot;upside down,&quot; with the root at the
   top, and the leaves at the bottom.

</pre>
</div>
<!-- XXX show: train decision tree, then print it as pseudocode & as more
compact tree. -->
<!-- XXX seems odd to talk about a decision tree "modeling a training set" below. -->
<p>Once we have a decision tree, it is straightforward to
use it to assign labels to new input values.  What's less straightforward
is how we can build a decision tree that models a given
training set.  But before we look at the learning algorithm for
building decision trees, we'll consider a simpler task: picking the
best &quot;decision stump&quot; for a corpus.  A <a name="decision_stump_index_term" /><span class="termdef">decision stump</span> is a
decision tree with a single node that decides how to classify inputs
based on a single feature.  It contains one leaf for each possible
feature value, specifying the class label that should be assigned to
inputs whose features have that value.  In order to build a decision
stump, we must first decide which feature should be used.  The
simplest method is to just build a decision stump for each possible
feature, and see which one achieves the highest accuracy on the
training data, although there are other alternatives that we will discuss below.
Once we've picked a feature, we can build the decision stump by assigning a
label to each leaf based on the most frequent label for the selected
examples in the training set (i.e., the examples where the selected
feature has that value).</p>
<!-- XX show building a decision stump -->
<!-- XX show refining a decision stump's leaf -->
<p>Given the algorithm for choosing decision stumps, the algorithm for
growing larger decision trees is straightforward.  We
begin by selecting the overall best decision stump for the
classification task.  We
then check the accuracy of each of the leaves on the training set.
Leaves that do not achieve sufficient accuracy are then
replaced by new decision stumps, trained on the subset of the training
corpus that is selected by the path to the leaf.  For example, we
could grow the decision tree in <a class="reference internal" href="#fig-decision-tree">fig-decision-tree</a> by replacing the
leftmost leaf with a new decision stump, trained on the subset of the
training set names that do not start with a &quot;k&quot; or end with a vowel
or an &quot;l.&quot;</p>
<div class="section" id="entropy-and-information-gain">
<h2>4.1&nbsp;&nbsp;&nbsp;Entropy and Information Gain</h2>
<p>As was mentioned before, there are several methods for identifying
the most informative feature for a decision stump.  One
popular alternative, called <a name="information_gain_index_term" /><span class="termdef">information gain</span>, measures how
much more organized the input values become when we divide them up
using a given feature.  To measure how disorganized the original set
of input values are, we calculate entropy of their labels, which will
be high if the input values have highly varied labels, and low if many
input values all have the same label.  In particular, entropy is
defined as the sum of the probability of each label times the log
probability of that same label:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(1)</td><td width="15"></td><td><span class="math">H</span> = &#8722;&#931;<sub>l |in| labels</sub><span class="math">P(l)</span> &#215; <span class="math">log</span><sub>2</sub><span class="math">P(l)</span>.</td></tr></table></p>
<div class="system-message" id="fig-entropy">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1449)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '50:150:150')
invalid literal for int() with base 10: '50:150:150'.</p>
<pre class="literal-block">
.. figure:: ../images/Binary_entropy_plot.png
   :scale: 50:150:150

   The entropy of labels in the name gender prediction task, as a
   function of the percentage of names in a given set that are male.

</pre>
</div>
<p>For example, <a class="reference internal" href="#fig-entropy">fig-entropy</a> shows how the entropy of labels in
the name gender prediction task depends on the ratio of male to female
names.  Note that if most input values have the same label (e.g., if
P(male) is near 0 or near 1), then entropy is low.  In particular,
labels that have low frequency do not contribute much to the entropy
(since <span class="math">P(l)</span> is small), and labels with high frequency also do
not contribute much to the entropy (since <span class="math">log</span><sub>2</sub><span class="math">P(l)</span> is small).  On the other hand, if the input values have
a wide variety of labels, then there are many labels with a &quot;medium&quot;
frequency, where neither <span class="math">P(l)</span> nor <span class="math">log</span><sub>2</sub><span class="math">P(l)</span> is small, so the entropy is high.
<a class="reference internal" href="#code-entropy">4.1</a> demonstrates how to calculate the entropy
of a list of labels.</p>
<!-- (skip doctest on the 0-entropy example below because it prints
-0.0, and it's not worth it to explain *why* it does that.) -->
<span class="target" id="code-entropy"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">import math
def entropy(labels):
    freqdist = nltk.FreqDist(labels)
    probs = [freqdist.freq(l) for l in freqdist]
    return -sum(p * math.log(p,2) for p in probs)</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(entropy(['male', 'male', 'male', 'male'])) 
0.0
>>> print(entropy(['male', 'female', 'male', 'male']))
0.811...
>>> print(entropy(['female', 'male', 'female', 'male']))
1.0
>>> print(entropy(['female', 'female', 'male', 'female']))
0.811...
>>> print(entropy(['female', 'female', 'female', 'female'])) 
0.0</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption">Calculating the Entropy of a List of Labels</td></tr></p>
</table></div>
<!-- The doctest: +SKIP in the last example is because doctest prints
-0.0 on some systems rather than 0.0, which can be confusing;
but isn't worth explaining here. -->
<p>Once we have calculated the entropy of the original set of input
values' labels, we can determine how much more organized the labels
become once we apply the decision stump.  To do so, we calculate the
entropy for each of the decision stump's leaves, and take the average
of those leaf entropy values (weighted by the number of samples in
each leaf).  The information gain is then equal to the original
entropy minus this new, reduced entropy.  The higher the information
gain, the better job the decision stump does of dividing the input
values into coherent groups, so we can build decision trees by
selecting the decision stumps with the highest information gain.</p>
<p>Another consideration for decision trees is efficiency.  The simple
algorithm for selecting decision stumps described above must construct
a candidate decision stump for every possible feature, and this
process must be repeated for every node in the constructed decision
tree.  A number of algorithms have been developed to cut down on the
training time by storing and reusing information about previously
evaluated examples.</p>
<!-- <<references>>. -->
<!-- XXX the point about decision trees being easy to interpret
has already been made, so this sounds repetitive.
It would be fine to delete the earlier mention of this point,
since it was not in a section on decision trees. -->
<p>Decision trees have a number of useful qualities.  To begin with,
they're simple to understand, and easy to interpret.  This is
especially true near the top of the decision tree, where it is usually
possible for the learning algorithm to find very useful features.
Decision trees are especially well suited to cases where many
hierarchical categorical distinctions can be made.  For example,
decision trees can be very effective at capturing phylogeny trees.</p>
<p>However, decision trees also have a few disadvantages.  One problem is
that, since each branch in the decision tree splits the training data,
the amount of training data available to train nodes lower in the tree
can become quite small.  As a result, these lower decision nodes may</p>
<!-- XXX overfitting was already introduced as a :dt: earlier -->
<p><a name="overfit_index_term" /><span class="termdef">overfit</span> the training set, learning patterns that reflect
idiosyncrasies of the training set rather than linguistically significant
patterns in
the underlying problem.  One solution to this problem is to stop
dividing nodes once the amount of training data becomes too small.
Another solution is to grow a full decision tree, but then to
<a name="prune_index_term" /><span class="termdef">prune</span> decision nodes that do not improve performance on a
dev-test.</p>
<p>A second problem with decision trees is that they force features to be
checked in a specific order, even when features may act relatively
independently of one another.  For example, when classifying documents
into topics (such as sports, automotive, or murder mystery), features
such as <tt class="doctest"><span class="pre">hasword(football)</span></tt> are highly indicative of a specific
label, regardless of what other the feature values are.  Since there
is limited space near the top of the decision tree, most of these
features will need to be repeated on many different branches in the
tree.  And since the number of branches increases exponentially as we
go down the tree, the amount of repetition can be very large.</p>
<p>A related problem is that decision trees are not good at making use of
features that are weak predictors of the correct label.  Since these
features make relatively small incremental improvements, they tend to
occur very low in the decision tree.  But by the time the decision
tree learner has descended far enough to use these features, there is
not enough training data left to reliably determine what effect they
should have.  If we could instead look at the effect of these features
across the entire training set, then we might be able to make some
conclusions about how they should affect the choice of label.</p>
<p>The fact that decision trees require that features be checked in a
specific order limits their ability to exploit features that are
relatively independent of one another.  The naive Bayes classification
method, which we'll discuss next, overcomes this limitation by
allowing all features to act &quot;in parallel.&quot;</p>
</div>
</div>
<div class="section" id="naive-bayes-classifiers">
<h1>5&nbsp;&nbsp;&nbsp;Naive Bayes Classifiers</h1>
<p>In <a name="naive_bayes_index_term" /><span class="termdef">naive Bayes</span> classifiers, every feature gets a say in
determining which label should be assigned to a given input value.  To
choose a label for an input value, the naive Bayes classifier begins
by calculating the <a name="prior_probability_index_term" /><span class="termdef">prior probability</span> of each label, which is
determined by checking frequency of each label in the training set.
The contribution from each feature is then combined with this prior
probability, to arrive at a likelihood estimate for each label.  The
label whose likelihood estimate is the highest is then assigned to the
input value.  <a class="reference internal" href="#fig-naive-bayes-triangle">fig-naive-bayes-triangle</a> illustrates this process.</p>
<!-- I go back and forth on whether we should include a figure like this
one.  I think it gives a good high-level feeling of what's going
on, but the details don't really line up with the algorithm's
specifics, and it takes a good amount of work to explain the figure. -->
<div class="system-message" id="fig-naive-bayes-triangle">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1595)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '30:100:30')
invalid literal for int() with base 10: '30:100:30'.</p>
<pre class="literal-block">
.. figure:: ../images/naive-bayes-triangle.png
   :scale: 30:100:30

   An abstract illustration of the procedure used by the naive Bayes
   classifier to choose the topic for a document.  In the training
   corpus, most documents are automotive, so the classifier starts out
   at a point closer to the &quot;automotive&quot; label.  But it then
   considers the effect of each feature.  In this example, the input
   document contains the word &quot;dark,&quot; which is a weak indicator for
   murder mysteries, but it also contains the word &quot;football,&quot; which
   is a strong indicator for sports documents.  After every feature
   has made its contribution, the classifier checks which label it is
   closest to, and assigns that label to the input.

</pre>
</div>
<p>Individual features make their contribution to the overall decision by
&quot;voting against&quot; labels that don't occur with that feature very often.
In particular, the likelihood score for each label is reduced by
multiplying it by the probability that an input value with that label
would have the feature.  For example, if the word <span class="example">run</span> occurs in 12%
of the sports documents, 10% of the murder mystery documents, and 2%
of the automotive documents, then the likelihood score for the sports
label will be multiplied by 0.12; the likelihood score for the murder
mystery label will be multiplied by 0.1, and the likelihood score for
the automotive label will be multiplied by 0.02.  The overall effect
will be to reduce the score of the murder mystery label slightly more
than the score of the sports label, and to significantly reduce the
automotive label with respect to the other two labels.  This
process is illustrated in <a class="reference internal" href="#fig-naive-bayes-bargraph">fig-naive-bayes-bargraph</a> and
<a class="reference internal" href="#fig-naive-bayes-graph">fig-naive-bayes-graph</a>.</p>
<div class="system-message" id="fig-naive-bayes-bargraph">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1626)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '30:120:30')
invalid literal for int() with base 10: '30:120:30'.</p>
<pre class="literal-block">
.. figure:: ../images/naive_bayes_bargraph.png
   :scale: 30:120:30

   Calculating label likelihoods with naive Bayes.  Naive Bayes begins
   by calculating the prior probability of each label, based on how
   frequently each label occurs in the training data.  Every feature
   then contributes to the likelihood estimate for each label, by
   multiplying it by the probability that input values with that label
   will have that feature.  The resulting likelihood score can be
   thought of as an estimate of the probability that a randomly
   selected value from the training set would have both the given
   label and the set of features, assuming that the feature
   probabilities are all independent.

</pre>
</div>
<div class="section" id="underlying-probabilistic-model">
<h2>5.1&nbsp;&nbsp;&nbsp;Underlying Probabilistic Model</h2>
<p>Another way of understanding the naive Bayes classifier is that it
chooses the most likely label for an input, under the assumption that
every input value is generated by first choosing a class label for
that input value, and then generating each feature, entirely
independent of every other feature.  Of course, this assumption is
unrealistic; features are often highly dependent on one another.  We'll
return to some of the consequences of this assumption at the end of
this section.  This simplifying assumption, known as the
<a name="naive_bayes_assumption_index_term" /><span class="termdef">naive Bayes assumption</span> (or <a name="independence_assumption_index_term" /><span class="termdef">independence assumption</span>)
makes it much
easier to combine the contributions of the different features, since
we don't need to worry about how they should interact with one
another.</p>
<div class="system-message" id="fig-naive-bayes-graph">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1658)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '30:120:30')
invalid literal for int() with base 10: '30:120:30'.</p>
<pre class="literal-block">
.. figure:: ../images/naive_bayes_graph.png
   :scale: 30:120:30

   A `Bayesian Network Graph`:dt: illustrating the generative process
   that is assumed by the naive Bayes classifier.  To generate a
   labeled input, the model first chooses a label for the input,
   then it generates each of the input's features based on that label.
   Every feature is assumed to be entirely independent of every other
   feature, given the label.

</pre>
</div>
<p>Based on this assumption, we can calculate an expression for
<span class="math">P(label|features)</span>, the probability that an input will have a
particular label given that it has a particular set of features.  To
choose a label for a new input, we can then simply pick the label
<span class="math">l</span> that maximizes <span class="math">P(l|features)</span>.</p>
<p>To begin, we note that <span class="math">P(label|features)</span> is equal to the
probability that an input has a particular label <em>and</em> the specified
set of features, divided by the probability that it has the specified
set of features:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(2)</td><td width="15"></td><td><span class="math">P(label|features) = P(features, label)/P(features)</span></td></tr></table></p>
<p>Next, we note that <span class="math">P(features)</span> will be the same for every
choice of label, so if we are simply interested in finding the most
likely label, it suffices to calculate <span class="math">P(features, label)</span>,
which we'll call the label likelihood.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>If we want to generate a probability estimate for each
label, rather than just choosing the most likely label, then the
easiest way to compute P(features) is to simply calculate the sum
over labels of P(features, label):</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(3)</td><td width="15"></td><td><span class="math">P(features)</span> =
&#931;<sub>l in| labels</sub> <span class="math">P(features, label)</span></td></tr></table></p>
</div>
<p>The label likelihood can be expanded out as the probability of the
label times the probability of the features given the label:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(4)</td><td width="15"></td><td><span class="math">P(features, label) = P(label)</span> &#215; <span class="math">P(features|label)</span></td></tr></table></p>
<p>Furthermore, since the features are all independent of one another
(given the label), we can separate out the probability of each
individual feature:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(5)</td><td width="15"></td><td><span class="math">P(features, label)</span> = <span class="math">P(label)</span> &#215; Prod<sub>f in| features</sub><span class="math">P(f|label)`</span></td></tr></table></p>
<p>This is exactly the equation we discussed above for calculating the
label likelihood: <span class="math">P(label)</span> is the prior probability for a
given label, and each <span class="math">P(f|label)</span> is the contribution of a single
feature to the label likelihood.</p>
</div>
<div class="section" id="zero-counts-and-smoothing">
<h2>5.2&nbsp;&nbsp;&nbsp;Zero Counts and Smoothing</h2>
<p>The simplest way to calculate <span class="math">P(f|label)</span>, the contribution of a
feature <cite>f</cite> toward the label likelihood for a label <cite>label</cite>, is to
take the percentage of training instances with the given label that
also have the given feature:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(6)</td><td width="15"></td><td><span class="math">P(f|label) = count(f, label) / count(label)</span></td></tr></table></p>
<p>However, this simple approach can become problematic when a feature
<em>never</em> occurs with a given label in the training set.  In this
case, our calculated value for <span class="math">P(f|label)</span> will be zero, which will
cause the label likelihood for the given label to be zero.  Thus, the
input will never be assigned this label, regardless of how well the
other features fit the label.</p>
<p>The basic problem here is with our calculation of <span class="math">P(f|label)</span>, the
probability that an input will have a feature, given a label.  In
particular, just because we haven't seen a feature/label combination
occur in the training set, doesn't mean it's impossible for that
combination to occur.  For example, we may not have seen any murder
mystery documents that contained the word &quot;football,&quot; but we wouldn't
want to conclude that it's completely impossible for such documents to
exist.</p>
<p>Thus, although <span class="math">count(f,label)/count(label)</span> is a good estimate for
<span class="math">P(f|label)</span> when <span class="math">count(f, label)</span> is relatively high, this
estimate becomes less reliable when <span class="math">count(f)</span> becomes smaller.
Therefore, when building naive Bayes models, we usually employ
more sophisticated techniques, known as <a name="smoothing_index_term" /><span class="termdef">smoothing</span> techniques,
for calculating <span class="math">P(f|label)</span>, the probability of a feature given a
label.  For example, the <a name="expected_likelihood_estimation_index_term" /><span class="termdef">Expected Likelihood Estimation</span> for the
probability of a feature given a label basically adds 0.5 to each
<span class="math">count(f,label)</span> value, and the <a name="heldout_estimation_index_term" /><span class="termdef">Heldout Estimation</span> uses a heldout
corpus to calculate the relationship between feature frequencies and
feature probabilities.  The <tt class="doctest"><span class="pre">nltk.probability</span></tt> module provides support
for a wide variety of smoothing techniques.</p>
<!-- XXX Refer to J&M 4.5 probably just in the further reading section -->
<!-- XXX Mention that NLTK supports many smoothing methods, and name some of them -->
</div>
<div class="section" id="non-binary-features">
<h2>5.3&nbsp;&nbsp;&nbsp;Non-Binary Features</h2>
<p>We have assumed here that each feature is binary, i.e.
that each input either has a feature or does not.  Label-valued
features (e.g., a color feature which could be red, green, blue,
white, or orange) can be converted to binary features by replacing
them with binary features such as &quot;color-is-red&quot;.  Numeric features can be
converted to binary features by <a name="binning_index_term" /><span class="termdef">binning</span>, which replaces them with
features such as &quot;4&lt;x&lt;6&quot;.</p>
<p>Another alternative is to use regression methods to model the
probabilities of numeric features.  For example, if we assume that the
height feature has a bell curve distribution, then we could estimate
P(height|label) by finding the mean and variance of the heights of the
inputs with each label.  In this case, <span class="math">P(f=v|label)</span> would not
be a fixed value, but would vary depending on the value of <cite>v</cite>.</p>
</div>
<div class="section" id="the-naivete-of-independence">
<h2>5.4&nbsp;&nbsp;&nbsp;The Naivete of Independence</h2>
<p>The reason that naive Bayes classifiers are called &quot;naive&quot; is that
it's unreasonable to assume that all features are independent of one
another (given the label).  In particular, almost all real-world
problems contain features with varying degrees of dependence on one
another.  If we had to avoid any features that were dependent on one
another, it would be very difficult to construct good feature sets
that provide the required information to the machine learning
algorithm.</p>
<p>So what happens when we ignore the independence assumption, and use
the naive Bayes classifier with features that are not independent?
One problem that arises is that the classifier can end up
&quot;double-counting&quot; the effect of highly correlated features, pushing
the classifier closer to a given label than is justified.</p>
<p>To see how this can occur, consider a name gender classifier that
contains two identical features, <span class="math">f</span><sub>1</sub> and <span class="math">f</span><sub>2</sub>.  In other words, <span class="math">f</span><sub>2</sub> is an exact copy of
<span class="math">f</span><sub>1</sub>, and contains no new information.
When the classifier is considering an input, it will include the
contribution of both <span class="math">f</span><sub>1</sub> and <span class="math">f</span><sub>2</sub> when
deciding which label to choose.  Thus, the information content of
these two features will be given more weight than it deserves.</p>
<p>Of course, we don't usually build naive Bayes classifiers that contain
two identical features.  However, we do build classifiers that contain
features which are dependent on one another.  For example, the
features <tt class="doctest"><span class="pre">ends-with(a)</span></tt> and <tt class="doctest"><span class="pre">ends-with(vowel)</span></tt> are dependent on
one another, because if an input value has the first feature, then it
must also have the second feature.  For features like these, the
duplicated information may be given more weight than is justified by
the training set.</p>
</div>
<div class="section" id="the-cause-of-double-counting">
<h2>5.5&nbsp;&nbsp;&nbsp;The Cause of Double-Counting</h2>
<p>The reason for the double-counting problem is that
during training, feature contributions are computed separately;
but when using the classifier to choose labels for new inputs, those
feature contributions are combined.  One solution, therefore, is to
consider the possible interactions between feature contributions
during training.  We could then use those interactions to adjust the
contributions that individual features make.</p>
<p>To make this more precise, we can rewrite the equation used to
calculate the likelihood of a label, separating out the
contribution made by each feature (or label):</p>
<!-- .. _parameterized-naive-bayes: -->
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(7)</td><td width="15"></td><td><span class="math">P(features, label) = w[label]</span> &#215; Prod<sub>f |in| features</sub> <span class="math">w[f, label]</span></td></tr></table></p>
<p>Here, <span class="math">w[label]</span> is the &quot;starting score&quot; for a given label, and
<span class="math">w[f, label]</span> is the contribution made by a given feature
towards a label's likelihood.  We call these values <span class="math">w[label]</span>
and <span class="math">w[f, label]</span> the <a name="parameters_index_term" /><span class="termdef">parameters</span> or <a name="weights_index_term" /><span class="termdef">weights</span> for the
model.  Using the naive Bayes algorithm, we set each of these
parameters independently:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(8)</td><td width="15"></td><td><span class="math">w[label] = P(label)</span></td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(9)</td><td width="15"></td><td><span class="math">w[f, label] = P(f|label)</span></td></tr></table></p>
<p>However, in the next section, we'll look at a classifier that
considers the possible interactions between these parameters when
choosing their values.</p>
</div>
</div>
<div class="section" id="maximum-entropy-classifiers">
<h1>6&nbsp;&nbsp;&nbsp;Maximum Entropy Classifiers</h1>
<p>The <a name="maximum_entropy_index_term" /><span class="termdef">Maximum Entropy</span> classifier uses a model that is very
similar to the model employed by the naive Bayes classifier.  But rather
than using probabilities to set the model's parameters, it uses search
techniques to find a set of parameters that will maximize the
performance of the classifier.  In particular, it looks for the set of
parameters that maximizes the <a name="total_likelihood_index_term" /><span class="termdef">total likelihood</span> of the training
corpus, which is defined as:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(10)</td><td width="15"></td><td><span class="math">P(features)</span> =
&#931;<sub>x |in| corpus</sub> <span class="math">P(label(x)|features(x))</span></td></tr></table></p>
<p>Where <tt class="doctest"><span class="pre">P(label|features)</span></tt>, the probability that an input whose
features are <tt class="doctest"><span class="pre">features</span></tt> will have class label <tt class="doctest"><span class="pre">label</span></tt>, is defined as:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(11)</td><td width="15"></td><td><span class="math">P(label|features) = P(label, features) /</span>
&#931;<sub>label</sub> <span class="math">P(label, features)</span></td></tr></table></p>
<p>Because of the potentially complex interactions between the effects of
related features, there is no way to directly calculate the model
parameters that maximize the likelihood of the training set.
Therefore, Maximum Entropy classifiers choose the model parameters
using <a name="iterative_optimization_index_term" /><span class="termdef">iterative optimization</span> techniques, which initialize the
model's parameters to random values, and then repeatedly refine those
parameters to bring them closer to the optimal solution.  These
iterative optimization techniques guarantee that each refinement of
the parameters will bring them closer to the optimal values, but do
not necessarily provide a means of determining when those optimal
values have been reached.  Because the parameters for Maximum Entropy
classifiers are selected using iterative optimization techniques, they
can take a long time to learn.  This is especially true when the size
of the training set, the number of features, and the number of
labels are all large.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Some iterative optimization techniques are much faster than
others.  When training Maximum Entropy models, avoid the use of
Generalized Iterative Scaling (GIS) or Improved Iterative Scaling
(IIS), which are both considerably slower than the Conjugate
Gradient (CG) and the BFGS optimization methods.</p>
</div>
<!-- The above note could go in the further reading section with a couple
of literature references -->
<!-- For related work section:?
The use of iterative optimization techniques to find the parameters
that maximize the performance of a model is quite common in machine
learning. -->
<div class="section" id="the-maximum-entropy-model">
<h2>6.1&nbsp;&nbsp;&nbsp;The Maximum Entropy Model</h2>
<p>The Maximum Entropy classifier model is a generalization of the model
used by the naive Bayes classifier.  Like the naive Bayes model, the
Maximum Entropy classifier calculates the likelihood of each label for
a given input value by multiplying together the parameters that are
applicable for the input value and label.  The naive Bayes classifier
model defines a parameter for each label, specifying its prior
probability, and a parameter for each (feature, label) pair,
specifying the contribution of individual features towards a label's
likelihood.</p>
<p>In contrast, the Maximum Entropy classifier model leaves it up to the
user to decide what combinations of labels and features should receive
their own parameters.  In particular, it is possible to use a single
parameter to associate a feature with more than one label; or to
associate more than one feature with a given label.  This will
sometimes allow the model to &quot;generalize&quot; over some of the
differences between related labels or features.</p>
<p>Each combination of labels and features that receives its own
parameter is called a <a name="joint_feature_index_term" /><span class="termdef">joint-feature</span>.  Note that joint-features
are properties of <em>labeled</em> values, whereas (simple) features are
properties of <em>unlabeled</em> values.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In literature that describes and discusses Maximum Entropy
models, the term &quot;features&quot; often refers to
joint-features; the term &quot;contexts&quot; refers to what
we have been calling (simple) features.</p>
</div>
<p>Typically, the joint-features that are used to construct Maximum
Entropy models exactly mirror those that are used by the naive Bayes
model.  In particular, a joint-feature is defined for each label,
corresponding to <span class="math">w</span>[<span class="math">label</span>], and for each combination of
(simple) feature and label, corresponding to <span class="math">w</span>[<span class="math">f</span>,<span class="math">label</span>].
Given the joint-features for a Maximum Entropy model, the score
assigned to a label for a given input is simply the product of the
parameters associated with the joint-features that apply to that input
and label:</p>
<!-- .. _parameterized-maxent: -->
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(12)</td><td width="15"></td><td><span class="math">P(input, label) =</span> Prod<sub>joint-features(input,label)</sub> <span class="math">w[joint-feature]</span></td></tr></table></p>
</div>
<div class="section" id="maximizing-entropy">
<h2>6.2&nbsp;&nbsp;&nbsp;Maximizing Entropy</h2>
<p>The intuition that motivates Maximum Entropy classification is that we
should build a model that captures the frequencies of individual
joint-features, without making any unwarranted assumptions.
An example will help to illustrate this principle.</p>
<p>Suppose we are assigned the task of picking the correct word sense for
a given word, from a list of ten possible senses (labeled A-J).  At
first, we are not told anything more about the word or the senses.
There are many probability distributions that we could choose for the
ten senses, such as:</p>
<table border="1" class="docutils">
<colgroup>
<col width="7%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"></th>
<th class="head">A</th>
<th class="head">B</th>
<th class="head">C</th>
<th class="head">D</th>
<th class="head">E</th>
<th class="head">F</th>
<th class="head">G</th>
<th class="head">H</th>
<th class="head">I</th>
<th class="head">J</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><cite>(i)</cite></td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
<td>10%</td>
</tr>
<tr><td><cite>(ii)</cite></td>
<td>5%</td>
<td>15%</td>
<td>0%</td>
<td>30%</td>
<td>0%</td>
<td>8%</td>
<td>12%</td>
<td>0%</td>
<td>6%</td>
<td>24%</td>
</tr>
<tr><td><cite>(iii)</cite></td>
<td>0%</td>
<td>100%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
</tr>
</tbody>
<p class="caption"></p>
</table>
<p>Although any of these distributions <em>might</em> be correct, we are likely
to choose distribution <cite>(i)</cite>, because without any more information,
there is no reason to believe that any word sense is more likely than
any other.  On the other hand, distributions <cite>(ii)</cite> and <cite>(iii)</cite> reflect
assumptions that are not supported by what we know.</p>
<p>One way to capture this intuition that distribution <cite>(i)</cite> is more &quot;fair&quot;
than the other two is to invoke the concept of entropy.  In the
discussion of decision trees, we described entropy as a measure of how
&quot;disorganized&quot; a set of labels was.  In particular, if a single label
dominates then entropy is low, but if the labels are more evenly
distributed then entropy is high.  In our example, we chose
distribution <cite>(i)</cite> because its label probabilities are evenly
distributed &#8212; in other words, because its entropy is high.  In
general, the <a name="maximum_entropy_principle_index_term" /><span class="termdef">Maximum Entropy principle</span> states that, among the
distributions that are consistent with what we know, we should choose
the distribution whose entropy is highest.</p>
<p>Next, suppose that we are told that sense A appears 55% of the time.
Once again, there are many distributions that are consistent with this
new piece of information, such as:</p>
<table border="1" class="docutils">
<colgroup>
<col width="7%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"></th>
<th class="head">A</th>
<th class="head">B</th>
<th class="head">C</th>
<th class="head">D</th>
<th class="head">E</th>
<th class="head">F</th>
<th class="head">G</th>
<th class="head">H</th>
<th class="head">I</th>
<th class="head">J</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><cite>(iv)</cite></td>
<td>55%</td>
<td>45%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
</tr>
<tr><td><cite>(v)</cite></td>
<td>55%</td>
<td>5%</td>
<td>5%</td>
<td>5%</td>
<td>5%</td>
<td>5%</td>
<td>5%</td>
<td>5%</td>
<td>5%</td>
<td>5%</td>
</tr>
<tr><td><cite>(vi)</cite></td>
<td>55%</td>
<td>3%</td>
<td>1%</td>
<td>2%</td>
<td>9%</td>
<td>5%</td>
<td>0%</td>
<td>25%</td>
<td>0%</td>
<td>0%</td>
</tr>
</tbody>
<p class="caption"></p>
</table>
<p>But again, we will likely choose the distribution that makes the
fewest unwarranted assumptions &#8212; in this case, distribution <cite>(v)</cite>.</p>
<p>Finally, suppose that we are told that the word &quot;up&quot; appears in the
nearby context 10% of the time, and that when it does appear in the
context there's an 80% chance that sense A or C will be used.  In
this case, we will have a harder time coming up with an appropriate
distribution by hand; however, we can verify that the following
distribution looks appropriate:</p>
<table border="1" class="docutils">
<colgroup>
<col width="7%" />
<col width="7%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"></th>
<th class="head">&nbsp;</th>
<th class="head">A</th>
<th class="head">B</th>
<th class="head">C</th>
<th class="head">D</th>
<th class="head">E</th>
<th class="head">F</th>
<th class="head">G</th>
<th class="head">H</th>
<th class="head">I</th>
<th class="head">J</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><cite>(vii)</cite></td>
<td>+up</td>
<td>5.1%</td>
<td>0.25%</td>
<td>2.9%</td>
<td>0.25%</td>
<td>0.25%</td>
<td>0.25%</td>
<td>0.25%</td>
<td>0.25%</td>
<td>0.25%</td>
<td>0.25%</td>
</tr>
<tr><td>` `</td>
<td>-up</td>
<td>49.9%</td>
<td>4.46%</td>
<td>4.46%</td>
<td>4.46%</td>
<td>4.46%</td>
<td>4.46%</td>
<td>4.46%</td>
<td>4.46%</td>
<td>4.46%</td>
<td>4.46%</td>
</tr>
</tbody>
<p class="caption"></p>
</table>
<p>In particular, the distribution is consistent with what we know: if we
add up the probabilities in column A, we get 55%; if we add up the
probabilities of row 1, we get 10%; and if we add up the boxes for
senses A and C in the +up row, we get 8% (or 80% of the +up cases).
Furthermore, the remaining probabilities appear to be &quot;evenly
distributed.&quot;</p>
<p>Throughout this example, we have restricted ourselves to distributions
that are consistent with what we know; among these, we chose the
distribution with the highest entropy.  This is exactly what the
Maximum Entropy classifier does as well.  In particular, for each
joint-feature, the Maximum Entropy model calculates the &quot;empirical
frequency&quot; of that feature &#8212; i.e., the frequency with which it occurs
in the training set.  It then searches for the distribution which
maximizes entropy, while still predicting the correct frequency for
each joint-feature.</p>
</div>
<div class="section" id="generative-vs-conditional-classifiers">
<h2>6.3&nbsp;&nbsp;&nbsp;Generative vs Conditional Classifiers</h2>
<!-- XXX cannot parse next sentence -->
<p>An important difference between the naive Bayes classifier and the
Maximum Entropy classifier concerns the type of questions they can be
used to answer.  The naive Bayes classifier is an example of a
<a name="generative_index_term" /><span class="termdef">generative</span> classifier, which builds a model that predicts
<span class="math">P(input, label)</span>, the joint probability of a <span class="math">(input,
label)</span> pair.  As a result, generative models can be used to
answer the following questions:</p>
<ol class="arabic simple">
<li>What is the most likely label for a given input?</li>
<li>How likely is a given label for a given input?</li>
<li>What is the most likely input value?</li>
<li>How likely is a given input value?</li>
<li>How likely is a given input value with a given label?</li>
<li>What is the most likely label for an input that might have one
of two values (but we don't know which)?</li>
</ol>
<!-- XXX possibly add expressions like argmax_label P(label|input) to each
of the above questions, to make more explicit connections with the
earlier discussion -->
<p>The Maximum Entropy classifier, on the other hand, is an example of a
<a name="conditional_index_term" /><span class="termdef">conditional</span> classifier.  Conditional classifiers build models
that predict <span class="math">P(label|input)</span> &#8212; the probability of a label
<em>given</em> the input value.  Thus, conditional models can still be used
to answer questions 1 and 2.  However, conditional models can <em>not</em> be
used to answer the remaining questions 3-6.</p>
<p>In general, generative models are strictly more powerful than
conditional models, since we can calculate the conditional probability
<span class="math">P(label|input)</span> from the joint probability <span class="math">P(input,
label)</span>, but not vice versa.
However, this additional power comes at a price.  Because the model is
more powerful, it has more &quot;free parameters&quot; which need to be learned.
However, the size of the training set is fixed.  Thus, when using a
more powerful model, we end up with less data that can be used to
train each parameter's value, making it harder to find the best
parameter values.  As a result, a generative model may not do as good
a job at answering questions 1 and 2 as a conditional model, since the
conditional model can focus its efforts on those two questions.
However, if we do need answers to questions like 3-6, then we have no
choice but to use a generative model.</p>
<p>The difference between a generative model and a conditional model is
analogous to the difference between a topographical map and a picture of
a skyline.  Although the topographical map can be used to answer a wider
variety of questions, it is significantly more difficult to generate
an accurate topographical map than it is to generate an accurate skyline.</p>
<!-- I want a figure here.  But the images I used for this in the past
(on powerpoint) are probably copyrighted, so I'll need to draw/find
some new images.  They should be a side-by-side picture of a
topographical map and a skyline.  The left/right axis of each should
be labeled as (output value), the up/down axis of each should be
labeled as (probability), and the forward/back axis of the topo map
should be (input value). -->
</div>
</div>
<div class="section" id="modeling-linguistic-patterns">
<h1>7&nbsp;&nbsp;&nbsp;Modeling Linguistic Patterns</h1>
<p>Classifiers can help us to understand the linguistic patterns that
occur in natural language, by allowing us to create explicit
<a name="models_index_term" /><span class="termdef">models</span> that capture those patterns.  Typically, these models are
using supervised classification techniques, but it is also possible to
build analytically motivated models.  Either way, these explicit
models serve two important purposes: they help us to understand
linguistic patterns, and they can be used to make predictions about
new language data.</p>
<p>The extent to which explicit models can give us insights into
linguistic patterns depends largely on what kind of model is used.
Some models, such as decision trees, are relatively transparent, and
give us direct information about which factors are important in making
decisions and about which factors are related to one another.  Other
models, such as multi-level neural networks, are much more opaque.
Although it can be possible to gain insight by studying them, it
typically takes a lot more work.</p>
<p>But all explicit models can make predictions about new &quot;<a name="unseen_index_term" /><span class="termdef">unseen</span>&quot;
language data that was not included in the corpus used to build the
model.  These predictions can be evaluated to assess the accuracy of
the model.  Once a model is deemed sufficiently accurate, it can then
be used to automatically predict information about new language
data.  These predictive models can be combined into systems that
perform many useful language processing tasks, such as document
classification, automatic translation, and question answering.</p>
<div class="section" id="what-do-models-tell-us">
<h2>7.1&nbsp;&nbsp;&nbsp;What do models tell us?</h2>
<p>It's important to understand what we can learn about language from an
automatically constructed model.  One important consideration when
dealing with models of language is the distinction between descriptive
models and explanatory models.  Descriptive models capture patterns in
the data but they don't provide any information about <em>why</em> the data
contains those patterns.  For example, as we saw in <a href="#id45"><span class="problematic" id="id46">tab-absolutely_</span></a>,
the synonyms <span class="example">absolutely</span> and <span class="example">definitely</span> are not
interchangeable: we say <span class="example">absolutely adore</span> not <span class="example">definitely
adore</span>, and <span class="example">definitely prefer</span> not <span class="example">absolutely prefer</span>.
In contrast, explanatory models attempt to capture properties and
relationships that cause the linguistic patterns.  For example, we
might introduce the abstract concept of &quot;polar verb&quot;, as one that
has an extreme meaning, and categorize some verb like
<span class="example">adore</span> and <span class="example">detest</span> as polar.  Our explanatory model would
contain the constraint that <span class="example">absolutely</span> can only combine with
polar verbs, and <span class="example">definitely</span> can only combine with non-polar
verbs.  In summary, descriptive models provide information about
correlations in the data, while explanatory models go further to
postulate causal relationships.</p>
<p>Most models that are automatically constructed from a corpus are
descriptive models; in other words, they can tell us what features are
relevant to a given pattern or construction, but they can't
necessarily tell us how those features and patterns relate to one
another.  If our goal is to understand the linguistic patterns, then
we can use this information about which features are related as a
starting point for further experiments designed to tease apart the
relationships between features and patterns.  On the other hand, if
we're just interested in using the model to make predictions (e.g., as
part of a language processing system), then we can use the model to
make predictions about new data without worrying about the details
of underlying causal relationships.</p>
</div>
</div>
<div class="section" id="summary">
<h1>8&nbsp;&nbsp;&nbsp;Summary</h1>
<ul class="simple">
<li>Modeling the linguistic data found in corpora can help us to
understand linguistic patterns, and can be used to make predictions
about new language data.</li>
<li>Supervised classifiers use labeled training corpora to build models
that predict the label of an input based on specific features of
that input.</li>
<li>Supervised classifiers can perform a wide variety of
NLP tasks, including document classification, part-of-speech
tagging, sentence segmentation, dialogue act type identification,
and determining entailment relations, and many other tasks.</li>
<li>When training a supervised classifier, you should split your corpus
into three datasets: a training set for building the
classifier model; a dev-test set for helping select
and tune the model's features; and a test set for
evaluating the final model's performance.</li>
<li>When evaluating a supervised classifier, it is important that you
use fresh data, that was not included in the training or dev-test
set.  Otherwise, your evaluation results may be unrealistically
optimistic.</li>
<li>Decision trees are automatically constructed tree-structured
flowcharts that are used to assign labels to input values based on
their features.  Although they're easy to interpret, they are not
very good at handling cases where feature values interact in
determining the proper label.</li>
<li>In naive Bayes classifiers, each feature independently contributes
to the decision of which label should be used.  This allows feature
values to interact, but can be problematic when two or more features
are highly correlated with one another.</li>
<li>Maximum Entropy classifiers use a basic model that is similar to the
model used by naive Bayes; however, they employ iterative optimization
to find the set of feature weights that maximizes the probability of
the training set.</li>
<li>Most of the models that are automatically constructed from a corpus
are descriptive &#8212; they let us know which features are relevant
to a given patterns or construction, but they don't give any
information about causal relationships between those features and
patterns.</li>
</ul>
</div>
<div class="section" id="further-reading">
<h1>9&nbsp;&nbsp;&nbsp;Further Reading</h1>
<p>Please consult <tt class="doctest"><span class="pre">http://nltk.org/</span></tt> for further materials on this chapter and on how to
install external machine learning packages, such as Weka, Mallet,
TADM, and MEGAM.
For more examples of classification and machine learning with NLTK,
please see the classification HOWTOs at <tt class="doctest"><span class="pre">http://nltk.org/howto</span></tt>.</p>
<p>For a general introduction to machine learning, we recommend
<a href="#id47"><span class="problematic" id="id1">[Alpaydin2004]_</span></a>.  For a more mathematically intense introduction to
the theory of machine learning, see <a href="#id48"><span class="problematic" id="id2">[Hastie2009]_</span></a>.  Excellent books on
using machine learning techniques for NLP include <a href="#id49"><span class="problematic" id="id3">[Abney2008]_</span></a>,
<a href="#id50"><span class="problematic" id="id4">[Daelemans2005]_</span></a>, <a href="#id51"><span class="problematic" id="id5">[Feldman2007]_</span></a>, <a href="#id52"><span class="problematic" id="id6">[Segaran2007]_</span></a>, <a href="#id53"><span class="problematic" id="id7">[Weiss2004]_</span></a>.  For
more on smoothing techniques for language problems, see
<a href="#id54"><span class="problematic" id="id8">[Manning1999FSN]_</span></a>.  For more on sequence modeling, and especially
hidden Markov models, see <a href="#id55"><span class="problematic" id="id9">[Manning1999FSN]_</span></a> or <a href="#id56"><span class="problematic" id="id10">[JurafskyMartin2008]_</span></a>.
Chapter 13 of <a href="#id57"><span class="problematic" id="id11">[Manning2008IR]_</span></a> discusses the use of naive Bayes for
classifying texts.</p>
<p>Many of the machine learning algorithms discussed in this chapter are
numerically intensive, and as a result, they will run slowly when
coded naively in Python.  For information on increasing the efficiency
of numerically intensive algorithms in Python, see <a href="#id58"><span class="problematic" id="id12">[Kiusalaas2005]_</span></a>.</p>
<p>The classification techniques described in this chapter can be applied
to a very wide variety of problems.  For example, <a href="#id59"><span class="problematic" id="id13">[Agirre2007]_</span></a> uses
classifiers to perform word-sense disambiguation; and <a href="#id60"><span class="problematic" id="id14">[Melamed2001]_</span></a>
uses classifiers to create parallel texts.  Recent textbooks that
cover text classification include <a href="#id61"><span class="problematic" id="id15">[Manning2008IR]_</span></a> and <a href="#id62"><span class="problematic" id="id16">[Croft2009]_</span></a>.</p>
<p>Much of the current research in the application of machine learning
techniques to NLP problems is driven by government-sponsored
&quot;challenges,&quot; where a set of research organizations are all provided
with the same development corpus, and asked to build a system; and the
resulting systems are compared based on a reserved test set.  Examples
of these challenge competitions include CoNLL Shared Tasks, the ACE
competitions, the Recognizing Textual Entailment competitions,
and the AQUAINT competitions.  Consult <tt class="doctest"><span class="pre">http://nltk.org/</span></tt> for a
list of pointers to the webpages for these challenges.</p>
<!-- - supported ML packages
- http://www.cs.waikato.ac.nz/~ml/weka/book.html
- choosing prepositions http://itre.cis.upenn.edu/~myl/languagelog/archives/002003.html
- Jurafsky and Martin
- publications describing some of the corpora that were created for the
  purpose of training and testing classifiers

Xin Li, Dan Roth, Learning Question Classifiers: The Role of Semantic Information
http://l2r.cs.uiuc.edu/~danr/Papers/LiRo05a.pdf

Current challenge: The problem of applying models trained on one genre
to another (portability across domains; domain adaptation);
Use of unsupervised or weakly supervised approaches to exploit a small
amount of in-domain training data, or of unlabeled in-domain training data
[REFS] -->
</div>
<div class="section" id="exercises">
<h1>10&nbsp;&nbsp;&nbsp;Exercises</h1>
<ol class="arabic">
<li><p class="first">&#9788; Read up on one of the language technologies mentioned in this section, such as
word sense disambiguation, semantic role labeling, question answering, machine translation,
named entity detection.
Find out what type and quantity of annotated data is required for developing such systems.
Why do you think a large amount of data is required?</p>
</li>
<li><p class="first">&#9788; Using any of the three classifiers described in this
chapter, and any features you can think of, build the best name
gender classifier you can.  Begin by splitting the Names Corpus
into three subsets: 500 words for the test set, 500 words for the
dev-test set, and the remaining 6900 words for the training set.
Then, starting with the example name gender classifier, make
incremental improvements.  Use the dev-test set to check your
progress.  Once you are satisfied with your classifier, check its
final performance on the test set.  How does the performance on
the test set compare to the performance on the dev-test set?
Is this what you'd expect?</p>
</li>
<li><p class="first">&#9788; The Senseval 2 Corpus contains data intended to train
word-sense disambiguation classifiers.  It contains data for
four words: hard, interest, line, and serve.  Choose one of these
four words, and load the corresponding data:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import senseval
>>> instances = senseval.instances('hard.pos')
>>> size = int(len(instances) * 0.1)
>>> train_set, test_set = instances[size:], instances[:size]</td>
</tr></table></td></tr>
</table></div>
<p>Using this dataset, build a classifier that predicts the correct
sense tag for a given instance.  See the corpus HOWTO at
<tt class="doctest"><span class="pre">http://nltk.org/howto</span></tt> for information on using the instance objects
returned by the Senseval 2 Corpus.</p>
</li>
<li><p class="first">&#9788;
Using the movie review document classifier discussed in this
chapter, generate a list of the 30 features that the classifier
finds to be most informative.  Can you explain why these particular
features are informative?  Do you find any of them surprising?</p>
</li>
<li><p class="first">&#9788;
Select one of the classification tasks described in this chapter,
such as name gender detection, document classification,
part-of-speech tagging, or dialog act classification.  Using the
same training and test data, and the same feature extractor,
build three classifiers for the task: a decision tree, a naive
Bayes classifier, and a Maximum Entropy classifier.  Compare
the performance of the three classifiers on your selected task.
How do you think that your results might be different if you used
a different feature extractor?</p>
</li>
<li><p class="first">&#9788;
The synonyms <span class="example">strong</span> and <span class="example">powerful</span> pattern
differently (try combining them with <span class="example">chip</span> and <span class="example">sales</span>).
What features are relevant in this distinction?
Build a classifier that predicts when each word should be used.</p>
</li>
<li><p class="first">&#9681;
The dialog act classifier assigns labels to individual posts,
without considering the context in which the post is found.
However, dialog acts are highly dependent on context, and some
sequences of dialog act are much more likely than others.  For
example, a ynQuestion dialog act is much more likely to be
answered by a <tt class="doctest"><span class="pre">yanswer</span></tt> than by a <tt class="doctest"><span class="pre">greeting</span></tt>.  Make use of this
fact to build a consecutive classifier for labeling dialog acts.
Be sure to consider what features might be useful.  See the code
for the consecutive classifier for part-of-speech tags in
<a class="reference internal" href="#code-consecutive-pos-tagger">1.5</a> to get some ideas.</p>
</li>
<li><p class="first">&#9681;
Word features can be very useful for performing document
classification, since the words that appear in a document give a
strong indication about what its semantic content is.  However,
many words occur very infrequently, and some of the most
informative words in a document may never have occurred in our
training data.  One solution is to make use of a <a name="lexicon_index_term" /><span class="termdef">lexicon</span>,
which describes how different words relate to one another.  Using
WordNet lexicon, augment the movie review document classifier
presented in this chapter to use features that generalize the words
that appear in a document, making it more likely that they will
match words found in the training data.</p>
</li>
<li><p class="first">&#9733;
The PP Attachment Corpus is a corpus describing
prepositional phrase attachment decisions.  Each instance in the
corpus is encoded as a <tt class="doctest"><span class="pre">PPAttachment</span></tt> object:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import ppattach
>>> ppattach.attachments('training')
[PPAttachment(sent='0', verb='join', noun1='board',
              prep='as', noun2='director', attachment='V'),
 PPAttachment(sent='1', verb='is', noun1='chairman',
              prep='of', noun2='N.V.', attachment='N'),
 ...]
>>> inst = ppattach.attachments('training')[1]
>>> (inst.noun1, inst.prep, inst.noun2)
('chairman', 'of', 'N.V.')</td>
</tr></table></td></tr>
</table></div>
<p>Select only the instances where <tt class="doctest"><span class="pre">inst.attachment</span></tt> is <tt class="doctest"><span class="pre">N</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> nattach = [inst for inst in ppattach.attachments('training')
...            if inst.attachment == 'N']</td>
</tr></table></td></tr>
</table></div>
<p>Using this sub-corpus, build a classifier that attempts to predict
which preposition is used to connect a given pair of nouns.  For
example, given the pair of nouns &quot;team&quot; and &quot;researchers,&quot; the
classifier should predict the preposition &quot;of&quot;.  See the corpus
HOWTO at <tt class="doctest"><span class="pre">http://nltk.org/howto</span></tt> for more information on using the PP
attachment corpus.</p>
</li>
<li><p class="first">&#9733; Suppose you wanted to automatically generate a prose description of a scene,
and already had a word to uniquely describe each entity, such as <span class="example">the jar</span>,
and simply wanted to decide whether to use <span class="example">in</span> or <span class="example">on</span> in relating
various items, e.g. <span class="example">the book is in the cupboard</span> vs <span class="example">the book is on the shelf</span>.
Explore this issue by looking at corpus data; writing programs as needed.</p>
</li>
</ol>
<span class="target" id="ex-prepositions"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(13)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>in the car <em>versus</em> on the train</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>in town <em>versus</em> on campus</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>in the picture <em>versus</em> on the screen</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">d.</td><td width="15"></td><td>in Macbeth <em>versus</em> on Letterman</td></tr></table></p>
</td></tr></table></p>
<!-- Footer to be used in all chapters -->
<div class="admonition admonition-about-this-document">
<p class="first admonition-title">About this document...</p>
<p>UPDATED FOR NLTK 3.0.
This is a chapter from <em>Natural Language Processing with Python</em>,
by <a class="reference external" href="http://estive.net/">Steven Bird</a>, <a class="reference external" href="http://homepages.inf.ed.ac.uk/ewan/">Ewan Klein</a> and <a class="reference external" href="http://ed.loper.org/">Edward Loper</a>,
Copyright &#169; 2014 the authors.
It is distributed with the <em>Natural Language Toolkit</em> [<tt class="doctest"><span class="pre">http://nltk.org/</span></tt>],
Version 3.0, under the terms of the
<em>Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License</em>
[<a class="reference external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/us/">http://creativecommons.org/licenses/by-nc-nd/3.0/us/</a>].</p>
<p class="last">This document was built on
Wed 20 Aug 2014 11:10:48 GET</p>
</div>
</div>
<div class="system-messages section">
<h1>Docutils System Messages</h1>
<div class="system-message" id="id17">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1264); <em><a href="#id18">backlink</a></em></p>
Undefined substitution referenced: &quot;ii&quot;.</div>
<div class="system-message" id="id19">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 41); <em><a href="#id20">backlink</a></em></p>
Unknown target name: &quot;chap-tag&quot;.</div>
<div class="system-message" id="id21">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 41); <em><a href="#id22">backlink</a></em></p>
Unknown target name: &quot;chap-words&quot;.</div>
<div class="system-message" id="id23">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 119); <em><a href="#id24">backlink</a></em></p>
Unknown target name: &quot;sec-lexical-resources&quot;.</div>
<div class="system-message" id="id25">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 414); <em><a href="#id26">backlink</a></em></p>
Unknown target name: &quot;sec-extracting-text-from-corpora&quot;.</div>
<div class="system-message" id="id27">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 458); <em><a href="#id28">backlink</a></em></p>
Unknown target name: &quot;sec-algorithm-design&quot;.</div>
<div class="system-message" id="id29">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 506); <em><a href="#id30">backlink</a></em></p>
Unknown target name: &quot;chap-tag&quot;.</div>
<div class="system-message" id="id31">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 714); <em><a href="#id32">backlink</a></em></p>
Unknown target name: &quot;sec-n-gram-tagging&quot;.</div>
<div class="system-message" id="id33">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 790); <em><a href="#id34">backlink</a></em></p>
Unknown target name: &quot;sec-transformation-based-tagging&quot;.</div>
<div class="system-message" id="id35">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 800); <em><a href="#id36">backlink</a></em></p>
Unknown target name: &quot;sec-algorithm-design&quot;.</div>
<div class="system-message" id="id37">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 913); <em><a href="#id38">backlink</a></em></p>
Unknown target name: &quot;sec-extracting-text-from-corpora&quot;.</div>
<div class="system-message" id="id39">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 958); <em><a href="#id40">backlink</a></em></p>
Unknown target name: &quot;sec-automatic-natural-language-understanding&quot;.</div>
<div class="system-message" id="id41">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1190); <em><a href="#id42">backlink</a></em></p>
Unknown target name: &quot;sec-life-cycle-of-a-corpus&quot;.</div>
<div class="system-message" id="id43">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 1264); <em><a href="#id44">backlink</a></em></p>
Unknown target name: &quot;sec-automatic-tagging&quot;.</div>
<div class="system-message" id="id45">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2159); <em><a href="#id46">backlink</a></em></p>
Unknown target name: &quot;tab-absolutely&quot;.</div>
<div class="system-message" id="id47">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id1">backlink</a></em></p>
Unknown target name: &quot;alpaydin2004&quot;.</div>
<div class="system-message" id="id48">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id2">backlink</a></em></p>
Unknown target name: &quot;hastie2009&quot;.</div>
<div class="system-message" id="id49">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id3">backlink</a></em></p>
Unknown target name: &quot;abney2008&quot;.</div>
<div class="system-message" id="id50">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id4">backlink</a></em></p>
Unknown target name: &quot;daelemans2005&quot;.</div>
<div class="system-message" id="id51">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id5">backlink</a></em></p>
Unknown target name: &quot;feldman2007&quot;.</div>
<div class="system-message" id="id52">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id6">backlink</a></em></p>
Unknown target name: &quot;segaran2007&quot;.</div>
<div class="system-message" id="id53">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id7">backlink</a></em></p>
Unknown target name: &quot;weiss2004&quot;.</div>
<div class="system-message" id="id54">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id8">backlink</a></em></p>
Unknown target name: &quot;manning1999fsn&quot;.</div>
<div class="system-message" id="id55">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id9">backlink</a></em></p>
Unknown target name: &quot;manning1999fsn&quot;.</div>
<div class="system-message" id="id56">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id10">backlink</a></em></p>
Unknown target name: &quot;jurafskymartin2008&quot;.</div>
<div class="system-message" id="id57">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2252); <em><a href="#id11">backlink</a></em></p>
Unknown target name: &quot;manning2008ir&quot;.</div>
<div class="system-message" id="id58">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2263); <em><a href="#id12">backlink</a></em></p>
Unknown target name: &quot;kiusalaas2005&quot;.</div>
<div class="system-message" id="id59">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2268); <em><a href="#id13">backlink</a></em></p>
Unknown target name: &quot;agirre2007&quot;.</div>
<div class="system-message" id="id60">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2268); <em><a href="#id14">backlink</a></em></p>
Unknown target name: &quot;melamed2001&quot;.</div>
<div class="system-message" id="id61">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2268); <em><a href="#id15">backlink</a></em></p>
Unknown target name: &quot;manning2008ir&quot;.</div>
<div class="system-message" id="id62">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch06.rst</tt>, line 2268); <em><a href="#id16">backlink</a></em></p>
Unknown target name: &quot;croft2009&quot;.</div>
</div>
</div>
</body>
</html>
