<?xml version="1.0" encoding="ascii" ?>

<script language="javascript" type="text/javascript">

function astext(node)
{
    return node.innerHTML.replace(/(<([^>]+)>)/ig,"")
                         .replace(/&gt;/ig, ">")
                         .replace(/&lt;/ig, "<")
                         .replace(/&quot;/ig, '"')
                         .replace(/&amp;/ig, "&");
}

function copy_notify(node, bar_color, data)
{
    // The outer box: relative + inline positioning.
    var box1 = document.createElement("div");
    box1.style.position = "relative";
    box1.style.display = "inline";
    box1.style.top = "2em";
    box1.style.left = "1em";
  
    // A shadow for fun
    var shadow = document.createElement("div");
    shadow.style.position = "absolute";
    shadow.style.left = "-1.3em";
    shadow.style.top = "-1.3em";
    shadow.style.background = "#404040";
    
    // The inner box: absolute positioning.
    var box2 = document.createElement("div");
    box2.style.position = "relative";
    box2.style.border = "1px solid #a0a0a0";
    box2.style.left = "-.2em";
    box2.style.top = "-.2em";
    box2.style.background = "white";
    box2.style.padding = ".3em .4em .3em .4em";
    box2.style.fontStyle = "normal";
    box2.style.background = "#f0e0e0";

    node.insertBefore(box1, node.childNodes.item(0));
    box1.appendChild(shadow);
    shadow.appendChild(box2);
    box2.innerHTML="Copied&nbsp;to&nbsp;the&nbsp;clipboard: " +
                   "<pre class='copy-notify'>"+
                   data+"</pre>";
    setTimeout(function() { node.removeChild(box1); }, 1000);

    var elt = node.parentNode.firstChild;
    elt.style.background = "#ffc0c0";
    setTimeout(function() { elt.style.background = bar_color; }, 200);
}

function copy_codeblock_to_clipboard(node)
{
    var data = astext(node)+"\n";
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#40a060", data);
    }
}

function copy_doctest_to_clipboard(node)
{
    var s = astext(node)+"\n   ";
    var data = "";

    var start = 0;
    var end = s.indexOf("\n");
    while (end >= 0) {
        if (s.substring(start, start+4) == ">>> ") {
            data += s.substring(start+4, end+1);
        }
        else if (s.substring(start, start+4) == "... ") {
            data += s.substring(start+4, end+1);
        }
        /*
        else if (end-start > 1) {
            data += "# " + s.substring(start, end+1);
        }*/
        // Grab the next line.
        start = end+1;
        end = s.indexOf("\n", start);
    }
    
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#4060a0", data);
    }
}
    
function copy_text_to_clipboard(data)
{
    if (window.clipboardData) {
        window.clipboardData.setData("Text", data);
        return true;
     }
    else if (window.netscape) {
        // w/ default firefox settings, permission will be denied for this:
        netscape.security.PrivilegeManager
                      .enablePrivilege("UniversalXPConnect");
    
        var clip = Components.classes["@mozilla.org/widget/clipboard;1"]
                      .createInstance(Components.interfaces.nsIClipboard);
        if (!clip) return;
    
        var trans = Components.classes["@mozilla.org/widget/transferable;1"]
                       .createInstance(Components.interfaces.nsITransferable);
        if (!trans) return;
    
        trans.addDataFlavor("text/unicode");
    
        var str = new Object();
        var len = new Object();
    
        var str = Components.classes["@mozilla.org/supports-string;1"]
                     .createInstance(Components.interfaces.nsISupportsString);
        var datacopy=data;
        str.data=datacopy;
        trans.setTransferData("text/unicode",str,datacopy.length*2);
        var clipid=Components.interfaces.nsIClipboard;
    
        if (!clip) return false;
    
        clip.setData(trans,null,clipid.kGlobalClipboard);
        return true;
    }
    return false;
}
//-->
</script>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ascii" />
<meta name="generator" content="Docutils 0.14: http://docutils.sourceforge.net/" />
<title>7. Extracting Information from Text</title>
<--- Cannot embed stylesheet '../nltkdoc.css': No such file or directory. --->
</head>
<body>
<div class="document" id="extracting-information-from-text">
<span id="chap-chunk"></span>
<h1 class="title">7. Extracting Information from Text</h1>

<!-- -*- mode: rst -*- -->
<!-- -*- mode: rst -*- -->
<!-- CAP abbreviations (map to small caps in LaTeX) -->
<!-- Other candidates for global consistency -->
<!-- PTB removed since it must be indexed -->
<!-- WN removed since it must be indexed -->
<!-- misc & punctuation -->
<!-- cdots was unicode U+22EF but not working -->
<!-- exercise meta-tags -->
<!-- Unicode tests -->
<!-- phonetic -->
<!-- misc -->
<!-- used in Unicode section -->
<!-- arrows -->
<!-- unification stuff -->
<!-- Math & Logic -->
<!-- sets -->
<!-- Greek -->
<!-- Chinese -->
<!-- URLs -->
<!-- Python example - a snippet of code in running text -->
<!-- PlaceHolder example -  something that should be replaced by actual code -->
<!-- Linguistic eXample - cited form in running text -->
<!-- Emphasized (more declarative than just using *) -->
<!-- Grammatical Category - e.g. NP and verb as technical terms
.. role:: gc
   :class: category -->
<!-- Math expression - e.g. especially for variables -->
<!-- Textual Math expression - for words 'inside' a math environment -->
<!-- Feature (or attribute) -->
<!-- Raw LaTeX -->
<!-- Raw HTML -->
<!-- Feature-value -->
<!-- Lexemes -->
<!-- Replacements that rely on previous definitions :-) -->
<!-- standard global imports

>>> from __future__ import division
>>> import nltk, re, pprint -->
<!-- XXX variety in using nltk.foo vs nltk.chunk.foo for chunk functions -->
<!-- XXX mention somewhere that for IE precision is often more important
than recall? -->
<p>For any given question, it's likely that someone has written the
answer down somewhere.  The amount of natural language text that is
available in electronic form is truly staggering, and is increasing
every day.  However, the complexity of natural language can make it
very difficult to access the information in that text.
The state of the art in NLP is still a long way from being
able to build general-purpose representations of meaning from unrestricted text.
If we instead focus our efforts on a limited set of questions or
&quot;entity relations,&quot; such as &quot;where are different facilities located,&quot;
or &quot;who is employed by what company,&quot; we can make significant progress.
The goal of this chapter is to answer the following questions:</p>
<ol class="arabic simple">
<li>How can we build a system that extracts structured data, such as
tables, from unstructured text?</li>
<li>What are some robust methods for identifying the entities and
relationships described in a text?</li>
<li>Which corpora are appropriate for this work, and how do we use
them for training and evaluating our models?</li>
</ol>
<p>Along the way, we'll apply techniques from the last two chapters to
the problems of chunking and named-entity recognition.</p>
<div class="section" id="information-extraction">
<span id="sec-information-extraction"></span><h1>1&nbsp;&nbsp;&nbsp;Information Extraction</h1>
<p>Information comes in many shapes and sizes. One important form is
<a name="structured_data_index_term" /><span class="termdef">structured data</span>, where there is a regular and predictable
organization of entities and relationships. For example,
we might be interested in the
relation between companies and locations. Given a particular company,
we would like to be able to identify the locations where it does
business; conversely, given a location, we would like to discover
which companies do business in that location. If our data is in tabular
form, such as the example in <a class="reference internal" href="#tab-db-locations">1.1</a>, then
answering these queries is straightforward.</p>
<span class="target" id="tab-db-locations"></span><table border="1" class="docutils" id="tab-db-locations">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">OrgName</th>
<th class="head">LocationName</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Omnicom</td>
<td>New York</td>
</tr>
<tr><td>DDB Needham</td>
<td>New York</td>
</tr>
<tr><td>Kaplan Thaler Group</td>
<td>New York</td>
</tr>
<tr><td>BBDO South</td>
<td>Atlanta</td>
</tr>
<tr><td>Georgia-Pacific</td>
<td>Atlanta</td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 1.1</span>: <p>Locations data</p>
</p>
</table>
<p>If this location data was stored in Python as a list of tuples
<tt class="doctest"><span class="pre">(entity, relation, entity)</span></tt>, then the question
&quot;Which organizations operate in Atlanta?&quot; could be
translated as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> locs = [('Omnicom', 'IN', 'New York'),
...         ('DDB Needham', 'IN', 'New York'),
...         ('Kaplan Thaler Group', 'IN', 'New York'),
...         ('BBDO South', 'IN', 'Atlanta'),
...         ('Georgia-Pacific', 'IN', 'Atlanta')]
>>> query = [e1 for (e1, rel, e2) in locs if e2=='Atlanta']
>>> print(query)
['BBDO South', 'Georgia-Pacific']</td>
</tr></table></td></tr>
</table></div>
<span class="target" id="tab-db-answers"></span><table border="1" class="docutils" id="tab-db-answers">
<colgroup>
<col width="100%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">OrgName</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>BBDO South</td>
</tr>
<tr><td>Georgia-Pacific</td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 1.2</span>: <p>Companies that operate in Atlanta</p>
</p>
</table>
<p>Things are more tricky if we try to get similar information out of
text. For example, consider the
following snippet (from <tt class="doctest"><span class="pre">nltk.corpus.ieer</span></tt>, for fileid <tt class="doctest"><span class="pre">NYT19980315.0085</span></tt>).</p>
<span class="target" id="ex-ie4"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(1)</td><td width="15"></td><td>The fourth Wells account moving to another agency is the packaged
paper-products division of Georgia-Pacific Corp., which arrived at
Wells only last fall. Like Hertz and the History Channel, it is
also leaving for an Omnicom-owned agency, the BBDO South unit of
BBDO Worldwide.  BBDO South in Atlanta, which handles corporate
advertising for Georgia-Pacific, will assume additional duties for
brands like Angel Soft toilet tissue and Sparkle paper towels,
said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.</td></tr></table></p>
<p>If you read through <a class="reference internal" href="#ex-ie4">(1)</a>, you will glean the information required
to answer the example question.
But how do we get a machine to understand enough
about <a class="reference internal" href="#ex-ie4">(1)</a> to return the answers in <a class="reference internal" href="#tab-db-answers">1.2</a>?  This is
obviously a much harder task. Unlike <a class="reference internal" href="#tab-db-locations">1.1</a>, <a class="reference internal" href="#ex-ie4">(1)</a>
contains no structure that links organization names with
location names.</p>
<p>One approach to this problem involves building a very general
representation of meaning (<a href="#id6"><span class="problematic" id="id7">chap-semantics_</span></a>).
In this chapter we take a different approach,
deciding in advance that we will only look for very specific kinds of
information in text, such as the relation between organizations and
locations.  Rather than trying to
use text like <a class="reference internal" href="#ex-ie4">(1)</a> to answer the question directly,
we first convert the <a name="unstructured_data_index_term" /><span class="termdef">unstructured
data</span> of natural language sentences into the structured data of
<a class="reference internal" href="#tab-db-locations">1.1</a>. Then we reap the benefits of powerful query
tools such as SQL. This method of getting meaning from text is
called <a name="information_extraction_index_term" /><span class="termdef">Information Extraction</span>.</p>
<p>Information Extraction has many applications, including
business intelligence, resume harvesting, media analysis, sentiment detection,
patent search, and email scanning. A
particularly important area of current research involves the attempt
to extract structured data out of electronically-available scientific
literature, especially in the domain of biology and medicine.</p>
<div class="section" id="information-extraction-architecture">
<h2>1.1&nbsp;&nbsp;&nbsp;Information Extraction Architecture</h2>
<p><a class="reference internal" href="#fig-ie-architecture">fig-ie-architecture</a> shows the architecture for a simple information
extraction system.  It begins by processing a document using several
of the procedures discussed in <a href="#id8"><span class="problematic" id="id9">chap-words_</span></a> and <a href="#id10"><span class="problematic" id="id11">chap-tag_</span></a>: first,
the raw text of the document is split into sentences using a sentence
segmenter, and each sentence is further subdivided into words using a
tokenizer.  Next, each sentence is tagged with part-of-speech tags,
which will prove very helpful in the next step, <a name="named_entity_detection_index_term" /><span class="termdef">named entity
detection</span>.  In this step, we search for mentions of potentially
interesting entities in each sentence.  Finally, we use <a name="relation_detection_index_term" /><span class="termdef">relation
detection</span> to search for likely relations between different
entities in the text.</p>
<div class="system-message" id="fig-ie-architecture">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 161)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '25:32:32')
invalid literal for int() with base 10: '25:32:32'.</p>
<pre class="literal-block">
.. figure:: ../images/ie-architecture.png
   :scale: 25:32:32

   Simple Pipeline Architecture for an Information Extraction System.
   This system takes the raw text of a document as its input, and
   generates a list of ``(entity, relation, entity)`` tuples as its
   output.  For example, given a document that indicates that the
   company Georgia-Pacific is located in Atlanta, it might generate
   the tuple ``([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])``.

</pre>
</div>
<p>To perform the first three tasks, we can define a simple function that
simply connects together NLTK's default sentence segmenter
<a class="reference internal" href="#ie-segment"><span id="ref-ie-segment"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>, word tokenizer <a class="reference internal" href="#ie-tokenize"><span id="ref-ie-tokenize"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>, and part-of-speech tagger
<a class="reference internal" href="#ie-postag"><span id="ref-ie-postag"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def ie_preprocess(document):
...    sentences = nltk.sent_tokenize(document) # [_ie-segment]
...    sentences = [nltk.word_tokenize(sent) for sent in sentences] # [_ie-tokenize]
...    sentences = [nltk.pos_tag(sent) for sent in sentences] # [_ie-postag]</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Remember that our program samples assume you
begin your interactive session or your program with: <tt class="doctest"><span class="pre">import nltk, re, pprint</span></tt></p>
</div>
<p>Next, in named entity detection, we segment and label the
entities that might participate in interesting relations with one
another.  Typically, these will be definite noun phrases such as <span class="example">the
knights who say &quot;ni&quot;</span>, or proper names such as <span class="example">Monty Python</span>.
In some tasks it is useful to also consider indefinite nouns or noun
chunks, such as <cite>every student</cite> or <cite>cats</cite>,
and these do not necessarily refer to
entities in the same way as definite <tt class="doctest"><span class="pre">NP</span></tt>s and proper names.</p>
<p>Finally, in relation extraction, we search for specific patterns
between pairs of entities that occur near one another in the text, and
use those patterns to build tuples recording the relationships
between the entities.</p>
</div>
</div>
<div class="section" id="chunking">
<span id="sec-chunking"></span><h1>2&nbsp;&nbsp;&nbsp;Chunking</h1>
<p>The basic technique we will use for entity detection is
<a name="chunking_index_term" /><span class="termdef">chunking</span>, which segments and labels multi-token sequences as
illustrated in <a class="reference internal" href="#fig-chunk-segmentation">fig-chunk-segmentation</a>.  The smaller boxes show the
word-level tokenization and part-of-speech tagging, while the large
boxes show higher-level chunking.  Each of these larger boxes is
called a <a name="chunk_index_term" /><span class="termdef">chunk</span>.
Like tokenization, which omits whitespace,
chunking usually selects a subset of the tokens.
Also like tokenization, the pieces produced by a chunker do not overlap
in the source text.</p>
<div class="system-message" id="fig-chunk-segmentation">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 216)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '25:30:30')
invalid literal for int() with base 10: '25:30:30'.</p>
<pre class="literal-block">
.. figure:: ../images/chunk-segmentation.png
   :scale: 25:30:30

   Segmentation and Labeling at both the Token and Chunk Levels

</pre>
</div>
<p>In this section, we will explore chunking in some depth, beginning
with the definition and representation of chunks.  We will see regular
expression and n-gram approaches to chunking, and will develop and
evaluate chunkers using the CoNLL-2000 chunking corpus. We will then return in
<a class="reference internal" href="#sec-ner">(5)</a> and <a class="reference internal" href="#sec-relextract">6</a>
to the tasks of named entity recognition and relation extraction.</p>
<div class="section" id="noun-phrase-chunking">
<h2>2.1&nbsp;&nbsp;&nbsp;Noun Phrase Chunking</h2>
<p>We will begin by considering the task of <a name="noun_phrase_chunking_index_term" /><span class="termdef">noun phrase chunking</span>,
or <a name="np_chunking_index_term" /><span class="termdef">NP-chunking</span>, where we search for chunks corresponding to
individual noun phrases.  For example, here is some Wall Street
Journal text with <tt class="doctest"><span class="pre">NP</span></tt>-chunks marked using brackets:</p>
<span class="target" id="ex-wsj-nx"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(2)</td><td width="15"></td><td>[ The/DT market/NN ] for/IN [ system-management/NN software/NN ]
for/IN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ
enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP
Associates/NNPS ] should/MD do/VB well/RB there/RB ./.</td></tr></table></p>
<p>As we can see, <tt class="doctest"><span class="pre">NP</span></tt>-chunks are often smaller pieces than complete
noun phrases.  For example, <span class="example">the market for system-management software
for Digital's hardware</span> is a single noun phrase (containing two
nested noun phrases), but it is captured in <tt class="doctest"><span class="pre">NP</span></tt>-chunks by the
simpler chunk <span class="example">the market</span>.  One of the motivations for this
difference is that <tt class="doctest"><span class="pre">NP</span></tt>-chunks are defined so as not to contain
other <tt class="doctest"><span class="pre">NP</span></tt>-chunks.  Consequently, any
prepositional phrases or subordinate clauses that modify a nominal
will not be included in the corresponding <tt class="doctest"><span class="pre">NP</span></tt>-chunk, since they
almost certainly contain further noun phrases.</p>
<p>One of the most useful sources of information for <tt class="doctest"><span class="pre">NP</span></tt>-chunking is
part-of-speech tags.  This is one of the motivations for
performing part-of-speech tagging in our information extraction
system.  We demonstrate this approach using an example sentence that
has been part-of-speech tagged in <a class="reference internal" href="#code-chunkex">2.1</a>.  In order to create an
<tt class="doctest"><span class="pre">NP</span></tt>-chunker, we will first define a <a name="chunk_grammar_index_term" /><span class="termdef">chunk grammar</span>, consisting of rules
that indicate how sentences should be chunked.  In this case, we will
define a simple grammar with a single regular-expression rule
<a class="reference internal" href="#chunkex-grammar"><span id="ref-chunkex-grammar"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.  This rule says that an NP chunk should be formed
whenever the chunker finds an optional determiner (<tt class="doctest"><span class="pre">DT</span></tt>) followed by any
number of adjectives (<tt class="doctest"><span class="pre">JJ</span></tt>) and then a noun (<tt class="doctest"><span class="pre">NN</span></tt>).  Using this grammar,
we create a chunk parser <a class="reference internal" href="#chunkex-cp"><span id="ref-chunkex-cp"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>, and test it on our example
sentence <a class="reference internal" href="#chunkex-test"><span id="ref-chunkex-test"><img src="callouts/callout4.gif" alt="[4]" class="callout" /></span></a>.  The result is a tree, which we can either
print <a class="reference internal" href="#chunkex-print"><span id="ref-chunkex-print"><img src="callouts/callout5.gif" alt="[5]" class="callout" /></span></a>, or display graphically <a class="reference internal" href="#chunkex-draw"><span id="ref-chunkex-draw"><img src="callouts/callout6.gif" alt="[6]" class="callout" /></span></a>.</p>
<span class="target" id="code-chunkex"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> sentence = [("the", "DT"), ("little", "JJ"), ("yellow", "JJ"), # [_chunkex-sent]
... ("dog", "NN"), ("barked", "VBD"), ("at", "IN"),  ("the", "DT"), ("cat", "NN")]

>>> grammar = "NP: {<DT>?<JJ>*<NN>}" # [_chunkex-grammar]

>>> cp = nltk.RegexpParser(grammar) # [_chunkex-cp]
>>> result = cp.parse(sentence) # [_chunkex-test]
>>> print(result) # [_chunkex-print]
(S
  (NP the/DT little/JJ yellow/JJ dog/NN)
  barked/VBD
  at/IN
  (NP the/DT cat/NN))
>>> result.draw() # [_chunkex-draw] </td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_chunkex.py" type="text/x-python"><span class="caption-label">Example 2.1 (code_chunkex.py)</span></a>: <span class="caption-label">Figure 2.1</span>: Example of a Simple Regular Expression Based NP Chunker.</td></tr></p>
</table></div>
<img alt="tree_images/None-tree-13.png" class="align-top" src="tree_images/None-tree-13.png" />
</div>
<div class="section" id="tag-patterns">
<h2>2.2&nbsp;&nbsp;&nbsp;Tag Patterns</h2>
<p>The rules that make up a chunk grammar use <a name="tag_patterns_index_term" /><span class="termdef">tag patterns</span> to
describe sequences of tagged words.
A tag pattern is a sequence of part-of-speech tags delimited
using angle brackets, e.g. <tt class="doctest"><span class="pre"><DT>?<JJ>*<NN></span></tt>.  Tag patterns are
similar to regular expression patterns (<a href="#id12"><span class="problematic" id="id13">sec-regular-expressions-word-patterns_</span></a>).
Now, consider the following noun phrases from the Wall Street Journal:</p>
<pre class="literal-block">
another/DT sharp/JJ dive/NN
trade/NN figures/NNS
any/DT new/JJ policy/NN measures/NNS
earlier/JJR stages/NNS
Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP
</pre>
<p>We can match these noun phrases using a slight refinement of the first tag pattern
above, i.e. <tt class="doctest"><span class="pre"><DT>?<JJ.*>*<NN.*>+</span></tt>.  This will chunk any sequence
of tokens beginning with an optional determiner, followed by
zero or more adjectives of any type (including relative
adjectives like <tt class="doctest"><span class="pre">earlier/JJR</span></tt>), followed by one or more nouns of any
type.  However, it is easy to find many more complicated examples which
this rule will not cover:</p>
<pre class="literal-block">
his/PRP$ Mansion/NNP House/NNP speech/NN
the/DT price/NN cutting/VBG
3/CD %/NN to/TO 4/CD %/NN
more/JJR than/IN 10/CD %/NN
the/DT fastest/JJS developing/VBG trends/NNS
's/POS skill/NN
</pre>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Try to come up with tag patterns to cover these cases.
Test them using the graphical interface
<tt class="doctest"><span class="pre">nltk.app.chunkparser()</span></tt>.  Continue to refine your
tag patterns with the help of the feedback given by this tool.</p>
</div>
</div>
<div class="section" id="chunking-with-regular-expressions">
<h2>2.3&nbsp;&nbsp;&nbsp;Chunking with Regular Expressions</h2>
<p>To find the chunk structure for a given sentence, the <tt class="doctest"><span class="pre">RegexpParser</span></tt>
chunker begins with a flat structure in which no tokens are
chunked.  The chunking rules are applied in turn,
successively updating the
chunk structure.  Once all of the rules have been invoked, the
resulting chunk structure is returned.</p>
<p><a class="reference internal" href="#code-chunker1">2.2</a> shows a
simple chunk grammar consisting of two rules.  The first rule
matches an optional determiner or possessive pronoun,
zero or more adjectives, then a noun.
The second rule matches one or more proper nouns.
We also define an example sentence to be chunked <a class="reference internal" href="#code-chunker1-ex"><span id="ref-code-chunker1-ex"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>,
and run the chunker on this input <a class="reference internal" href="#code-chunker1-run"><span id="ref-code-chunker1-run"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.</p>
<span class="target" id="code-chunker1"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">grammar = r"""
  NP: {<DT|PP\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun
      {<NNP>+}                # chunk sequences of proper nouns
"""
cp = nltk.RegexpParser(grammar)
sentence = [("Rapunzel", "NNP"), ("let", "VBD"), ("down", "RP"), # [_code-chunker1-ex]
                 ("her", "PP$"), ("long", "JJ"), ("golden", "JJ"), ("hair", "NN")]</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(cp.parse(sentence)) # [_code-chunker1-run]
(S
  (NP Rapunzel/NNP)
  let/VBD
  down/RP
  (NP her/PP$ long/JJ golden/JJ hair/NN))</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_chunker1.py" type="text/x-python"><span class="caption-label">Example 2.2 (code_chunker1.py)</span></a>: <span class="caption-label">Figure 2.2</span>: Simple Noun Phrase Chunker</td></tr></p>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <tt class="doctest"><span class="pre">$</span></tt> symbol is a special character in regular
expressions, and must be backslash escaped
in order to match the tag <tt class="doctest"><span class="pre">PP$</span></tt>.</p>
</div>
<p>If a tag pattern matches at overlapping locations, the leftmost
match takes precedence.  For example, if we apply a rule that matches
two consecutive nouns to a text containing three consecutive nouns,
then only the first two nouns will be chunked:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> nouns = [("money", "NN"), ("market", "NN"), ("fund", "NN")]
>>> grammar = "NP: {<NN><NN>}  # Chunk two consecutive nouns"
>>> cp = nltk.RegexpParser(grammar)
>>> print(cp.parse(nouns))
(S (NP money/NN market/NN) fund/NN)</td>
</tr></table></td></tr>
</table></div>
<p>Once we have created the chunk for <span class="example">money market</span>, we have
removed the context that would have permitted <span class="example">fund</span> to be
included in a chunk.  This issue would have been avoided with
a more permissive chunk rule, e.g. <tt class="doctest"><span class="pre">NP: {<NN>+}</span></tt>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We have added a comment to each of our chunk rules.
These are optional; when they are present, the chunker
prints these comments as part of its tracing output.</p>
</div>
</div>
<div class="section" id="exploring-text-corpora">
<h2>2.4&nbsp;&nbsp;&nbsp;Exploring Text Corpora</h2>
<p>In <a href="#id14"><span class="problematic" id="id15">sec-tagged-corpora_</span></a> we saw how we could interrogate
a tagged corpus to extract phrases matching a particular
sequence of part-of-speech tags.  We can do the same work
more easily with a chunker, as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')
>>> brown = nltk.corpus.brown
>>> for sent in brown.tagged_sents():
...     tree = cp.parse(sent)
...     for subtree in tree.subtrees():
...         if subtree.label() == 'CHUNK': print(subtree)
...
(CHUNK combined/VBN to/TO achieve/VB)
(CHUNK continue/VB to/TO place/VB)
(CHUNK serve/VB to/TO protect/VB)
(CHUNK wanted/VBD to/TO wait/VB)
(CHUNK allowed/VBN to/TO place/VB)
(CHUNK expected/VBN to/TO become/VB)
...
(CHUNK seems/VBZ to/TO overtake/VB)
(CHUNK want/VB to/TO buy/VB)</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Encapsulate the above example inside a function <tt class="doctest"><span class="pre">find_chunks()</span></tt>
that takes a chunk string like <tt class="doctest"><span class="pre">"CHUNK: {<V.*> <TO> <V.*>}"</span></tt> as an argument.
Use it to search the corpus for several other patterns, such as four
or more nouns in a row, e.g. <tt class="doctest"><span class="pre">"NOUNS: {<N.*>{4,}}"</span></tt></p>
</div>
</div>
<div class="section" id="chinking">
<h2>2.5&nbsp;&nbsp;&nbsp;Chinking</h2>
<p>Sometimes it is easier to define what we want to <span class="emphasis">exclude</span> from
a chunk.  We can define a <a name="chink_index_term" /><span class="termdef">chink</span> to be a sequence
of tokens that is not included in a chunk.
In the following example, <tt class="doctest"><span class="pre">barked/VBD at/IN</span></tt> is a chink:</p>
<pre class="literal-block">
[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]
</pre>
<p>Chinking is the process of removing a sequence of tokens from a
chunk.  If the matching sequence of tokens spans an entire chunk, then the
whole chunk is removed; if the sequence of tokens appears in the
middle of the chunk, these tokens are removed, leaving two chunks
where there was only one before.  If the sequence is at the periphery
of the chunk, these tokens are removed, and a smaller chunk remains.
These three possibilities are illustrated in <a class="reference internal" href="#tab-chinking-example">2.1</a>.</p>
<span class="target" id="tab-chinking-example"></span><table border="1" class="docutils" id="tab-chinking-example">
<colgroup>
<col width="18%" />
<col width="29%" />
<col width="28%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">` `</th>
<th class="head">Entire chunk</th>
<th class="head">Middle of a chunk</th>
<th class="head">End of a chunk</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><em>Input</em></td>
<td>[a/DT little/JJ
dog/NN]</td>
<td>[a/DT little/JJ
dog/NN]</td>
<td>[a/DT little/JJ
dog/NN]</td>
</tr>
<tr><td><em>Operation</em></td>
<td>Chink &quot;DT JJ NN&quot;</td>
<td>Chink &quot;JJ&quot;</td>
<td>Chink &quot;NN&quot;</td>
</tr>
<tr><td><em>Pattern</em></td>
<td>}DT JJ NN{</td>
<td>}JJ{</td>
<td>}NN{</td>
</tr>
<tr><td><em>Output</em></td>
<td>a/DT little/JJ
dog/NN</td>
<td>[a/DT] little/JJ
[dog/NN]</td>
<td>[a/DT little/JJ]
dog/NN</td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 2.1</span>: <p>Three chinking rules applied to the same chunk</p>
</p>
</table>
<p>In <a class="reference internal" href="#code-chinker">2.3</a>, we put the entire sentence into a single chunk,
then excise the chinks.</p>
<span class="target" id="code-chinker"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">grammar = r"""
  NP:
    {<.*>+}          # Chunk everything
    }<VBD|IN>+{      # Chink sequences of VBD and IN
  """
sentence = [("the", "DT"), ("little", "JJ"), ("yellow", "JJ"),
       ("dog", "NN"), ("barked", "VBD"), ("at", "IN"),  ("the", "DT"), ("cat", "NN")]
cp = nltk.RegexpParser(grammar)</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(cp.parse(sentence))
 (S
   (NP the/DT little/JJ yellow/JJ dog/NN)
   barked/VBD
   at/IN
   (NP the/DT cat/NN))</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_chinker.py" type="text/x-python"><span class="caption-label">Example 2.3 (code_chinker.py)</span></a>: <span class="caption-label">Figure 2.3</span>: Simple Chinker</td></tr></p>
</table></div>
<!-- We haven't talked about using conll yet; and these results are
very far from impressive anyway: :)

>>> from nltk.corpus import conll2000
>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])
>>> print(nltk.chunk.accuracy(cp, test_sents))
 0.5810414336070245 -->
<!-- Section: "Multiple Chunk Types" was here.  I moved it to
ch07-extras because I didn't see that it added much, and it didn't
feel very motivated. -->
<!-- Section" Chunking vs Parsing" was here.  I moved it to
ch07-extras for now.  We might fold it back in later in the chapter,
or somewhere in the parsing chapter, but here seemed like an odd
place for it. -->
</div>
<div class="section" id="representing-chunks-tags-vs-trees">
<h2>2.6&nbsp;&nbsp;&nbsp;Representing Chunks: Tags vs Trees</h2>
<p>As befits their intermediate status between tagging and parsing (<a href="#id16"><span class="problematic" id="id17">chap-parse_</span></a>),
chunk structures can be represented using either tags or trees.  The most
widespread file representation uses <a name="iob_tags_index_term" /><span class="termdef">IOB tags</span>.  In this
scheme, each token is tagged with one of three special chunk tags,
<tt class="doctest"><span class="pre">I</span></tt> (inside), <tt class="doctest"><span class="pre">O</span></tt> (outside), or <tt class="doctest"><span class="pre">B</span></tt> (begin).  A token is tagged
as <tt class="doctest"><span class="pre">B</span></tt> if it marks the beginning of a chunk.  Subsequent tokens
within the chunk are tagged <tt class="doctest"><span class="pre">I</span></tt>.  All other tokens are tagged <tt class="doctest"><span class="pre">O</span></tt>.
The <tt class="doctest"><span class="pre">B</span></tt> and <tt class="doctest"><span class="pre">I</span></tt> tags are suffixed with the chunk type,
e.g. <tt class="doctest"><span class="pre">B-NP</span></tt>, <tt class="doctest"><span class="pre">I-NP</span></tt>.  Of course, it is not necessary to specify a
chunk type for tokens that appear outside a chunk, so these are just
labeled <tt class="doctest"><span class="pre">O</span></tt>. An example of this scheme is shown in <a class="reference internal" href="#fig-chunk-tagrep">fig-chunk-tagrep</a>.</p>
<div class="system-message" id="fig-chunk-tagrep">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 510)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '25:30:30')
invalid literal for int() with base 10: '25:30:30'.</p>
<pre class="literal-block">
.. figure:: ../images/chunk-tagrep.png
   :scale: 25:30:30

   Tag Representation of Chunk Structures

</pre>
</div>
<p>IOB tags have become the standard way to represent chunk structures in
files, and we will also be using this format.  Here is
how the information in <a class="reference internal" href="#fig-chunk-tagrep">fig-chunk-tagrep</a> would appear in a file:</p>
<pre class="literal-block">
We PRP B-NP
saw VBD O
the DT B-NP
yellow JJ I-NP
dog NN I-NP
</pre>
<p>In this representation there is one token per line, each with
its part-of-speech tag and chunk tag.  This format permits us
to represent more than one chunk type, so long as the chunks do not overlap.
As we saw earlier, chunk structures can also be represented using
trees.  These have the benefit that each chunk is a constituent that
can be manipulated directly.  An example is shown in <a class="reference internal" href="#fig-chunk-treerep">fig-chunk-treerep</a>.</p>
<div class="system-message" id="fig-chunk-treerep">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 533)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '25:30:30')
invalid literal for int() with base 10: '25:30:30'.</p>
<pre class="literal-block">
.. figure:: ../images/chunk-treerep.png
   :scale: 25:30:30

   Tree Representation of Chunk Structures

</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">NLTK uses trees for its internal representation of chunks, but
provides methods for reading and writing such trees to the IOB format.</p>
</div>
</div>
</div>
<div class="section" id="developing-and-evaluating-chunkers">
<h1>3&nbsp;&nbsp;&nbsp;Developing and Evaluating Chunkers</h1>
<p>Now you have a taste of what chunking does, but we haven't
explained how to evaluate chunkers.
As usual, this requires a suitably annotated corpus.
We begin by looking at the mechanics of converting IOB format into an
NLTK tree, then at how this is done on a larger scale using a
chunked corpus.  We will see how to score
the accuracy of a chunker relative to a corpus,
then look at some more data-driven ways to search for NP chunks.
Our focus throughout will be on expanding the coverage of a chunker.</p>
<!-- Section: "Developing chunkers" was here.  I moved it to
ch07-extras.  I don't think it added much. -->
<div class="section" id="reading-iob-format-and-the-conll-2000-corpus">
<h2>3.1&nbsp;&nbsp;&nbsp;Reading IOB Format and the CoNLL 2000 Corpus</h2>
<p>Using the <tt class="doctest"><span class="pre">corpus</span></tt> module we can load Wall Street Journal
text that has been tagged then chunked using the IOB notation.  The
chunk categories provided in this corpus are <tt class="doctest"><span class="pre">NP</span></tt>, <tt class="doctest"><span class="pre">VP</span></tt> and <tt class="doctest"><span class="pre">PP</span></tt>.  As we
have seen, each sentence is represented using multiple lines, as shown
below:</p>
<pre class="literal-block">
he PRP B-NP
accepted VBD B-VP
the DT B-NP
position NN I-NP
...
</pre>
<p>A conversion function <tt class="doctest"><span class="pre">chunk.conllstr2tree()</span></tt> builds a tree
representation from one of these multi-line strings.  Moreover, it
permits us to choose any subset of the three chunk types to use,
here just for <tt class="doctest"><span class="pre">NP</span></tt> chunks:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = '''
... he PRP B-NP
... accepted VBD B-VP
... the DT B-NP
... position NN I-NP
... of IN B-PP
... vice NN B-NP
... chairman NN I-NP
... of IN B-PP
... Carlyle NNP B-NP
... Group NNP I-NP
... , , O
... a DT B-NP
... merchant NN I-NP
... banking NN I-NP
... concern NN I-NP
... . . O
... '''
>>> nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()</td>
</tr></table></td></tr>
</table></div>
<div class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 600)</p>
<p>Error in &quot;tree&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '80:80:60')
invalid literal for int() with base 10: '80:80:60'.</p>
<pre class="literal-block">
.. tree:: (S (NP (PRP he))
             (VBD accepted)
             (NP (DT the) (NN position))
             (IN of)
             (NP (NN vice) (NN chairman))
             (IN of)
             (NP (NNP Carlyle) (NNP Group))
             (, ,)
             (NP (DT a) (NN merchant) (NN banking) (NN concern))
             (. .))
   :scale: 80:80:60


</pre>
</div>
<p>We can use the NLTK corpus module to access a larger amount of chunked
text.  The CoNLL 2000 corpus contains 270k words of Wall Street
Journal text, divided into &quot;train&quot; and &quot;test&quot; portions, annotated with
part-of-speech tags and chunk tags in the IOB format.  We can access
the data using <tt class="doctest"><span class="pre">nltk.corpus.conll2000</span></tt>.  Here is an
example that reads the 100th sentence of the &quot;train&quot; portion of the corpus:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import conll2000
>>> print(conll2000.chunked_sents('train.txt')[99])
(S
  (PP Over/IN)
  (NP a/DT cup/NN)
  (PP of/IN)
  (NP coffee/NN)
  ,/,
  (NP Mr./NNP Stone/NNP)
  (VP told/VBD)
  (NP his/PRP$ story/NN)
  ./.)</td>
</tr></table></td></tr>
</table></div>
<p>As you can see, the CoNLL 2000 corpus contains three chunk types:
<tt class="doctest"><span class="pre">NP</span></tt> chunks, which we have already seen; <tt class="doctest"><span class="pre">VP</span></tt> chunks such as
<span class="example">has already delivered</span>; and <tt class="doctest"><span class="pre">PP</span></tt> chunks such as <span class="example">because of</span>.
Since we are only interested in the <tt class="doctest"><span class="pre">NP</span></tt> chunks right now, we can use the
<tt class="doctest"><span class="pre">chunk_types</span></tt> argument to select them:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])
(S
  Over/IN
  (NP a/DT cup/NN)
  of/IN
  (NP coffee/NN)
  ,/,
  (NP Mr./NNP Stone/NNP)
  told/VBD
  (NP his/PRP$ story/NN)
  ./.)</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="simple-evaluation-and-baselines">
<h2>3.2&nbsp;&nbsp;&nbsp;Simple Evaluation and Baselines</h2>
<p>Now that we can access a chunked corpus, we can evaluate chunkers.
We start off by establishing a baseline for the trivial chunk parser
<tt class="doctest"><span class="pre">cp</span></tt> that creates no chunks:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import conll2000
>>> cp = nltk.RegexpParser("")
>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])
>>> print(cp.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  43.4%
    Precision:      0.0%
    Recall:         0.0%
    F-Measure:      0.0%</td>
</tr></table></td></tr>
</table></div>
<p>The IOB tag accuracy indicates that more than a third of the words are
tagged with <tt class="doctest"><span class="pre">O</span></tt>, i.e. not in an <tt class="doctest"><span class="pre">NP</span></tt> chunk.  However, since our
tagger did not find <em>any</em> chunks, its precision, recall, and f-measure
are all zero.  Now let's try a naive regular expression chunker that
looks for tags beginning with letters that are characteristic of noun phrase tags
(e.g. <tt class="doctest"><span class="pre">CD</span></tt>, <tt class="doctest"><span class="pre">DT</span></tt>, and <tt class="doctest"><span class="pre">JJ</span></tt>).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> grammar = r"NP: {<[CDJNP].*>+}"
>>> cp = nltk.RegexpParser(grammar)
>>> print(cp.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  87.7%
    Precision:     70.6%
    Recall:        67.8%
    F-Measure:     69.2%</td>
</tr></table></td></tr>
</table></div>
<p>As you can see, this approach achieves decent results.  However, we
can improve on it by adopting a more data-driven approach, where we
use the training corpus to find the chunk tag (<tt class="doctest"><span class="pre">I</span></tt>, <tt class="doctest"><span class="pre">O</span></tt>, or <tt class="doctest"><span class="pre">B</span></tt>)
that is most likely for each part-of-speech tag.  In other words, we
can build a chunker using a <em>unigram tagger</em> (<a href="#id18"><span class="problematic" id="id19">sec-automatic-tagging_</span></a>).
But rather than trying to determine the correct part-of-speech tag for
each word, we are trying to determine the correct chunk tag, given
each word's part-of-speech tag.</p>
<p>In <a class="reference internal" href="#code-unigram-chunker">3.1</a>, we define the <tt class="doctest"><span class="pre">UnigramChunker</span></tt> class, which
uses a unigram tagger to label sentences with chunk tags.  Most of the
code in this class is simply used to convert back and forth between
the chunk tree representation used by NLTK's <tt class="doctest"><span class="pre">ChunkParserI</span></tt>
interface, and the IOB representation used by the embedded tagger.
The class defines two methods: a constructor
<a class="reference internal" href="#code-unigram-chunker-constructor"><span id="ref-code-unigram-chunker-constructor"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> which is called when we build a new
UnigramChunker; and the <tt class="doctest"><span class="pre">parse</span></tt> method <a class="reference internal" href="#code-unigram-chunker-parse"><span id="ref-code-unigram-chunker-parse"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>
which is used to chunk new sentences.</p>
<span class="target" id="code-unigram-chunker"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">class UnigramChunker(nltk.ChunkParserI):
    def __init__(self, train_sents): # [_code-unigram-chunker-constructor]
        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]
                      for sent in train_sents]
        self.tagger = nltk.UnigramTagger(train_data) # [_code-unigram-chunker-buildit]

    def parse(self, sentence): # [_code-unigram-chunker-parse]
        pos_tags = [pos for (word,pos) in sentence]
        tagged_pos_tags = self.tagger.tag(pos_tags)
        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]
        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)
                     in zip(sentence, chunktags)]
        return nltk.chunk.conlltags2tree(conlltags)</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_unigram_chunker.py" type="text/x-python"><span class="caption-label">Example 3.1 (code_unigram_chunker.py)</span></a>: <span class="caption-label">Figure 3.1</span>: Noun Phrase Chunking with a Unigram Tagger</td></tr></p>
</table></div>
<p>The constructor <a class="reference internal" href="#code-unigram-chunker-constructor"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a> expects a list of
training sentences, which will be in the form of chunk trees.  It
first converts training data to a form that is suitable for training the
tagger, using <tt class="doctest"><span class="pre">tree2conlltags</span></tt> to map each chunk tree to a list of
<tt class="doctest"><span class="pre">word,tag,chunk</span></tt> triples.  It then uses that converted training data
to train a unigram tagger, and stores it in <tt class="doctest"><span class="pre">self.tagger</span></tt> for later
use.</p>
<p>The <tt class="doctest"><span class="pre">parse</span></tt> method <a class="reference internal" href="#code-unigram-chunker-parse"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></a> takes a tagged sentence
as its input, and begins by extracting the part-of-speech tags from
that sentence.  It then tags the part-of-speech tags with IOB chunk
tags, using the tagger <tt class="doctest"><span class="pre">self.tagger</span></tt> that was trained in the
constructor.  Next, it extracts the chunk tags, and combines them with
the original sentence, to yield <tt class="doctest"><span class="pre">conlltags</span></tt>.  Finally, it uses
<tt class="doctest"><span class="pre">conlltags2tree</span></tt> to convert the result back into a chunk tree.</p>
<p>Now that we have <tt class="doctest"><span class="pre">UnigramChunker</span></tt>, we can train it using the CoNLL
2000 corpus, and test its resulting performance:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])
>>> train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])
>>> unigram_chunker = UnigramChunker(train_sents)
>>> print(unigram_chunker.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  92.9%
    Precision:     79.9%
    Recall:        86.8%
    F-Measure:     83.2%</td>
</tr></table></td></tr>
</table></div>
<p>This chunker does reasonably well, achieving an overall f-measure
score of 83%.  Let's take a look at what it's learned, by using its
unigram tagger to assign a tag to each of the part-of-speech tags that
appear in the corpus:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> postags = sorted(set(pos for sent in train_sents
...                      for (word,pos) in sent.leaves()))
>>> print(unigram_chunker.tagger.tag(postags))
[('#', 'B-NP'), ('$', 'B-NP'), ("''", 'O'), ('(', 'O'), (')', 'O'),
 (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),
 ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),
 ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),
 ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),
 ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),
 ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),
 ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),
 ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),
 ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]</td>
</tr></table></td></tr>
</table></div>
<p>It has discovered that most punctuation marks occur outside of NP
chunks, with the exception of <tt class="doctest"><span class="pre">#</span></tt> and <tt class="doctest"><span class="pre">$</span></tt>, both of which are used
as currency markers.  It has also found that determiners (<tt class="doctest"><span class="pre">DT</span></tt>) and
possessives (<tt class="doctest"><span class="pre">PRP$</span></tt> and <tt class="doctest"><span class="pre">WP$</span></tt>) occur at the beginnings of NP chunks,
while noun types (<tt class="doctest"><span class="pre">NN</span></tt>, <tt class="doctest"><span class="pre">NNP</span></tt>, <tt class="doctest"><span class="pre">NNPS</span></tt>, <tt class="doctest"><span class="pre">NNS</span></tt>) mostly occur
inside of NP chunks.</p>
<!-- Commented out because we use it but don't bother to define it in
the text of the book, since it's trivial:

>>> class BigramChunker(nltk.ChunkParserI):
...     def __init__(self, train_sents):
...         train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]
...                       for sent in train_sents]
...         self.tagger = nltk.BigramTagger(train_data)
...
...     def parse(self, sentence):
...         pos_tags = [pos for (word,pos) in sentence]
...         tagged_pos_tags = self.tagger.tag(pos_tags)
...         chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]
...         conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)
...                      in zip(sentence, chunktags)]
...         return nltk.chunk.conlltags2tree(conlltags) -->
<p>Having built a unigram chunker, it is quite easy to build a bigram
chunker: we simply change the class name to <tt class="doctest"><span class="pre">BigramChunker</span></tt>, and
modify line <a class="reference internal" href="#code-unigram-chunker-buildit"><span id="ref-code-unigram-chunker-buildit"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a> in <a class="reference internal" href="#code-unigram-chunker">3.1</a>
to construct a <tt class="doctest"><span class="pre">BigramTagger</span></tt> rather than a <tt class="doctest"><span class="pre">UnigramTagger</span></tt>.
The resulting chunker has slightly higher performance than the unigram chunker:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> bigram_chunker = BigramChunker(train_sents)
>>> print(bigram_chunker.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  93.3%
    Precision:     82.3%
    Recall:        86.8%
    F-Measure:     84.5%</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="training-classifier-based-chunkers">
<h2>3.3&nbsp;&nbsp;&nbsp;Training Classifier-Based Chunkers</h2>
<p>Both the regular-expression based chunkers and the n-gram chunkers
decide what chunks to create entirely based on part-of-speech tags.
However, sometimes part-of-speech tags
are insufficient to determine how a sentence should be chunked.
For example, consider the following two statements:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(3)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.</td></tr></table></p>
</td></tr></table></p>
<p>These two sentences have the same part-of-speech tags,
yet they are chunked differently.  In the first sentence,
<span class="example">the farmer</span> and <span class="example">rice</span> are separate chunks, while the
corresponding material in the second sentence,
<span class="example">the computer monitor</span>, is a single chunk.  Clearly, we need to make
use of information about the content of the words, in addition to just
their part-of-speech tags, if we wish to maximize chunking
performance.</p>
<p>One way that we can incorporate information about the content of words
is to use a classifier-based tagger to chunk the sentence.  Like the
n-gram chunker considered in the previous section, this
classifier-based chunker will work by assigning IOB tags to the words
in a sentence, and then converting those tags to chunks.  For the
classifier-based tagger itself, we will use the same approach that we
used in <a href="#id20"><span class="problematic" id="id21">sec-supervised-classification_</span></a> to build a part-of-speech tagger.</p>
<p>The basic code for the classifier-based NP chunker is shown in
<a class="reference internal" href="#code-classifier-chunker">3.2</a>.  It consists of two classes.  The first
class <a class="reference internal" href="#consec-chunk-tagger"><span id="ref-consec-chunk-tagger"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> is almost identical to the
<tt class="doctest"><span class="pre">ConsecutivePosTagger</span></tt> class from <a href="#id22"><span class="problematic" id="id23">code-consecutive-pos-tagger_</span></a>.
The only two differences are that it calls a different feature
extractor <a class="reference internal" href="#consec-use-fe"><span id="ref-consec-use-fe"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a> and that it uses a MaxentClassifier rather
than a NaiveBayesClassifier <a class="reference internal" href="#consec-use-maxent"><span id="ref-consec-use-maxent"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>.  The second class
<a class="reference internal" href="#consec-chunker"><span id="ref-consec-chunker"><img src="callouts/callout4.gif" alt="[4]" class="callout" /></span></a> is basically a wrapper around the tagger class that
turns it into a chunker.  During training, this second class maps the
chunk trees in the training corpus into tag sequences; in the
<tt class="doctest"><span class="pre">parse()</span></tt> method, it converts the tag sequence provided by the
tagger back into a chunk tree.</p>
<span class="target" id="code-classifier-chunker"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">class ConsecutiveNPChunkTagger(nltk.TaggerI): # [_consec-chunk-tagger]

    def __init__(self, train_sents):
        train_set = []
        for tagged_sent in train_sents:
            untagged_sent = nltk.tag.untag(tagged_sent)
            history = []
            for i, (word, tag) in enumerate(tagged_sent):
                featureset = npchunk_features(untagged_sent, i, history) # [_consec-use-fe]
                train_set.append( (featureset, tag) )
                history.append(tag)
        self.classifier = nltk.MaxentClassifier.train( # [_consec-use-maxent]
            train_set, algorithm='megam', trace=0)

    def tag(self, sentence):
        history = []
        for i, word in enumerate(sentence):
            featureset = npchunk_features(sentence, i, history)
            tag = self.classifier.classify(featureset)
            history.append(tag)
        return zip(sentence, history)

class ConsecutiveNPChunker(nltk.ChunkParserI): # [_consec-chunker]
    def __init__(self, train_sents):
        tagged_sents = [[((w,t),c) for (w,t,c) in
                         nltk.chunk.tree2conlltags(sent)]
                        for sent in train_sents]
        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)

    def parse(self, sentence):
        tagged_sents = self.tagger.tag(sentence)
        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]
        return nltk.chunk.conlltags2tree(conlltags)</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_classifier_chunker.py" type="text/x-python"><span class="caption-label">Example 3.2 (code_classifier_chunker.py)</span></a>: <span class="caption-label">Figure 3.2</span>: Noun Phrase Chunking with a Consecutive Classifier</td></tr></p>
</table></div>
<!-- Pre-load megam, so we won't get random trace output when it's loaded:
>>> nltk.classify.config_megam()
[Found megam: ...] -->
<p>The only piece left to fill in is the feature extractor.  We begin by
defining a simple feature extractor which just provides the
part-of-speech tag of the current token.  Using this feature extractor, our
classifier-based chunker is very similar to the unigram chunker, as is
reflected in its performance:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def npchunk_features(sentence, i, history):
...     word, pos = sentence[i]
...     return {"pos": pos}
>>> chunker = ConsecutiveNPChunker(train_sents)
>>> print(chunker.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  92.9%
    Precision:     79.9%
    Recall:        86.7%
    F-Measure:     83.2%</td>
</tr></table></td></tr>
</table></div>
<p>We can also add a feature for the previous part-of-speech tag.  Adding this
feature allows the classifier to model interactions between adjacent
tags, and results in a chunker that is closely related to the bigram
chunker.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def npchunk_features(sentence, i, history):
...     word, pos = sentence[i]
...     if i == 0:
...         prevword, prevpos = "<START>", "<START>"
...     else:
...         prevword, prevpos = sentence[i-1]
...     return {"pos": pos, "prevpos": prevpos}
>>> chunker = ConsecutiveNPChunker(train_sents)
>>> print(chunker.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  93.6%
    Precision:     81.9%
    Recall:        87.2%
    F-Measure:     84.5%</td>
</tr></table></td></tr>
</table></div>
<p>Next, we'll try adding a feature for the current word, since we
hypothesized that word content should be useful for chunking.  We find
that this feature does indeed improve the chunker's performance,
by about 1.5 percentage points (which corresponds to about a 10%
reduction in the error rate).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def npchunk_features(sentence, i, history):
...     word, pos = sentence[i]
...     if i == 0:
...         prevword, prevpos = "<START>", "<START>"
...     else:
...         prevword, prevpos = sentence[i-1]
...     return {"pos": pos, "word": word, "prevpos": prevpos}
>>> chunker = ConsecutiveNPChunker(train_sents)
>>> print(chunker.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  94.5%
    Precision:     84.2%
    Recall:        89.4%
    F-Measure:     86.7%</td>
</tr></table></td></tr>
</table></div>
<p>Finally, we can try extending the feature extractor with a variety of
additional features, such as lookahead features <a class="reference internal" href="#chunk-fe-lookahead"><span id="ref-chunk-fe-lookahead"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>,
paired features <a class="reference internal" href="#chunk-fe-paired"><span id="ref-chunk-fe-paired"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>, and complex contextual features
<a class="reference internal" href="#chunk-fe-complex"><span id="ref-chunk-fe-complex"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>.  This last feature, called <tt class="doctest"><span class="pre">tags-since-dt</span></tt>, creates a
string describing the set of all part-of-speech tags that have been
encountered since the most recent determiner, or since the beginning
of the sentence if there is no determiner before index <tt class="doctest"><span class="pre">i</span></tt>.
.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def npchunk_features(sentence, i, history):
...     word, pos = sentence[i]
...     if i == 0:
...         prevword, prevpos = "<START>", "<START>"
...     else:
...         prevword, prevpos = sentence[i-1]
...     if i == len(sentence)-1:
...         nextword, nextpos = "<END>", "<END>"
...     else:
...         nextword, nextpos = sentence[i+1]
...     return {"pos": pos,
...             "word": word,
...             "prevpos": prevpos,
...             "nextpos": nextpos, # [_chunk-fe-lookahead]
...             "prevpos+pos": "%s+%s" % (prevpos, pos),  # [_chunk-fe-paired]
...             "pos+nextpos": "%s+%s" % (pos, nextpos),
...             "tags-since-dt": tags_since_dt(sentence, i)}  # [_chunk-fe-complex]</td>
</tr></table></td></tr>
</table></div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def tags_since_dt(sentence, i):
...     tags = set()
...     for word, pos in sentence[:i]:
...         if pos == 'DT':
...             tags = set()
...         else:
...             tags.add(pos)
...     return '+'.join(sorted(tags))</td>
</tr></table></td></tr>
</table></div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> chunker = ConsecutiveNPChunker(train_sents)
>>> print(chunker.evaluate(test_sents))
ChunkParse score:
    IOB Accuracy:  96.0%
    Precision:     88.6%
    Recall:        91.0%
    F-Measure:     89.8%</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Try adding different features to the feature extractor function
<tt class="doctest"><span class="pre">npchunk_features</span></tt>, and see if you can further improve the
performance of the NP chunker.</p>
</div>
</div>
</div>
<div class="section" id="recursion-in-linguistic-structure">
<span id="sec-recursion-in-linguistic-structure"></span><h1>4&nbsp;&nbsp;&nbsp;Recursion in Linguistic Structure</h1>
<div class="section" id="building-nested-structure-with-cascaded-chunkers">
<h2>4.1&nbsp;&nbsp;&nbsp;Building Nested Structure with Cascaded Chunkers</h2>
<p>So far, our chunk structures have been relatively flat.  Trees consist
of tagged tokens, optionally grouped under a chunk node such as
<tt class="doctest"><span class="pre">NP</span></tt>.  However, it is possible to build chunk structures of
arbitrary depth, simply by creating a multi-stage chunk grammar
containing recursive rules.  <a class="reference internal" href="#code-cascaded-chunker">4.1</a> has
patterns for noun phrases, prepositional phrases, verb phrases, and
sentences.
This is a four-stage chunk grammar, and can be used to create
structures having a depth of at most four.</p>
<!-- I changed this example grammar to use "CLAUSE" rather than "S",
since there's an "S" node that's automatically supplied by the
chunk parser.  And the fact that we have these two different "S"
nodes is confusing. -->
<span class="target" id="code-cascaded-chunker"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">grammar = r"""
  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN
  PP: {<IN><NP>}               # Chunk prepositions followed by NP
  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments
  CLAUSE: {<NP><VP>}           # Chunk NP, VP
  """
cp = nltk.RegexpParser(grammar)
sentence = [("Mary", "NN"), ("saw", "VBD"), ("the", "DT"), ("cat", "NN"),
    ("sit", "VB"), ("on", "IN"), ("the", "DT"), ("mat", "NN")]</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(cp.parse(sentence))
(S
  (NP Mary/NN)
  saw/VBD
  (CLAUSE
    (NP the/DT cat/NN)
    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_cascaded_chunker.py" type="text/x-python"><span class="caption-label">Example 4.1 (code_cascaded_chunker.py)</span></a>: <span class="caption-label">Figure 4.1</span>: A Chunker that Handles NP, PP, VP and S</td></tr></p>
</table></div>
<p>Unfortunately this result misses the <tt class="doctest"><span class="pre">VP</span></tt> headed by <span class="example">saw</span>.  It has
other shortcomings too.  Let's see what happens when we apply this
chunker to a sentence having deeper nesting.  Notice that it fails to
identify the <tt class="doctest"><span class="pre">VP</span></tt> chunk starting at <a class="reference internal" href="#saw-vbd"><span id="ref-saw-vbd"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> sentence = [("John", "NNP"), ("thinks", "VBZ"), ("Mary", "NN"),
...     ("saw", "VBD"), ("the", "DT"), ("cat", "NN"), ("sit", "VB"),
...     ("on", "IN"), ("the", "DT"), ("mat", "NN")]
>>> print(cp.parse(sentence))
(S
  (NP John/NNP)
  thinks/VBZ
  (NP Mary/NN)
  saw/VBD # [_saw-vbd]
  (CLAUSE
    (NP the/DT cat/NN)
    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))</td>
</tr></table></td></tr>
</table></div>
<p>The solution to these problems is to get the chunker to loop over its
patterns: after trying all of them, it repeats the process.
We add an optional second argument <tt class="doctest"><span class="pre">loop</span></tt> to specify the number
of times the set of patterns should be run:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> cp = nltk.RegexpParser(grammar, loop=2)
>>> print(cp.parse(sentence))
(S
  (NP John/NNP)
  thinks/VBZ
  (CLAUSE
    (NP Mary/NN)
    (VP
      saw/VBD
      (CLAUSE
        (NP the/DT cat/NN)
        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This cascading process enables us to create deep structures.  However,
creating and debugging a cascade is difficult, and there comes
a point where it is more effective to do full parsing (see <a href="#id24"><span class="problematic" id="id25">chap-parse_</span></a>).
Also, the cascading process can only produce trees of fixed depth
(no deeper than the number of stages in the cascade), and this is
insufficient for complete syntactic analysis.</p>
</div>
</div>
<div class="section" id="trees">
<h2>4.2&nbsp;&nbsp;&nbsp;Trees</h2>
<p>A <a name="tree_index_term" /><span class="termdef">tree</span> is a set of connected labeled nodes, each reachable
by a unique path from a distinguished root node.  Here's an
example of a tree (note that they are standardly drawn upside-down):</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(4)</td><td width="15"></td><td><img alt="tree_images/None-tree-14.png" class="align-top" src="tree_images/None-tree-14.png" /></td></tr></table></p>
<p>We use a 'family' metaphor to talk about the
relationships of nodes in a tree: for example, <tt class="doctest"><span class="pre">S</span></tt> is the
<a name="parent_index_term" /><span class="termdef">parent</span> of <tt class="doctest"><span class="pre">VP</span></tt>; conversely <tt class="doctest"><span class="pre">VP</span></tt> is a <a name="child_index_term" /><span class="termdef">child</span>
of <tt class="doctest"><span class="pre">S</span></tt>.  Also, since <tt class="doctest"><span class="pre">NP</span></tt> and <tt class="doctest"><span class="pre">VP</span></tt> are both
children of <tt class="doctest"><span class="pre">S</span></tt>, they are also <a name="siblings_index_term" /><span class="termdef">siblings</span>.
For convenience, there is also a text format for specifying
trees:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">(S
   (NP Alice)
   (VP
      (V chased)
      (NP
         (Det the)
         (N rabbit))))</td>
</tr></table></td></tr>
</table></div>
<p>Although we will focus on syntactic trees, trees can be used to encode
<span class="emphasis">any</span> homogeneous hierarchical structure that spans a sequence
of linguistic forms (e.g. morphological structure, discourse structure).
In the general case, leaves and node values do not have to be strings.</p>
<p>In NLTK, we create a tree by giving a node label and a list of children:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tree1 = nltk.Tree('NP', ['Alice'])
>>> print(tree1)
(NP Alice)
>>> tree2 = nltk.Tree('NP', ['the', 'rabbit'])
>>> print(tree2)
(NP the rabbit)</td>
</tr></table></td></tr>
</table></div>
<p>We can incorporate these into successively larger trees as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tree3 = nltk.Tree('VP', ['chased', tree2])
>>> tree4 = nltk.Tree('S', [tree1, tree3])
>>> print(tree4)
(S (NP Alice) (VP chased (NP the rabbit)))</td>
</tr></table></td></tr>
</table></div>
<p>Here are some of the methods available for tree objects:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(tree4[1])
(VP chased (NP the rabbit))
>>> tree4[1].label()
'VP'
>>> tree4.leaves()
['Alice', 'chased', 'the', 'rabbit']
>>> tree4[1][1][1]
'rabbit'</td>
</tr></table></td></tr>
</table></div>
<p>The bracketed representation for complex trees can be difficult to read.
In these cases, the <tt class="doctest"><span class="pre">draw</span></tt> method can be very useful.
It opens a new window, containing a graphical representation
of the tree.  The tree display window allows you to zoom in and out,
to collapse and expand subtrees, and to print the graphical
representation to a postscript file (for inclusion in a document).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tree3.draw()                           </td>
</tr></table></td></tr>
</table></div>
<img alt="../images/parse_draw.png" src="../images/parse_draw.png" />
</div>
<div class="section" id="tree-traversal">
<h2>4.3&nbsp;&nbsp;&nbsp;Tree Traversal</h2>
<p>It is standard to use a recursive function to traverse a tree.
The listing in <a class="reference internal" href="#code-traverse">4.2</a> demonstrates this.</p>
<span class="target" id="code-traverse"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def traverse(t):
    try:
        t.label()
    except AttributeError:
        print(t, end=" ")
    else:
        # Now we know that t.node is defined
        print('(', t.label(), end=" ")
        for child in t:
            traverse(child)
        print(')', end=" ")

 >>> t = nltk.Tree('(S (NP Alice) (VP chased (NP the rabbit)))')
 >>> traverse(t)
 ( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) )</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_traverse.py" type="text/x-python"><span class="caption-label">Example 4.2 (code_traverse.py)</span></a>: <span class="caption-label">Figure 4.2</span>: A Recursive Function to Traverse a Tree</td></tr></p>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We have used a technique called <a name="duck_typing_index_term" /><span class="termdef">duck typing</span> to detect that <tt class="doctest"><span class="pre">t</span></tt>
is a tree (i.e. <tt class="doctest"><span class="pre">t.label()</span></tt> is defined).</p>
</div>
</div>
</div>
<div class="section" id="named-entity-recognition">
<span id="sec-ner"></span><h1>5&nbsp;&nbsp;&nbsp;Named Entity Recognition</h1>
<p>At the start of this chapter, we briefly introduced named entities
(NEs). Named entities are definite noun phrases that
refer to specific types of individuals, such as organizations, persons,
dates, and so on. <a class="reference internal" href="#tab-ne-types">5.1</a> lists some of the more commonly used
types of NEs. These should be self-explanatory, except for &quot;Facility&quot;:
human-made artifacts in the domains of architecture and civil
engineering; and &quot;GPE&quot;: geo-political entities such as city, state/province, and country.</p>
<span class="target" id="tab-ne-types"></span><table border="1" class="docutils" id="tab-ne-types">
<colgroup>
<col width="22%" />
<col width="78%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">NE Type</th>
<th class="head">Examples</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>ORGANIZATION</td>
<td><span class="example">Georgia-Pacific Corp.</span>, <span class="example">WHO</span></td>
</tr>
<tr><td>PERSON</td>
<td><span class="example">Eddy Bonte</span>, <span class="example">President Obama</span></td>
</tr>
<tr><td>LOCATION</td>
<td><span class="example">Murray River</span>, <span class="example">Mount Everest</span></td>
</tr>
<tr><td>DATE</td>
<td><span class="example">June</span>, <span class="example">2008-06-29</span></td>
</tr>
<tr><td>TIME</td>
<td><span class="example">two fifty a m</span>, <span class="example">1:30 p.m.</span></td>
</tr>
<tr><td>MONEY</td>
<td><span class="example">175 million Canadian Dollars</span>, <span class="example">GBP 10.40</span></td>
</tr>
<tr><td>PERCENT</td>
<td><span class="example">twenty pct</span>, <span class="example">18.75 %</span></td>
</tr>
<tr><td>FACILITY</td>
<td><span class="example">Washington Monument</span>, <span class="example">Stonehenge</span></td>
</tr>
<tr><td>GPE</td>
<td><span class="example">South East Asia</span>, <span class="example">Midlothian</span></td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 5.1</span>: <p>Commonly Used Types of Named Entity</p>
</p>
</table>
<p>The goal of a <a name="named_entity_recognition_index_term" /><span class="termdef">named entity recognition</span> (NER) system is to identify all
textual mentions of the named entities. This can be broken down into
two sub-tasks: identifying the boundaries of the NE, and identifying its
type.
While named entity recognition is frequently a prelude to identifying
relations in Information Extraction, it can also contribute to other
tasks.  For example, in Question Answering (QA), we try to improve the
precision of Information Retrieval by recovering not whole pages, but
just those parts which contain an answer to the user's question. Most
QA systems take the documents returned by standard Information
Retrieval, and then attempt to isolate the minimal text snippet in the
document containing the answer. Now suppose the question was <span class="example">Who was
the first President of the US?</span>, and one of the documents that was
retrieved contained the following passage:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(5)</td><td width="15"></td><td>The Washington Monument is the most prominent structure in
Washington, D.C. and one of the city's early attractions.  It was
built in honor of George Washington, who led the country to
independence and then became its first President.</td></tr></table></p>
<p>Analysis of the question leads us to expect that an answer should be
of the form  <span class="example">X was the first President of the US</span>, where <cite>X</cite>
is not only a noun phrase, but also refers to a named entity of type
PERSON. This should allow us to ignore the first sentence in the
passage.  While it contains two occurrences of <span class="example">Washington</span>,
named entity recognition should tell us that neither of them
has the correct type.</p>
<p>How do we go about identifying named entities?  One option would be to
look up each word in an appropriate list of names.
For example, in the case of locations, we could use a <a name="gazetteer_index_term" /><span class="termdef">gazetteer</span>,
or geographical dictionary, such as the Alexandria Gazetteer or the
Getty Gazetteer.  However, doing this
blindly runs into problems, as shown in <a class="reference internal" href="#fig-locations">fig-locations</a>.</p>
<div class="system-message" id="fig-locations">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1265)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '25:25:30')
invalid literal for int() with base 10: '25:25:30'.</p>
<pre class="literal-block">
.. figure:: ../images/locations.png
   :scale: 25:25:30

   Location Detection by Simple Lookup for a News Story: Looking up every
   word in a gazetteer is error-prone; case distinctions may help, but
   these are not always present.

</pre>
</div>
<p>Observe that the gazetteer has good coverage of locations in many countries,
and incorrectly finds locations like Sanchez in the Dominican Republic
and On in Vietnam.
Of course we could omit such locations from the gazetteer, but then we won't
be able to identify them when they do appear in a document.</p>
<p>It gets even harder in the case of names for people or organizations.
Any list of such names will probably have poor coverage. New organizations
come into existence every day, so if we are trying to deal
with contemporary newswire or blog entries, it is unlikely that
we will be able to recognize many of the entities using gazetteer lookup.</p>
<p>Another major source of difficulty is caused by the fact that many
named entity terms are ambiguous. Thus
<span class="example">May</span> and <span class="example">North</span> are likely to be parts of named entities for DATE
and LOCATION, respectively, but could both be part of a PERSON;
conversely <span class="example">Christian Dior</span> looks like a PERSON but is more
likely to be of type ORGANIZATION. A term like <span class="example">Yankee</span> will be
ordinary modifier in some contexts, but will be marked as an entity of
type ORGANIZATION in the phrase <span class="example">Yankee infielders</span>.</p>
<p>Further challenges are posed by multi-word names like
<span class="example">Stanford University</span>, and by names that contain other names
such as <span class="example">Cecil H. Green Library</span> and <span class="example">Escondido Village Conference
Service Center</span>. In named entity recognition, therefore, we need
to be able to identify the beginning and end of multi-token
sequences.</p>
<p>Named entity recognition is a task that is well-suited to the type of
classifier-based approach that we saw for noun phrase chunking.  In
particular, we can build a tagger that labels each word in a sentence
using the IOB format, where chunks are labeled by their appropriate type.
Here is part of the CONLL 2002 (<tt class="doctest"><span class="pre">conll2002</span></tt>) Dutch training data:</p>
<pre class="literal-block">
Eddy N B-PER
Bonte N I-PER
is V O
woordvoerder N O
van Prep O
diezelfde Pron O
Hogeschool N B-ORG
. Punc O
</pre>
<p>In this representation, there is one token per line, each with its
part-of-speech tag and its named entity tag.  Based on this training
corpus, we can construct a tagger that can be used to label new
sentences; and use the <tt class="doctest"><span class="pre">nltk.chunk.conlltags2tree()</span></tt> function to
convert the tag sequences into a chunk tree.</p>
<p>NLTK provides a classifier that has already been trained to recognize named entities,
accessed with the function <tt class="doctest"><span class="pre">nltk.ne_chunk()</span></tt>.  If we set the
parameter <tt class="doctest"><span class="pre">binary=True</span></tt> <a class="reference internal" href="#binary-ne"><span id="ref-binary-ne"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>, then named entities are just
tagged as <tt class="doctest"><span class="pre">NE</span></tt>; otherwise, the classifier adds category labels such
as PERSON, ORGANIZATION, and GPE.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> sent = nltk.corpus.treebank.tagged_sents()[22]
>>> print(nltk.ne_chunk(sent, binary=True)) # [_binary-ne] 
(S
  The/DT
  (NE U.S./NNP)
  is/VBZ
  one/CD
  ...
  according/VBG
  to/TO
  (NE Brooke/NNP T./NNP Mossman/NNP)
  ...)</td>
</tr></table></td></tr>
</table></div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(nltk.ne_chunk(sent)) 
(S
  The/DT
  (GPE U.S./NNP)
  is/VBZ
  one/CD
  ...
  according/VBG
  to/TO
  (PERSON Brooke/NNP T./NNP Mossman/NNP)
  ...)</td>
</tr></table></td></tr>
</table></div>
<!-- Overview of ACE here: http://www.nist.gov/speech/tests/ace/2000/doc/ace-tides00/ -->
<!-- Leaving this out: I don't see that we need it really.

.. XXX change to "the NE classifier is?

.. XXX the following wants a normal citation.  (Would we ever say that a
   book was not freely available, but that you could buy it from XYZ
   publisher?)

   Both chunkers are trained based on the ACE-2 corpus.  This
   corpus is not freely available, but a license to use the corpus can be
   purchased from the Linguistic Data Consortium (catalog id LDC2003T11). -->
</div>
<div class="section" id="relation-extraction">
<span id="sec-relextract"></span><h1>6&nbsp;&nbsp;&nbsp;Relation Extraction</h1>
<!-- XXX next para introduces regexp zero-width assertions -->
<p>Once named entities have been identified in a text, we then want to extract
the relations that exist between them. As indicated earlier, we will
typically be looking for relations between specified types of
named entity. One way of approaching this task is to initially look for all
triples of the form (<em>X</em>, &#945;, <em>Y</em>), where <em>X</em> and <em>Y</em> are named entities
of the required types, and &#945; is the string of words that
intervenes between <em>X</em> and <em>Y</em>. We can then use regular expressions to
pull out just those instances of &#945; that express the relation
that we are looking for. The following example searches for strings
that contain the word <span class="example">in</span>. The special regular expression
<tt class="doctest"><span class="pre">(?!\b.+ing\b)</span></tt> is a negative lookahead assertion that allows us to
disregard strings such as <span class="example">success in supervising the transition
of</span>, where <span class="example">in</span> is followed by a gerund.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> IN = re.compile(r'.*\bin\b(?!\b.+ing)')
>>> for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):
...     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,
...                                      corpus='ieer', pattern = IN):
...         print(nltk.sem.rtuple(rel))
[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']
[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']
[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']
[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']
[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']
[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']
[ORG: 'WGBH'] 'in' [LOC: 'Boston']
[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']
[ORG: 'Omnicom'] 'in' [LOC: 'New York']
[ORG: 'DDB Needham'] 'in' [LOC: 'New York']
[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']
[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']
[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']</td>
</tr></table></td></tr>
</table></div>
<p>Searching for the keyword <span class="example">in</span> works reasonably well,
though it will also retrieve false positives such as <tt class="doctest"><span class="pre">[ORG: House
Transportation Committee] , secured the most money in the [LOC: New
York]</span></tt>; there is unlikely to be simple string-based method of
excluding filler strings such as this.</p>
<!-- TODO fix processing of tagged corpora -->
<p>As shown above, the <tt class="doctest"><span class="pre">conll2002</span></tt> Dutch corpus contains not just named entity
annotation but also part-of-speech tags. This allows us to devise
patterns that are sensitive to these tags, as shown in the next
example. The method <tt class="doctest"><span class="pre">clause()</span></tt> prints out the relations in a
clausal form, where the binary relation symbol is specified as the
value of parameter <tt class="doctest"><span class="pre">relsym</span></tt> <a class="reference internal" href="#relsym"><span id="ref-relsym"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import conll2002
>>> vnv = """
... (
... is/V|    # 3rd sing present and
... was/V|   # past forms of the verb zijn ('be')
... werd/V|  # and also present
... wordt/V  # past of worden ('become)
... )
... .*       # followed by anything
... van/Prep # followed by van ('of')
... """
>>> VAN = re.compile(vnv, re.VERBOSE)
>>> for doc in conll2002.chunked_sents('ned.train'):
...     for rel in nltk.sem.extract_rels('PER', 'ORG', doc,
...                                    corpus='conll2002', pattern=VAN):
...         print(nltk.sem.clause(rel, relsym="VAN")) # [_relsym]
VAN("cornet_d'elzius", 'buitenlandse_handel')
VAN('johan_rottiers', 'kardinaal_van_roey_instituut')
VAN('annie_lennox', 'eurythmics')</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong> Replace the last line <a class="reference internal" href="#relsym"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>, by
<tt class="doctest"><span class="pre">print(nltk.rtuple(rel, lcon=True, rcon=True))</span></tt>. This will show you
the actual words that intervene between the two NEs and
also their left and right context, within a default 10-word
window. With the help of a Dutch dictionary, you might be able to
figure out why the result <tt class="doctest"><span class="pre">VAN('annie_lennox', 'eurythmics')</span></tt> is
a false hit.</p>
</div>
<!-- This is too weak to include:

Message Understanding
- - - - - - - - - - - - - - - - - - - - -

A message understanding system
will extract salient chunks of text from a news story and populate a
database.

.. figure:: ../images/chunk-muc.png
   :scale: 60

   The Message Understanding Process (from Abney 1996)

Consider the units that have been selected in this process:
a name (``Garcia Alvarado``), a verb cluster (``was killed``),
a locative prepositional phrase (``on his vehicle``).  These
are examples of ``NP``, ``VP`` and ``PP`` chunks. -->
<!-- XXX there shouldn't be a conclusion here, but a bulletted summary -->
</div>
<div class="section" id="summary">
<h1>7&nbsp;&nbsp;&nbsp;Summary</h1>
<ul class="simple">
<li>Information extraction systems search large bodies of unrestricted
text for specific types of entities and relations, and use them to
populate well-organized databases.  These databases can then be used
to find answers for specific questions.</li>
<li>The typical architecture for an information extraction system begins
by segmenting, tokenizing, and part-of-speech tagging the text.
The resulting data is then searched for specific types of entity.
Finally, the information extraction system looks at entities that
are mentioned near one another in the text, and tries to determine
whether specific relationships hold between those entities.</li>
<li>Entity recognition is often performed using chunkers, which
segment multi-token sequences, and label them with the appropriate
entity type.  Common entity types include ORGANIZATION, PERSON,
LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity).</li>
<li>Chunkers can be constructed using rule-based systems, such as the
<tt class="doctest"><span class="pre">RegexpParser</span></tt> class provided by NLTK; or using machine learning
techniques, such as the <tt class="doctest"><span class="pre">ConsecutiveNPChunker</span></tt> presented in this
chapter.  In either case, part-of-speech tags are often a very
important feature when searching for chunks.</li>
<li>Although chunkers are specialized to create relatively flat
data structures, where no two chunks are allowed to overlap,
they can be cascaded together to build nested structures.</li>
<li>Relation extraction can be performed using either rule-based
systems which typically look for specific patterns in the
text that connect entities and the intervening words; or using
machine-learning systems which typically attempt to learn
such patterns automatically from a training corpus.</li>
</ul>
</div>
<div class="section" id="further-reading">
<h1>8&nbsp;&nbsp;&nbsp;Further Reading</h1>
<p>Extra materials for this chapter are posted at <tt class="doctest"><span class="pre">http://nltk.org/</span></tt>,
including links to freely available resources on the web.
For more examples of chunking with NLTK, please see the
Chunking HOWTO at <tt class="doctest"><span class="pre">http://nltk.org/howto</span></tt>.</p>
<p>The popularity of chunking is due in great part to pioneering work by
Abney e.g., <a href="#id26"><span class="problematic" id="id1">[Abney1996PST]_</span></a>. Abney's Cass chunker is described in
<tt class="doctest"><span class="pre">http://www.vinartus.net/spa/97a.pdf</span></tt>.</p>
<p>The word <a name="chink_index_term_2" /><span class="termdef">chink</span> initially meant a sequence of stopwords,
according to a 1975 paper by Ross and Tukey <a href="#id27"><span class="problematic" id="id2">[Abney1996PST]_</span></a>.</p>
<p>The IOB format (or sometimes  <a name="bio_format_index_term" /><span class="termdef">BIO Format</span>) was developed for
<tt class="doctest"><span class="pre">NP</span></tt> chunking by <a href="#id28"><span class="problematic" id="id3">[Ramshaw1995TCU]_</span></a>, and was used for the shared <tt class="doctest"><span class="pre">NP</span></tt>
bracketing task run by the <em>Conference on Natural Language Learning</em>
(CoNLL) in 1999.  The same format was
adopted by CoNLL 2000 for annotating a section of Wall Street
Journal text as part of a shared task on <tt class="doctest"><span class="pre">NP</span></tt> chunking.</p>
<p>Section 13.5 of <a href="#id29"><span class="problematic" id="id4">[JurafskyMartin2008]_</span></a> contains a discussion of chunking.
Chapter 22 covers information extraction, including named entity recognition.
For information about text mining in biology and medicine, see
<a href="#id30"><span class="problematic" id="id5">[Ananiadou2006]_</span></a>.</p>
<!-- Alexandria Gazetteer: http://www.alexandria.ucsb.edu/gazetteer -->
<!-- Getty Gazetteer -->
<!-- Other topics that use an IE approach: anaphora resolution -->
<!-- [Mitkov2002]_, question answering [Pasca2003]_. -->
</div>
<div class="section" id="exercises">
<h1>9&nbsp;&nbsp;&nbsp;Exercises</h1>
<ol class="arabic simple">
<li>&#9788; The IOB format categorizes tagged tokens as <tt class="doctest"><span class="pre">I</span></tt>,
<tt class="doctest"><span class="pre">O</span></tt> and <tt class="doctest"><span class="pre">B</span></tt>.  Why are three tags necessary?  What
problem would be caused if we used <tt class="doctest"><span class="pre">I</span></tt> and <tt class="doctest"><span class="pre">O</span></tt> tags
exclusively?</li>
<li>&#9788; Write a tag pattern to match noun phrases containing plural head nouns,
e.g. &quot;many/JJ researchers/NNS&quot;, &quot;two/CD weeks/NNS&quot;, &quot;both/DT new/JJ positions/NNS&quot;.
Try to do this by generalizing the tag pattern that handled singular
noun phrases.</li>
<li>&#9788;
Pick one of the three chunk types in the CoNLL corpus.
Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences
that make up this kind of chunk.  Develop a simple chunker using
the regular expression chunker <tt class="doctest"><span class="pre">nltk.RegexpParser</span></tt>.
Discuss any tag sequences that are difficult to chunk reliably.</li>
<li>&#9788;
An early definition of <em>chunk</em> was the material that occurs between chinks.
Develop a chunker that starts by putting the whole sentence in a single
chunk, and then does the rest of its work solely by chinking.
Determine which tags (or tag sequences) are most likely to make up chinks
with the help of your own utility program.  Compare the performance and
simplicity of this approach relative to a chunker based entirely on
chunk rules.</li>
<li>&#9681; Write a tag pattern to cover noun phrases that contain gerunds,
e.g. &quot;the/DT receiving/VBG end/NN&quot;, &quot;assistant/NN managing/VBG editor/NN&quot;.
Add these patterns to the grammar, one per line.  Test your work using
some tagged sentences of your own devising.</li>
<li>&#9681; Write one or more tag patterns to handle coordinated noun phrases,
e.g. &quot;July/NNP and/CC August/NNP&quot;,
&quot;all/DT your/PRP$ managers/NNS and/CC supervisors/NNS&quot;,
&quot;company/NN courts/NNS and/CC adjudicators/NNS&quot;.</li>
<li>&#9681; Carry out the following evaluation tasks for
any of the chunkers you have developed earlier.
(Note that most chunking corpora contain some internal
inconsistencies, such that any reasonable rule-based approach
will produce errors.)<ol class="loweralpha">
<li>Evaluate your chunker on 100 sentences from a chunked corpus,
and report the precision, recall and F-measure.</li>
<li>Use the <tt class="doctest"><span class="pre">chunkscore.missed()</span></tt> and <tt class="doctest"><span class="pre">chunkscore.incorrect()</span></tt>
methods to identify the errors made by your chunker.  Discuss.</li>
<li>Compare the performance of your chunker to the baseline chunker
discussed in the evaluation section of this chapter.</li>
</ol>
</li>
<li>&#9681;
Develop a chunker for one of the chunk types in the CoNLL corpus using a
regular-expression based chunk grammar <tt class="doctest"><span class="pre">RegexpChunk</span></tt>.  Use any
combination of rules for chunking, chinking, merging or splitting.</li>
<li>&#9681; Sometimes a word is incorrectly tagged, e.g. the head noun in
&quot;12/CD or/CC so/RB cases/VBZ&quot;.  Instead of requiring manual correction of
tagger output, good chunkers are able to work with the erroneous
output of taggers.  Look for other examples of correctly chunked
noun phrases with incorrect tags.</li>
<li>&#9681;
The bigram chunker scores about 90% accuracy.
Study its errors and try to work out why it doesn't get 100% accuracy.
Experiment with trigram chunking.  Are you able to improve the performance any more?</li>
<li>&#9733;
Apply the n-gram and Brill tagging methods to IOB chunk tagging.
Instead of assigning POS tags to words, here we will assign IOB tags
to the POS tags.  E.g. if the tag <tt class="doctest"><span class="pre">DT</span></tt> (determiner) often occurs
at the start of a chunk, it will be tagged <tt class="doctest"><span class="pre">B</span></tt> (begin).  Evaluate
the performance of these chunking methods relative to the regular
expression chunking methods covered in this chapter.</li>
<li>&#9733;
We saw in <a href="#id31"><span class="problematic" id="id32">chap-tag_</span></a> that it is possible to establish
an upper limit to tagging performance by looking for ambiguous n-grams,
n-grams that are tagged in more than one possible way in the training data.
Apply the same method to determine an upper bound on the performance
of an n-gram chunker.</li>
<li>&#9733;
Pick one of the three chunk types in the CoNLL corpus.  Write functions
to do the following tasks for your chosen type:<ol class="loweralpha">
<li>List all the tag sequences that occur with each instance of this chunk type.</li>
<li>Count the frequency of each tag sequence, and produce a ranked list in
order of decreasing frequency; each line should consist of an integer (the frequency)
and the tag sequence.</li>
<li>Inspect the high-frequency tag sequences.  Use these as the basis for
developing a better chunker.</li>
</ol>
</li>
<li>&#9733;
The baseline chunker presented in the evaluation section tends to
create larger chunks than it should.  For example, the
phrase:
<tt class="doctest"><span class="pre">[every/DT time/NN] [she/PRP] sees/VBZ [a/DT newspaper/NN]</span></tt>
contains two consecutive chunks, and our baseline chunker will
incorrectly combine the first two: <tt class="doctest"><span class="pre">[every/DT time/NN she/PRP]</span></tt>.
Write a program that finds which of these chunk-internal tags
typically occur at the start of a chunk, then
devise one or more rules that will split up these chunks.
Combine these with the existing baseline chunker and
re-evaluate it, to see if you have discovered an improved baseline.</li>
<li>&#9733;
Develop an <tt class="doctest"><span class="pre">NP</span></tt> chunker that converts POS-tagged text into a list of
tuples, where each tuple consists of a verb followed by a sequence of
noun phrases and prepositions,
e.g. <tt class="doctest"><span class="pre">the little cat sat on the mat</span></tt> becomes <tt class="doctest"><span class="pre">('sat', 'on', 'NP')</span></tt>...</li>
<li>&#9733;
The Penn Treebank contains a section of tagged Wall Street Journal text
that has been chunked into noun phrases.  The format uses square brackets,
and we have encountered it several times during this chapter.
The Treebank corpus can be accessed using:
<tt class="doctest"><span class="pre">for sent in nltk.corpus.treebank_chunk.chunked_sents(fileid)</span></tt>.  These are flat trees,
just as we got using <tt class="doctest"><span class="pre">nltk.corpus.conll2000.chunked_sents()</span></tt>.<ol class="loweralpha">
<li>The functions <tt class="doctest"><span class="pre">nltk.tree.pprint()</span></tt> and <tt class="doctest"><span class="pre">nltk.chunk.tree2conllstr()</span></tt>
can be used to create Treebank and IOB strings from a tree.
Write functions <tt class="doctest"><span class="pre">chunk2brackets()</span></tt> and <tt class="doctest"><span class="pre">chunk2iob()</span></tt> that take a single
chunk tree as their sole argument, and return the required multi-line string
representation.</li>
<li>Write command-line conversion utilities <tt class="doctest"><span class="pre">bracket2iob.py</span></tt> and <tt class="doctest"><span class="pre">iob2bracket.py</span></tt>
that take a file in Treebank or CoNLL format (resp) and convert it to the other
format.  (Obtain some raw Treebank or CoNLL data from the NLTK Corpora, save it
to a file, and then use <tt class="doctest"><span class="pre">for line in open(filename)</span></tt> to access it from Python.)</li>
</ol>
</li>
<li>&#9733;
An n-gram chunker can use information other than the current
part-of-speech tag and the <span class="math">n-1</span> previous chunk tags.
Investigate other models of the context, such as
the <span class="math">n-1</span> previous part-of-speech tags, or some combination of
previous chunk tags along with previous and following part-of-speech tags.</li>
<li>&#9733;
Consider the way an n-gram tagger uses recent tags to inform its tagging choice.
Now observe how a chunker may re-use this sequence information.  For example,
both tasks will make use of the information that nouns tend to follow adjectives
(in English).  It would appear that the same information is being maintained in
two places.  Is this likely to become a problem as the size of the rule sets grows?
If so, speculate about any ways that this problem might be addressed.</li>
</ol>
<!-- Footer to be used in all chapters -->
<div class="admonition admonition-about-this-document">
<p class="first admonition-title">About this document...</p>
<p>UPDATED FOR NLTK 3.0.
This is a chapter from <em>Natural Language Processing with Python</em>,
by <a class="reference external" href="http://estive.net/">Steven Bird</a>, <a class="reference external" href="http://homepages.inf.ed.ac.uk/ewan/">Ewan Klein</a> and <a class="reference external" href="http://ed.loper.org/">Edward Loper</a>,
Copyright &#169; 2014 the authors.
It is distributed with the <em>Natural Language Toolkit</em> [<tt class="doctest"><span class="pre">http://nltk.org/</span></tt>],
Version 3.0, under the terms of the
<em>Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License</em>
[<a class="reference external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/us/">http://creativecommons.org/licenses/by-nc-nd/3.0/us/</a>].</p>
<p class="last">This document was built on
Wed 20 Aug 2014 11:10:48 GET</p>
</div>
<!-- Not used anymore:
Identifying the boundaries of specific types of word sequences is also
required when we want to recognize pieces of syntactic
structure. Suppose for example that as a preliminary to named entity
recognition, we have decided that it would be useful to just pick out
noun phrases from a piece of text. To carry this out in a complete
way, we would probably want to use a proper syntactic parser. But
parsing can be quite challenging and computationally expensive |mdash|
is there an easier alternative? The answer is Yes: we can look for
sequences of part-of-speech tags in a tagged text, using one or more
patterns that capture the typical ingredients of a noun phrase. -->
</div>
<div class="system-messages section">
<h1>Docutils System Messages</h1>
<div class="system-message" id="id6">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 124); <em><a href="#id7">backlink</a></em></p>
Unknown target name: &quot;chap-semantics&quot;.</div>
<div class="system-message" id="id8">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 148); <em><a href="#id9">backlink</a></em></p>
Unknown target name: &quot;chap-words&quot;.</div>
<div class="system-message" id="id10">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 148); <em><a href="#id11">backlink</a></em></p>
Unknown target name: &quot;chap-tag&quot;.</div>
<div class="system-message" id="id12">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 294); <em><a href="#id13">backlink</a></em></p>
Unknown target name: &quot;sec-regular-expressions-word-patterns&quot;.</div>
<div class="system-message" id="id14">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 392); <em><a href="#id15">backlink</a></em></p>
Unknown target name: &quot;sec-tagged-corpora&quot;.</div>
<div class="system-message" id="id16">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 497); <em><a href="#id17">backlink</a></em></p>
Unknown target name: &quot;chap-parse&quot;.</div>
<div class="system-message" id="id18">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 685); <em><a href="#id19">backlink</a></em></p>
Unknown target name: &quot;sec-automatic-tagging&quot;.</div>
<div class="system-message" id="id20">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 830); <em><a href="#id21">backlink</a></em></p>
Unknown target name: &quot;sec-supervised-classification&quot;.</div>
<div class="system-message" id="id22">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 838); <em><a href="#id23">backlink</a></em></p>
Unknown target name: &quot;code-consecutive-pos-tagger&quot;.</div>
<div class="system-message" id="id24">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1081); <em><a href="#id25">backlink</a></em></p>
Unknown target name: &quot;chap-parse&quot;.</div>
<div class="system-message" id="id26">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1519); <em><a href="#id1">backlink</a></em></p>
Unknown target name: &quot;abney1996pst&quot;.</div>
<div class="system-message" id="id27">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1523); <em><a href="#id2">backlink</a></em></p>
Unknown target name: &quot;abney1996pst&quot;.</div>
<div class="system-message" id="id28">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1526); <em><a href="#id3">backlink</a></em></p>
Unknown target name: &quot;ramshaw1995tcu&quot;.</div>
<div class="system-message" id="id29">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1533); <em><a href="#id4">backlink</a></em></p>
Unknown target name: &quot;jurafskymartin2008&quot;.</div>
<div class="system-message" id="id30">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1533); <em><a href="#id5">backlink</a></em></p>
Unknown target name: &quot;ananiadou2006&quot;.</div>
<div class="system-message" id="id31">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch07.rst</tt>, line 1622); <em><a href="#id32">backlink</a></em></p>
Unknown target name: &quot;chap-tag&quot;.</div>
</div>
</div>
</body>
</html>
