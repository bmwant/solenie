<?xml version="1.0" encoding="ascii" ?>

<script language="javascript" type="text/javascript">

function astext(node)
{
    return node.innerHTML.replace(/(<([^>]+)>)/ig,"")
                         .replace(/&gt;/ig, ">")
                         .replace(/&lt;/ig, "<")
                         .replace(/&quot;/ig, '"')
                         .replace(/&amp;/ig, "&");
}

function copy_notify(node, bar_color, data)
{
    // The outer box: relative + inline positioning.
    var box1 = document.createElement("div");
    box1.style.position = "relative";
    box1.style.display = "inline";
    box1.style.top = "2em";
    box1.style.left = "1em";
  
    // A shadow for fun
    var shadow = document.createElement("div");
    shadow.style.position = "absolute";
    shadow.style.left = "-1.3em";
    shadow.style.top = "-1.3em";
    shadow.style.background = "#404040";
    
    // The inner box: absolute positioning.
    var box2 = document.createElement("div");
    box2.style.position = "relative";
    box2.style.border = "1px solid #a0a0a0";
    box2.style.left = "-.2em";
    box2.style.top = "-.2em";
    box2.style.background = "white";
    box2.style.padding = ".3em .4em .3em .4em";
    box2.style.fontStyle = "normal";
    box2.style.background = "#f0e0e0";

    node.insertBefore(box1, node.childNodes.item(0));
    box1.appendChild(shadow);
    shadow.appendChild(box2);
    box2.innerHTML="Copied&nbsp;to&nbsp;the&nbsp;clipboard: " +
                   "<pre class='copy-notify'>"+
                   data+"</pre>";
    setTimeout(function() { node.removeChild(box1); }, 1000);

    var elt = node.parentNode.firstChild;
    elt.style.background = "#ffc0c0";
    setTimeout(function() { elt.style.background = bar_color; }, 200);
}

function copy_codeblock_to_clipboard(node)
{
    var data = astext(node)+"\n";
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#40a060", data);
    }
}

function copy_doctest_to_clipboard(node)
{
    var s = astext(node)+"\n   ";
    var data = "";

    var start = 0;
    var end = s.indexOf("\n");
    while (end >= 0) {
        if (s.substring(start, start+4) == ">>> ") {
            data += s.substring(start+4, end+1);
        }
        else if (s.substring(start, start+4) == "... ") {
            data += s.substring(start+4, end+1);
        }
        /*
        else if (end-start > 1) {
            data += "# " + s.substring(start, end+1);
        }*/
        // Grab the next line.
        start = end+1;
        end = s.indexOf("\n", start);
    }
    
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#4060a0", data);
    }
}
    
function copy_text_to_clipboard(data)
{
    if (window.clipboardData) {
        window.clipboardData.setData("Text", data);
        return true;
     }
    else if (window.netscape) {
        // w/ default firefox settings, permission will be denied for this:
        netscape.security.PrivilegeManager
                      .enablePrivilege("UniversalXPConnect");
    
        var clip = Components.classes["@mozilla.org/widget/clipboard;1"]
                      .createInstance(Components.interfaces.nsIClipboard);
        if (!clip) return;
    
        var trans = Components.classes["@mozilla.org/widget/transferable;1"]
                       .createInstance(Components.interfaces.nsITransferable);
        if (!trans) return;
    
        trans.addDataFlavor("text/unicode");
    
        var str = new Object();
        var len = new Object();
    
        var str = Components.classes["@mozilla.org/supports-string;1"]
                     .createInstance(Components.interfaces.nsISupportsString);
        var datacopy=data;
        str.data=datacopy;
        trans.setTransferData("text/unicode",str,datacopy.length*2);
        var clipid=Components.interfaces.nsIClipboard;
    
        if (!clip) return false;
    
        clip.setData(trans,null,clipid.kGlobalClipboard);
        return true;
    }
    return false;
}
//-->
</script>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ascii" />
<meta name="generator" content="Docutils 0.14: http://docutils.sourceforge.net/" />
<title>ch03.rst</title>
<--- Cannot embed stylesheet '../nltkdoc.css': No such file or directory. --->
</head>
<body>
<div class="document">


<!-- -*- mode: rst -*- -->
<!-- -*- mode: rst -*- -->
<!-- CAP abbreviations (map to small caps in LaTeX) -->
<!-- Other candidates for global consistency -->
<!-- PTB removed since it must be indexed -->
<!-- WN removed since it must be indexed -->
<!-- misc & punctuation -->
<!-- cdots was unicode U+22EF but not working -->
<!-- exercise meta-tags -->
<!-- Unicode tests -->
<!-- phonetic -->
<!-- misc -->
<!-- used in Unicode section -->
<!-- arrows -->
<!-- unification stuff -->
<!-- Math & Logic -->
<!-- sets -->
<!-- Greek -->
<!-- Chinese -->
<!-- URLs -->
<!-- Python example - a snippet of code in running text -->
<!-- PlaceHolder example -  something that should be replaced by actual code -->
<!-- Linguistic eXample - cited form in running text -->
<!-- Emphasized (more declarative than just using *) -->
<!-- Grammatical Category - e.g. NP and verb as technical terms
.. role:: gc
   :class: category -->
<!-- Math expression - e.g. especially for variables -->
<!-- Textual Math expression - for words 'inside' a math environment -->
<!-- Feature (or attribute) -->
<!-- Raw LaTeX -->
<!-- Raw HTML -->
<!-- Feature-value -->
<!-- Lexemes -->
<!-- Replacements that rely on previous definitions :-) -->
<div class="compound">
</div>
<!-- standard global imports

>>> import nltk, re, pprint
>>> from nltk import word_tokenize -->
<!-- TODO: more on regular expressions, including () -->
<!-- TODO: talk about fact that English lexicon is open set (e.g. malware = malicious software) -->
<!-- TODO: add pointers to regexp toolkits (e.g. Kodos) -->
<!-- TODO: other issues
- nltk.corpus.brown.items returns a tuple, not a list (cf discussion in ch 6)
- invocation of pprint.pprint is a little clunky
- regexp_tokenize() doesn't work when it is given a compiled pattern -->
<!-- TODO: add more graphical plots -->
<!-- TODO: map and reduce -->
<!-- FreqDist of CHARACTER BIGRAMS... -->
<!-- TODO: corpus of word frequencies, so we can do certain tasks on the n most frequent words -->
<!-- TODO: plain wsj corpus -->
<!-- TODO: cover tag soup when talking about HTML -->
<!-- TODO: type conversion using int(), list(), etc -->
<!-- TODO: vowel harmony example: extract vowel sequence using re.findall; extract bigrams from the
vowel sequences; then build a conditional frequency distribution -->
<div class="section" id="processing-raw-text">
<span id="chap-words"></span><h1>3&nbsp;&nbsp;&nbsp;Processing Raw Text</h1>
<p>The most important source of texts is undoubtedly the Web.  It's convenient
to have existing text collections to explore, such as the corpora we saw
in the previous chapters.  However, you probably have your own text sources
in mind, and need to learn how to access them.</p>
<p>The goal of this chapter is to answer the following questions:</p>
<ol class="arabic simple">
<li>How can we write programs to access text from local files and
from the web, in order to get hold of an unlimited range of
language material?</li>
<li>How can we split documents up into individual words and
punctuation symbols, so we can carry out the same kinds of
analysis we did with text corpora in earlier chapters?</li>
<li>How can we write programs to produce formatted output
and save it in a file?</li>
</ol>
<p>In order to address these questions, we will be covering
key concepts in NLP, including tokenization and stemming.
Along the way you will consolidate your Python knowledge and
learn about strings, files, and regular expressions.  Since
so much text on the web is in HTML format, we will also
see how to dispense with markup.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>Important:</strong>
From this chapter onwards, our program samples will assume you
begin your interactive session or your program with the following import
statements:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from __future__ import division  # Python 2 users only
>>> import nltk, re, pprint
>>> from nltk import word_tokenize</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="accessing-text-from-the-web-and-from-disk">
<span id="sec-accessing-text"></span><h2>3.1&nbsp;&nbsp;&nbsp;Accessing Text from the Web and from Disk</h2>
<div class="section" id="electronic-books">
<h3>Electronic Books</h3>
<p>A small sample of texts from Project Gutenberg appears in the NLTK corpus collection.
However, you may be interested in analyzing other texts from Project Gutenberg.
You can browse the catalog of 25,000 free online books at
<tt class="doctest"><span class="pre">http://www.gutenberg.org/catalog/</span></tt>, and obtain a URL to an ASCII text file.
Although 90% of the texts in Project Gutenberg are in English, it
includes material in over 50 other languages, including Catalan, Chinese, Dutch,
Finnish, French, German, Italian, Portuguese and Spanish (with more than
100 texts each).</p>
<p>Text number 2554 is an English translation of <em>Crime and Punishment</em>,
and we can access it as follows.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from urllib import request
>>> url = "http://www.gutenberg.org/files/2554/2554-0.txt"
>>> response = request.urlopen(url)
>>> raw = response.read().decode('utf8')
>>> type(raw)
<class 'str'>
>>> len(raw)
1176893
>>> raw[:75]
'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n'</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The <tt class="doctest"><span class="pre">read()</span></tt> process will take a few seconds as it downloads this large book.
If you're using an internet proxy which is not correctly detected by Python,
you may need to specify the proxy manually, before using <tt class="doctest"><span class="pre">urlopen</span></tt>, as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> proxies = {'http': 'http://www.someproxy.com:3128'}
>>> request.ProxyHandler(proxies)</td>
</tr></table></td></tr>
</table></div>
</div>
<p>The variable <tt class="doctest"><span class="pre">raw</span></tt> contains a string with 1,176,893 characters.
(We can see that it is a string, using <tt class="doctest"><span class="pre">type(raw)</span></tt>.)
This is the raw content of the book,
including many details we are not interested in such as
whitespace, line breaks and blank lines.  Notice the <tt class="doctest"><span class="pre">\r</span></tt> and <tt class="doctest"><span class="pre">\n</span></tt>
in the opening line of the file, which is how Python displays the
special carriage return and line feed characters (the file must
have been created on a Windows machine).  For our language
processing, we want to break up the string into
words and punctuation, as we saw in <a href="#id14"><span class="problematic" id="id15">chap-introduction_</span></a>.  This step is
called <a name="tokenization_index_term" /><span class="termdef">tokenization</span>, and it produces our familiar structure, a list of words
and punctuation.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tokens = word_tokenize(raw)
>>> type(tokens)
<class 'list'>
>>> len(tokens)
254354
>>> tokens[:10]
['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']</td>
</tr></table></td></tr>
</table></div>
<p>Notice that NLTK was needed for tokenization, but not for any of the
earlier tasks of opening a URL and reading it into a string.
If we now take the further step of creating an NLTK text from this
list, we can carry out all of the other linguistic processing we saw
in <a href="#id16"><span class="problematic" id="id17">chap-introduction_</span></a>, along with the regular list operations
like slicing:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = nltk.Text(tokens)
>>> type(text)
<class 'nltk.text.Text'>
>>> text[1024:1062]
['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',
 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',
 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly',
 ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.']
>>> text.collocations()
Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya
Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old
woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;
great deal; Nikodim Fomitch; young man; Ilya Petrovitch; n't know;
Project Gutenberg; Dmitri Prokofitch; Andrey Semyonovitch; Hay Market</td>
</tr></table></td></tr>
</table></div>
<p>Notice that <span class="example">Project Gutenberg</span> appears as a collocation.
This is because each text downloaded from Project Gutenberg contains a header with the
name of the text, the author, the names of people who scanned and
corrected the text, a license, and so on.  Sometimes this information
appears in a footer at the end of the file.  We cannot reliably detect
where the content begins and ends, and so have to resort to manual
inspection of the file, to discover unique strings that mark the beginning
and the end, before trimming <tt class="doctest"><span class="pre">raw</span></tt> to be just the content and nothing else:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> raw.find("PART I")
5338
>>> raw.rfind("End of Project Gutenberg's Crime")
1157743
>>> raw = raw[5338:1157743] # [_raw-slice]
>>> raw.find("PART I")
0</td>
</tr></table></td></tr>
</table></div>
<p>The <tt class="doctest"><span class="pre">find()</span></tt> and <tt class="doctest"><span class="pre">rfind()</span></tt> (&quot;reverse find&quot;) methods help us get
the right index values to use for slicing the string <a class="reference internal" href="#raw-slice"><span id="ref-raw-slice"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.
We overwrite <tt class="doctest"><span class="pre">raw</span></tt> with this slice, so now it begins
with &quot;PART I&quot; and goes up to (but not including)
the phrase that marks the end of the content.</p>
<p>This was our first brush with the reality of the web:
texts found on the web may contain unwanted material,
and there may not be an automatic way to remove it.
But with a small amount of extra work we can extract the material we need.</p>
</div>
<div class="section" id="dealing-with-html">
<h3>Dealing with HTML</h3>
<p>Much of the text on the web is in the form of HTML documents.
You can use a web browser to save a page as text to a local
file, then access this as described in the section on files below.
However, if you're going to do this often, it's easiest to get Python
to do the work directly.  The first step is the same as before,
using <tt class="doctest"><span class="pre">urlopen</span></tt>.  For fun we'll pick a BBC News story
called <em>Blondes to die out in 200 years</em>, an urban legend
passed along by the BBC as established scientific fact:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> url = "http://news.bbc.co.uk/2/hi/health/2284783.stm"
>>> html = request.urlopen(url).read().decode('utf8')
>>> html[:60]
'<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN'</td>
</tr></table></td></tr>
</table></div>
<p>You can type <tt class="doctest"><span class="pre">print(html)</span></tt> to see the HTML content in all its glory,
including meta tags, an image map, JavaScript, forms, and tables.</p>
<p>To get text out of HTML we will use a Python library called <em>BeautifulSoup</em>,
available from <tt class="doctest"><span class="pre">http://www.crummy.com/software/BeautifulSoup/</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from bs4 import BeautifulSoup
>>> raw = BeautifulSoup(html, 'html.parser').get_text()
>>> tokens = word_tokenize(raw)
>>> tokens
['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', "'to", 'die', 'out', ...]</td>
</tr></table></td></tr>
</table></div>
<p>This still contains unwanted material concerning site navigation and related
stories.  With some trial and error you can find the start and end indexes of the
content and select the tokens of interest, and initialize a text as before.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tokens = tokens[110:390]
>>> text = nltk.Text(tokens)
>>> text.concordance('gene')
Displaying 5 of 5 matches:
hey say too few people now carry the gene for blondes to last beyond the next
blonde hair is caused by a recessive gene . In order for a child to have blond
have blonde hair , it must have the gene on both sides of the family in the g
ere is a disadvantage of having that gene or by chance . They do n't disappear
des would disappear is if having the gene was a disadvantage and I do not thin</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="processing-search-engine-results">
<h3>Processing Search Engine Results</h3>
<p>The web can be thought of as a huge corpus of unannotated text.  Web
search engines provide an efficient means of searching this large
quantity of text for relevant linguistic examples.  The main advantage
of search engines is size: since you are searching such a large set of
documents, you are more likely to find any linguistic pattern you
are interested in.  Furthermore, you can make use of very specific
patterns, which would only match one or two examples on a smaller
example, but which might match tens of thousands of examples when run
on the web.  A second advantage of web search engines is that they are
very easy to use.  Thus, they provide a very convenient tool for
quickly checking a theory, to see if it is reasonable.</p>
<!-- XXX Accessing a search engine programmatically: search results; counts;
Python code to produce the contents of tab-absolutely_; mention
Yahoo Python API and xref to discussion of this in chap-data_.] -->
<span class="target" id="tab-absolutely"></span><table border="1" class="docutils" id="tab-absolutely">
<colgroup>
<col width="27%" />
<col width="19%" />
<col width="17%" />
<col width="17%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Google hits</th>
<th class="head"><span class="example">adore</span></th>
<th class="head"><span class="example">love</span></th>
<th class="head"><span class="example">like</span></th>
<th class="head"><span class="example">prefer</span></th>
</tr>
</thead>
<tbody valign="top">
<tr><td><span class="example">absolutely</span></td>
<td>289,000</td>
<td>905,000</td>
<td>16,200</td>
<td>644</td>
</tr>
<tr><td><span class="example">definitely</span></td>
<td>1,460</td>
<td>51,000</td>
<td>158,000</td>
<td>62,600</td>
</tr>
<tr><td>ratio</td>
<td>198:1</td>
<td>18:1</td>
<td>1:10</td>
<td>1:97</td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 3.1</span>: <p>Google Hits for Collocations: The number of hits for collocations
involving the words <span class="example">absolutely</span> or <span class="example">definitely</span>, followed
by one of <span class="example">adore</span>, <span class="example">love</span>, <span class="example">like</span>, or <span class="example">prefer</span>.
(Liberman, in <em>LanguageLog</em>, 2005).</p>
</p>
</table>
<p>Unfortunately, search engines have some significant shortcomings.
First, the allowable range of search patterns is severely restricted.
Unlike local corpora, where you write programs to search for
arbitrarily complex patterns, search engines generally
only allow you to search for individual words or strings of
words, sometimes with wildcards.  Second, search engines give
inconsistent results, and can give widely different figures when used
at different times or in different geographical regions.  When content has been
duplicated across multiple sites, search results may be boosted.
Finally, the markup in the result returned by a search engine may change unpredictably,
breaking any pattern-based method of locating particular content (a problem
which is ameliorated by the use of search engine APIs).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Search the web for <tt class="doctest"><span class="pre">"the of"</span></tt> (inside quotes).  Based on the large
count, can we conclude that <span class="example">the of</span> is a frequent collocation
in English?</p>
</div>
</div>
<div class="section" id="processing-rss-feeds">
<h3>Processing RSS Feeds</h3>
<!-- XX We either need to control the feed more (is this possible?) or -->
<!-- else warn the reader that they will get different results -->
<p>The blogosphere is an important source of text, in both formal and informal registers.
With the help of a Python library called the <em>Universal Feed Parser</em>,
available from <tt class="doctest"><span class="pre">https://pypi.python.org/pypi/feedparser</span></tt>, we can access the content
of a blog, as shown below:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> import feedparser
>>> llog = feedparser.parse("http://languagelog.ldc.upenn.edu/nll/?feed=atom")
>>> llog['feed']['title']
'Language Log'
>>> len(llog.entries)
15
>>> post = llog.entries[2]
>>> post.title
"He's My BF"
>>> content = post.content[0].value
>>> content[:70]
'<p>Today I was chatting with three of our visiting graduate students f'
>>> raw = BeautifulSoup(content, 'html.parser').get_text()
>>> word_tokenize(raw)
['Today', 'I', 'was', 'chatting', 'with', 'three', 'of', 'our', 'visiting',
'graduate', 'students', 'from', 'the', 'PRC', '.', 'Thinking', 'that', 'I',
'was', 'being', 'au', 'courant', ',', 'I', 'mentioned', 'the', 'expression',
'DUI4XIANG4', '\u5c0d\u8c61', '("', 'boy', '/', 'girl', 'friend', '"', ...]</td>
</tr></table></td></tr>
</table></div>
<p>With some further work, we can write programs to create a small corpus of blog posts,
and use this as the basis for our NLP work.</p>
<!-- XXX I've played around with feeds myself, and found it kind of
frustrating, in the sense that it's very hard to know what kind of
data structure you're getting back, and therefore hard to know what
kind of operations you can perform. This snippet illustrates the
problem rather poignantly. How can the reader get a handle on what
something like this means?
   >>> content = post.content[0].value
NB this also prints out unicode strings, which haven't been
explained yet. There's also a "so what" feeling about this - -
there's a chunk of code, but no discussion about what it amounts to. -->
</div>
<div class="section" id="reading-local-files">
<h3>Reading Local Files</h3>
<!-- Monkey-patching to fake the file/web examples in this section:

>>> from io import StringIO
>>> def fake_open(filename, mode=None):
...     return StringIO('Time flies like an arrow.\nFruit flies like a banana.\n')
>>> def fake_urlopen(url):
...     return StringIO('<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN"')
>>> open = fake_open
>>> from urllib import request
>>> request.urlopen.read = lambda: fake_urlopen -->
<p>In order to read a local file, we need to use Python's built-in <tt class="doctest"><span class="pre">open()</span></tt> function,
followed by the <tt class="doctest"><span class="pre">read()</span></tt> method.  Suppose you have a file <tt class="doctest"><span class="pre">document.txt</span></tt>, you
can load its contents like this:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> f = open('document.txt')
>>> raw = f.read()</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Create a file called <tt class="doctest"><span class="pre">document.txt</span></tt> using a text editor, and type in a few lines of text,
and save it as plain text.
If you are using IDLE, select the <em>New Window</em> command in the <em>File</em> menu, typing
the required text into this window, and then saving the file as
<tt class="doctest"><span class="pre">document.txt</span></tt> inside the directory that IDLE offers in the pop-up dialogue box.
Next, in the Python interpreter, open the file using <tt class="doctest"><span class="pre">f = open('document.txt')</span></tt>, then
inspect its contents using <tt class="doctest"><span class="pre">print(f.read())</span></tt>.</p>
</div>
<p>Various things might have gone wrong when you tried this.
If the interpreter couldn't find your file, you would have seen an
error like this:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> f = open('document.txt')
Traceback (most recent call last):
File "<pyshell#7>", line 1, in -toplevel-
f = open('document.txt')
IOError: [Errno 2] No such file or directory: 'document.txt'</td>
</tr></table></td></tr>
</table></div>
<p>To check that the file that you are trying to open is really in the
right directory, use IDLE's <em>Open</em> command in the <em>File</em> menu;
this will display a list of all the files in the directory where
IDLE is running. An alternative is to examine the current
directory from within Python:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> import os
>>> os.listdir('.')</td>
</tr></table></td></tr>
</table></div>
<p>Another possible problem you might have encountered when accessing a text file
is the newline conventions, which are different for different operating systems.
The built-in <tt class="doctest"><span class="pre">open()</span></tt> function has a second parameter for controlling how
the file is opened: <tt class="doctest"><span class="pre">open('document.txt', 'rU')</span></tt> &#8212;
<tt class="doctest"><span class="pre">'r'</span></tt> means to open the file for reading (the default), and
<tt class="doctest"><span class="pre">'U'</span></tt> stands for &quot;Universal&quot;, which lets us ignore the different
conventions used for marking newlines.</p>
<p>Assuming that you can open the file, there are several methods for reading it.
The <tt class="doctest"><span class="pre">read()</span></tt> method creates a string with the contents of the entire file:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> f.read()
'Time flies like an arrow.\nFruit flies like a banana.\n'</td>
</tr></table></td></tr>
</table></div>
<p>Recall that the <tt class="doctest"><span class="pre">'\n'</span></tt> characters are <a name="newlines_index_term" /><span class="termdef">newlines</span>; this
is equivalent to pressing <em>Enter</em> on a keyboard and starting a new line.</p>
<!-- XXX I think we also mentioned print, for suppressing a newline - -
do they need to know about both of these? -->
<p>We can also read a file one line at a time using a <tt class="doctest"><span class="pre">for</span></tt> loop:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> f = open('document.txt', 'rU')
>>> for line in f:
...     print(line.strip())
Time flies like an arrow.
Fruit flies like a banana.</td>
</tr></table></td></tr>
</table></div>
<p>Here we use the <tt class="doctest"><span class="pre">strip()</span></tt> method to remove the newline character at the end of
the input line.</p>
<p>NLTK's corpus files can also be accessed using these methods.  We simply
have to use <tt class="doctest"><span class="pre">nltk.data.find()</span></tt> to get the filename for any corpus item.
Then we can open and read it in the way we just demonstrated above:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')
>>> raw = open(path, 'rU').read()</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="extracting-text-from-pdf-msword-and-other-binary-formats">
<h3>Extracting Text from PDF, MSWord and other Binary Formats</h3>
<p>ASCII text and HTML text are human readable formats.  Text often comes in binary
formats &#8212; like PDF and MSWord &#8212; that can only be opened using specialized
software.  Third-party libraries such as <tt class="doctest"><span class="pre">pypdf</span></tt> and <tt class="doctest"><span class="pre">pywin32</span></tt>
provide access to
these formats.  Extracting text from multi-column documents is particularly
challenging.  For once-off conversion of a few documents,
it is simpler to open the document with a suitable application, then save it as text
to your local drive, and access it as described below.
If the document is already on the web, you can enter its URL in Google's search box.
The search result often includes a link to an HTML version of the document,
which you can save as text.</p>
</div>
<div class="section" id="capturing-user-input">
<h3>Capturing User Input</h3>
<p>Sometimes we want to capture the text that a user inputs when she is
interacting with our program. To prompt the user
to type a line of input, call the Python function <tt class="doctest"><span class="pre">input()</span></tt>.
After saving the input to a variable, we can
manipulate it just as we have done for other strings.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> s = input("Enter some text: ")
Enter some text: On an exceptionally hot evening early in July
>>> print("You typed", len(word_tokenize(s)), "words.")
You typed 8 words.</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="the-nlp-pipeline">
<h3>The NLP Pipeline</h3>
<p><a class="reference internal" href="#fig-pipeline1">fig-pipeline1</a> summarizes what we have covered in this section, including the process
of building a vocabulary that we saw in <a href="#id18"><span class="problematic" id="id19">chap-introduction_</span></a>.  (One step, normalization,
will be discussed in <a class="reference internal" href="#sec-normalizing-text">3.6</a>.)</p>
<div class="system-message" id="fig-pipeline1">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 446)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '30:120:35')
invalid literal for int() with base 10: '30:120:35'.</p>
<pre class="literal-block">
.. figure:: ../images/pipeline1.png
   :scale: 30:120:35

   The Processing Pipeline: We open a URL and read its HTML content,
   remove the markup and select a slice of characters;
   this is then tokenized and optionally converted into an ``nltk.Text`` object;
   we can also lowercase all the words and extract the vocabulary.

</pre>
</div>
<p>There's a lot going on in this pipeline.  To understand it properly, it helps to be
clear about the type of each variable that it mentions.  We find out the type
of any Python object <tt class="doctest"><span class="pre">x</span></tt> using <tt class="doctest"><span class="pre">type(x)</span></tt>, e.g. <tt class="doctest"><span class="pre">type(1)</span></tt> is <tt class="doctest"><span class="pre"><int></span></tt>
since <tt class="doctest"><span class="pre">1</span></tt> is an integer.</p>
<p>When we load the contents of a URL or file, and when we strip out HTML markup,
we are dealing with strings, Python's <tt class="doctest"><span class="pre"><str></span></tt> data type.
(We will learn more about strings in <a class="reference internal" href="#sec-strings">3.2</a>):</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> raw = open('document.txt').read()
>>> type(raw)
<class 'str'></td>
</tr></table></td></tr>
</table></div>
<p>When we tokenize a string we produce a list (of words), and this is Python's <tt class="doctest"><span class="pre"><list></span></tt>
type.  Normalizing and sorting lists produces other lists:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> tokens = word_tokenize(raw)
>>> type(tokens)
<class 'list'>
>>> words = [w.lower() for w in tokens]
>>> type(words)
<class 'list'>
>>> vocab = sorted(set(words))
>>> type(vocab)
<class 'list'></td>
</tr></table></td></tr>
</table></div>
<p>The type of an object determines what operations you can perform on it.
So, for example, we can append to a list but not to a string:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> vocab.append('blog')
>>> raw.append('blog')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'str' object has no attribute 'append'</td>
</tr></table></td></tr>
</table></div>
<!-- XXX partial duplication with text on p86 of hardcopy -->
<p>Similarly, we can concatenate strings with strings, and lists with
lists, but we cannot concatenate strings with lists:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> query = 'Who knows?'
>>> beatles = ['john', 'paul', 'george', 'ringo']
>>> query + beatles
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: cannot concatenate 'str' and 'list' objects</td>
</tr></table></td></tr>
</table></div>
</div>
</div>
<div class="section" id="strings-text-processing-at-the-lowest-level">
<span id="sec-strings"></span><h2>3.2&nbsp;&nbsp;&nbsp;Strings: Text Processing at the Lowest Level</h2>
<p>It's time to examine a fundamental data type that we've been studiously avoiding
so far.  In earlier chapters we focused on a text as a list of words.  We didn't
look too closely at words and how they are handled in the programming
language.  By using NLTK's corpus interface we were able to ignore
the files that these texts had come from.  The contents of a word, and
of a file, are represented by programming languages as a fundamental
data type known as a <a name="string_index_term" /><span class="termdef">string</span>.  In this section we explore strings
in detail, and show the connection between strings, words, texts and files.</p>
<div class="section" id="basic-operations-with-strings">
<h3>Basic Operations with Strings</h3>
<p>Strings are specified using single quotes <a class="reference internal" href="#single-quotes"><span id="ref-single-quotes"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>
or double quotes <a class="reference internal" href="#double-quotes"><span id="ref-double-quotes"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>, as shown below.
If a string contains a single quote, we must backslash-escape
the quote <a class="reference internal" href="#backslash-escape"><span id="ref-backslash-escape"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a> so Python knows a literal quote character is intended,
or else put the string in double quotes <a class="reference internal" href="#double-quotes"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></a>.
Otherwise, the quote inside the string <a class="reference internal" href="#unescaped-quote"><span id="ref-unescaped-quote"><img src="callouts/callout4.gif" alt="[4]" class="callout" /></span></a>
will be interpreted as a close quote, and the Python interpreter
will report a syntax error:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty = 'Monty Python' # [_single-quotes]
>>> monty
'Monty Python'
>>> circus = "Monty Python's Flying Circus" # [_double-quotes]
>>> circus
"Monty Python's Flying Circus"
>>> circus = 'Monty Python\'s Flying Circus' # [_backslash-escape]
>>> circus
"Monty Python's Flying Circus"
>>> circus = 'Monty Python's Flying Circus' # [_unescaped-quote]
  File "<stdin>", line 1
    circus = 'Monty Python's Flying Circus'
                           ^
SyntaxError: invalid syntax</td>
</tr></table></td></tr>
</table></div>
<p>Sometimes strings go over several lines.  Python provides us with various
ways of entering them.  In the next example, a sequence of two strings is
joined into a single string.
We need to use backslash <a class="reference internal" href="#string-backslash"><span id="ref-string-backslash"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> or parentheses <a class="reference internal" href="#string-parentheses"><span id="ref-string-parentheses"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a> so that
the interpreter knows that the statement is not complete after the first line.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> couplet = "Shall I compare thee to a Summer's day?"\
...           "Thou are more lovely and more temperate:" # [_string-backslash]
>>> print(couplet)
Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:
>>> couplet = ("Rough winds do shake the darling buds of May,"
...           "And Summer's lease hath all too short a date:") # [_string-parentheses]
>>> print(couplet)
Rough winds do shake the darling buds of May,And Summer's lease hath all too short a date:</td>
</tr></table></td></tr>
</table></div>
<p>Unfortunately the above methods do not give us a newline between
the two lines of the sonnet.  Instead, we can use a triple-quoted
string as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> couplet = """Shall I compare thee to a Summer's day?
... Thou are more lovely and more temperate:"""
>>> print(couplet)
Shall I compare thee to a Summer's day?
Thou are more lovely and more temperate:
>>> couplet = '''Rough winds do shake the darling buds of May,
... And Summer's lease hath all too short a date:'''
>>> print(couplet)
Rough winds do shake the darling buds of May,
And Summer's lease hath all too short a date:</td>
</tr></table></td></tr>
</table></div>
<p>Now that we can define strings, we can try some simple operations on them.
First let's look at the <tt class="doctest"><span class="pre">+</span></tt> operation, known as <a name="concatenation_index_term" /><span class="termdef">concatenation</span> <a class="reference internal" href="#string-concatenation"><span id="ref-string-concatenation"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.
It produces a new string that is a copy of the
two original strings pasted together end-to-end.  Notice that
concatenation doesn't do anything clever like insert a space between
the words.  We can even multiply strings <a class="reference internal" href="#string-multiplication"><span id="ref-string-multiplication"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> 'very' + 'very' + 'very' # [_string-concatenation]
'veryveryvery'
>>> 'very' * 3 # [_string-multiplication]
'veryveryvery'</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>Your Turn:</strong>
Try running the following code, then try to use your understanding
of the string <tt class="doctest"><span class="pre">+</span></tt> and <tt class="doctest"><span class="pre">*</span></tt> operations to figure out how it works.
Be careful to distinguish between the string <tt class="doctest"><span class="pre">' '</span></tt>, which
is a single whitespace character, and <tt class="doctest"><span class="pre">''</span></tt>, which is the empty string.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]
>>> b = [' ' * 2 * (7 - i) + 'very' * i for i in a]
>>> for line in b:
...     print(line)</td>
</tr></table></td></tr>
</table></div>
</div>
<!-- XXX we haven't drawn an analogy yet. -->
<p>We've seen that the addition and multiplication operations apply to
strings, not just numbers.  However, note that we cannot use
subtraction or division with strings:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> 'very' - 'y'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for -: 'str' and 'str'
>>> 'very' / 2
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for /: 'str' and 'int'</td>
</tr></table></td></tr>
</table></div>
<p>These error messages are another example of Python telling us that we
have got our data types in a muddle. In the first case, we are told
that the operation of subtraction (i.e., <tt class="doctest"><span class="pre">-</span></tt>) cannot apply to
objects of type <tt class="doctest"><span class="pre">str</span></tt> (strings), while in the second, we are told that
division cannot take <tt class="doctest"><span class="pre">str</span></tt> and <tt class="doctest"><span class="pre">int</span></tt> as its two operands.</p>
</div>
<div class="section" id="printing-strings">
<h3>Printing Strings</h3>
<p>So far, when we have wanted to look at the contents of a variable or
see the result of a calculation, we have just typed the variable name
into the interpreter.  We can also see the contents of a variable
using the <tt class="doctest"><span class="pre">print</span></tt> statement:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(monty)
Monty Python</td>
</tr></table></td></tr>
</table></div>
<p>Notice that there are no quotation marks this time.  When we inspect
a variable by typing its name in the interpreter, the interpreter prints
the Python representation of its value.  Since it's a string,
the result is quoted.  However, when we tell the
interpreter to <tt class="doctest"><span class="pre">print</span></tt> the contents of the variable, we don't see
quotation characters since there are none inside the string.</p>
<p>The <tt class="doctest"><span class="pre">print</span></tt> statement allows us to display more than one item on a line
in various ways, as shown below:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> grail = 'Holy Grail'
>>> print(monty + grail)
Monty PythonHoly Grail
>>> print(monty, grail)
Monty Python Holy Grail
>>> print(monty, "and the", grail)
Monty Python and the Holy Grail</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="accessing-individual-characters">
<h3>Accessing Individual Characters</h3>
<p>As we saw in <a href="#id20"><span class="problematic" id="id21">sec-a-closer-look-at-python-texts-as-lists-of-words_</span></a> for lists, strings are indexed, starting from zero.
When we index a string, we get one of its characters (or letters).  A single character is nothing special &#8212; it's just
a string of length <tt class="doctest"><span class="pre">1</span></tt>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty[0]
'M'
>>> monty[3]
't'
>>> monty[5]
' '</td>
</tr></table></td></tr>
</table></div>
<p>As with lists, if we try to access an index that is outside of the string we get an error:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty[20]
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
IndexError: string index out of range</td>
</tr></table></td></tr>
</table></div>
<!-- XXX I don't think it works very well here to have these two observations
followed by two code examples - - it's hard to see what the point of
the ``5 = len(monty) - 7`` remark is in this context. Since you
probably don't want to split up the two examples, callouts might
ameliorate it. -->
<p>Again as with lists, we can use negative indexes for strings,
where <tt class="doctest"><span class="pre">-1</span></tt> is the index of the last character <a class="reference internal" href="#last-character"><span id="ref-last-character"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.
Positive and negative indexes give us two ways to refer to
any position in a string.  In this case, when the string had a length of 12,
indexes <tt class="doctest"><span class="pre">5</span></tt> and <tt class="doctest"><span class="pre">-7</span></tt> both refer to the same character (a space).
(Notice that <tt class="doctest"><span class="pre">5 = len(monty) - 7</span></tt>.)</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty[-1] # [_last-character]
'n'
>>> monty[5]
' '
>>> monty[-7]
' '</td>
</tr></table></td></tr>
</table></div>
<p>We can write <tt class="doctest"><span class="pre">for</span></tt> loops to iterate over the characters
in strings.  This <tt class="doctest"><span class="pre">print</span></tt> function includes the optional <tt class="doctest"><span class="pre">end=' '</span></tt>
parameter, which is how we tell Python to print a space instead of a newline at the end.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> sent = 'colorless green ideas sleep furiously'
>>> for char in sent:
...     print(char, end=' ')
...
c o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y</td>
</tr></table></td></tr>
</table></div>
<p>We can count individual characters as well.  We should ignore the case
distinction by normalizing everything to lowercase, and filter out non-alphabetic characters:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import gutenberg
>>> raw = gutenberg.raw('melville-moby_dick.txt')
>>> fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())
>>> fdist.most_common(5)
[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]
>>> [char for (char, count) in fdist.most_common()]
['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',
'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']</td>
</tr></table></td></tr>
</table></div>
<table class="docutils citation" frame="void" id="sb" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[sb]</td><td>explain this tuple unpacking somewhere?</td></tr>
</tbody>
</table>
<p>This gives us the letters of the alphabet, with the most frequently occurring letters
listed first (this is quite complicated and we'll explain it more carefully below).
You might like to visualize the distribution using <tt class="doctest"><span class="pre">fdist.plot()</span></tt>.
The relative character frequencies of a text can be used in automatically identifying
the language of the text.</p>
</div>
<div class="section" id="accessing-substrings">
<h3>Accessing Substrings</h3>
<div class="system-message" id="fig-string-slicing">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 723)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '25:35:30')
invalid literal for int() with base 10: '25:35:30'.</p>
<pre class="literal-block">
.. figure:: ../images/string-slicing.png
   :scale: 25:35:30

   String Slicing: The string &quot;Monty Python&quot; is shown along with its positive and
   negative indexes; two substrings are selected using &quot;slice&quot; notation.
   The slice ``[m,n]`` contains the characters from position ``m`` through ``n-1``.

</pre>
</div>
<p>A substring is any continuous section of a string that we want to pull out for
further processing.  We can easily access substrings using the same slice notation
we used for lists (see <a class="reference internal" href="#fig-string-slicing">fig-string-slicing</a>).
For example, the following code accesses the substring starting at index <tt class="doctest"><span class="pre">6</span></tt>,
up to (but not including) index <tt class="doctest"><span class="pre">10</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty[6:10]
'Pyth'</td>
</tr></table></td></tr>
</table></div>
<p>Here we see the characters are <tt class="doctest"><span class="pre">'P'</span></tt>, <tt class="doctest"><span class="pre">'y'</span></tt>, <tt class="doctest"><span class="pre">'t'</span></tt>, and <tt class="doctest"><span class="pre">'h'</span></tt> which correspond
to <tt class="doctest"><span class="pre">monty[6]</span></tt> ... <tt class="doctest"><span class="pre">monty[9]</span></tt> but not <tt class="doctest"><span class="pre">monty[10]</span></tt>. This is because
a slice <span class="emphasis">starts</span> at the first index but finishes <span class="emphasis">one before</span> the end index.</p>
<p>We can also slice with negative indexes &#8212; the same basic rule of starting
from the start index and stopping one before the end index applies;
here we stop before the space character.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty[-12:-7]
'Monty'</td>
</tr></table></td></tr>
</table></div>
<p>As with list slices, if we omit the first value, the substring begins at the start
of the string.  If we omit the second value, the substring continues to the end
of the string:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty[:5]
'Monty'
>>> monty[6:]
'Python'</td>
</tr></table></td></tr>
</table></div>
<p>We test if a string contains a particular substring using the <tt class="doctest"><span class="pre">in</span></tt> operator, as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> phrase = 'And now for something completely different'
>>> if 'thing' in phrase:
...     print('found "thing"')
found "thing"</td>
</tr></table></td></tr>
</table></div>
<p>We can also find the position of a substring within a string, using <tt class="doctest"><span class="pre">find()</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> monty.find('Python')
6</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Make up a sentence and assign it to a variable, e.g. <tt class="doctest"><span class="pre">sent = 'my sentence...'</span></tt>.
Now write slice expressions to pull out individual words.  (This is obviously
not a convenient way to process the words of a text!)</p>
</div>
</div>
<div class="section" id="more-operations-on-strings">
<h3>More operations on strings</h3>
<p>Python has comprehensive support for processing strings.  A summary, including some operations
we haven't seen yet, is shown in <a class="reference internal" href="#tab-string-methods">3.2</a>.  For more information on strings, type
<tt class="doctest"><span class="pre">help(str)</span></tt> at the Python prompt.</p>
<span class="target" id="tab-string-methods"></span><table border="1" class="docutils" id="tab-string-methods">
<colgroup>
<col width="21%" />
<col width="79%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Method</th>
<th class="head">Functionality</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><tt class="doctest"><span class="pre">s.find(t)</span></tt></td>
<td>index of first instance of string <tt class="doctest"><span class="pre">t</span></tt> inside <tt class="doctest"><span class="pre">s</span></tt> (<tt class="doctest"><span class="pre">-1</span></tt> if not found)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.rfind(t)</span></tt></td>
<td>index of last instance of string <tt class="doctest"><span class="pre">t</span></tt> inside <tt class="doctest"><span class="pre">s</span></tt> (<tt class="doctest"><span class="pre">-1</span></tt> if not found)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.index(t)</span></tt></td>
<td>like <tt class="doctest"><span class="pre">s.find(t)</span></tt> except it raises <tt class="doctest"><span class="pre">ValueError</span></tt> if not found</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.rindex(t)</span></tt></td>
<td>like <tt class="doctest"><span class="pre">s.rfind(t)</span></tt> except it raises <tt class="doctest"><span class="pre">ValueError</span></tt> if not found</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.join(text)</span></tt></td>
<td>combine the words of the text into a string using <tt class="doctest"><span class="pre">s</span></tt> as the glue</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.split(t)</span></tt></td>
<td>split <tt class="doctest"><span class="pre">s</span></tt> into a list wherever a <tt class="doctest"><span class="pre">t</span></tt> is found (whitespace by default)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.splitlines()</span></tt></td>
<td>split <tt class="doctest"><span class="pre">s</span></tt> into a list of strings, one per line</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.lower()</span></tt></td>
<td>a lowercased version of the string <tt class="doctest"><span class="pre">s</span></tt></td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.upper()</span></tt></td>
<td>an uppercased version of the string <tt class="doctest"><span class="pre">s</span></tt></td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.title()</span></tt></td>
<td>a titlecased version of the string <tt class="doctest"><span class="pre">s</span></tt></td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.strip()</span></tt></td>
<td>a copy of <tt class="doctest"><span class="pre">s</span></tt> without leading or trailing whitespace</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">s.replace(t, u)</span></tt></td>
<td>replace instances of <tt class="doctest"><span class="pre">t</span></tt> with <tt class="doctest"><span class="pre">u</span></tt> inside <tt class="doctest"><span class="pre">s</span></tt></td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 3.2</span>: <p>Useful String Methods: operations on strings in addition to the string tests
shown in <a href="#id22"><span class="problematic" id="id23">tab-word-tests_</span></a>; all methods produce a new string or list</p>
</p>
</table>
</div>
<div class="section" id="the-difference-between-lists-and-strings">
<h3>The Difference between Lists and Strings</h3>
<p>Strings and lists are both kinds of <a name="sequence_index_term" /><span class="termdef">sequence</span>.  We can pull them
apart by indexing and slicing them, and we can join them together
by concatenating them.  However, we cannot join strings and lists:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> query = 'Who knows?'
>>> beatles = ['John', 'Paul', 'George', 'Ringo']
>>> query[2]
'o'
>>> beatles[2]
'George'
>>> query[:2]
'Wh'
>>> beatles[:2]
['John', 'Paul']
>>> query + " I don't"
"Who knows? I don't"
>>> beatles + 'Brian'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: can only concatenate list (not "str") to list
>>> beatles + ['Brian']
['John', 'Paul', 'George', 'Ringo', 'Brian']</td>
</tr></table></td></tr>
</table></div>
<p>When we open a file
for reading into a Python program, we get a string
corresponding to the contents of the whole file. If we use a <tt class="doctest"><span class="pre">for</span></tt> loop to
process the elements of this string, all we can pick out are the
individual characters &#8212; we don't get to choose the
granularity. By contrast, the elements of a list can be as big or
small as we like: for example, they could be paragraphs, sentences,
phrases, words, characters. So lists have the advantage that we
can be flexible about the elements they contain, and
correspondingly flexible about any downstream processing.
Consequently, one of the first things we are likely to do in a piece of NLP
code is tokenize a string into a list of strings (<a class="reference internal" href="#sec-tokenization">3.7</a>).
Conversely, when we want to write our results to a file, or to a terminal,
we will usually format them as a string (<a class="reference internal" href="#sec-formatting">3.9</a>).</p>
<p>Lists and strings do not have exactly the same functionality.
Lists have the added power that you can change their elements:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> beatles[0] = "John Lennon"
>>> del beatles[-1]
>>> beatles
['John Lennon', 'Paul', 'George']</td>
</tr></table></td></tr>
</table></div>
<p>On the other hand if we try to do that with a <em>string</em>
&#8212; changing the 0th character in <tt class="doctest"><span class="pre">query</span></tt> to <tt class="doctest"><span class="pre">'F'</span></tt> &#8212; we get:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> query[0] = 'F'
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
TypeError: object does not support item assignment</td>
</tr></table></td></tr>
</table></div>
<p> This is because strings are <a name="immutable_index_term" /><span class="termdef">immutable</span> &#8212; you can't change a
string once you have created it.  However, lists are <a name="mutable_index_term" /><span class="termdef">mutable</span>,
and their contents can be modified at any time.  As a result, lists
support operations that modify the original value rather than producing a new value.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Consolidate your knowledge of strings by trying some of the exercises on
strings at the end of this chapter.</p>
</div>
</div>
</div>
<div class="section" id="text-processing-with-unicode">
<span id="sec-unicode"></span><h2>3.3&nbsp;&nbsp;&nbsp;Text Processing with Unicode</h2>
<p>Our programs will often need to deal with different languages, and
different character sets.  The concept of &quot;plain text&quot; is a fiction.
If you live in the English-speaking world you probably use ASCII,
possibly without realizing it.  If you live in Europe you might use
one of the extended Latin character sets, containing such characters
as &quot;&#248;&quot; for Danish and Norwegian, &quot;&#337;&quot; for Hungarian,
&quot;&#241;&quot; for Spanish and Breton, and &quot;&#328;&quot; for Czech and
Slovak. In this section, we will give an overview of how to use
Unicode for processing texts that use non-ASCII character sets.</p>
<div class="section" id="what-is-unicode">
<h3>What is Unicode?</h3>
<p>Unicode supports over a million characters.  Each
character is assigned a number, called a <a name="code_point_index_term" /><span class="termdef">code point</span>.  In Python, code
points are written in the form <tt class="doctest"><span class="pre">\u</span></tt><em>XXXX</em>, where <em>XXXX</em> is the number
in 4-digit hexadecimal form.</p>
<p>Within a program, we can manipulate Unicode strings just like normal strings.
However, when Unicode characters are stored in files or displayed on a terminal,
they must be encoded as a stream of bytes.  Some encodings (such
as ASCII and Latin-2) use a single byte per code point, so they can only support a
small subset of Unicode, enough for a single language.  Other encodings
(such as UTF-8) use multiple bytes and can represent the full range of
Unicode characters.</p>
<p>Text in files will be in a particular encoding, so we need some
mechanism for translating it into Unicode &#8212; translation into
Unicode is called <a name="decoding_index_term" /><span class="termdef">decoding</span>. Conversely, to write out Unicode to a
file or a terminal, we first need to translate it into a suitable
encoding &#8212; this translation out of Unicode is called <a name="encoding_index_term" /><span class="termdef">encoding</span>,
and is illustrated in <a class="reference internal" href="#fig-unicode">fig-unicode</a>.</p>
<div class="system-message" id="fig-unicode">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 914)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '10:15:15')
invalid literal for int() with base 10: '10:15:15'.</p>
<pre class="literal-block">
.. figure:: ../images/unicode.png
   :scale: 10:15:15

   Unicode Decoding and Encoding

</pre>
</div>
<p>From a Unicode perspective, characters are abstract entities which can
be realized as one or more <a name="glyphs_index_term" /><span class="termdef">glyphs</span>. Only glyphs can appear on a
screen or be printed on paper. A font is a mapping from characters to glyphs.</p>
</div>
<div class="section" id="extracting-encoded-text-from-files">
<h3>Extracting encoded text from files</h3>
<p>Let's assume that we have a small text file, and that we know how it
is encoded. For example, <tt class="doctest"><span class="pre">polish-lat2.txt</span></tt>, as the name suggests, is
a snippet of Polish text (from the Polish Wikipedia; see
<tt class="doctest"><span class="pre">http://pl.wikipedia.org/wiki/Biblioteka_Pruska</span></tt>).  This file is encoded as Latin-2,
also known as ISO-8859-2. The function <tt class="doctest"><span class="pre">nltk.data.find()</span></tt> locates the
file for us.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')</td>
</tr></table></td></tr>
</table></div>
<p>The Python <tt class="doctest"><span class="pre">open()</span></tt> function can read encoded data
into Unicode strings, and write out Unicode strings in encoded
form.  It takes a parameter to
specify the encoding of the file being read or written. So let's open
our Polish file
with the encoding <tt class="doctest"><span class="pre">'latin2'</span></tt> and  inspect the contents of the file:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> f = open(path, encoding='latin2')
>>> for line in f:
...    line = line.strip()
...    print(line)
Pruska Biblioteka Pa&#324;stwowa. Jej dawne zbiory znane pod nazw&#261;
"Berlinka" to skarb kultury i sztuki niemieckiej. Przewiezione przez
Niemc&#243;w pod koniec II wojny &#347;wiatowej na Dolny &#346;l&#261;sk, zosta&#322;y
odnalezione po 1945 r. na terytorium Polski. Trafi&#322;y do Biblioteki
Jagiello&#324;skiej w Krakowie, obejmuj&#261; ponad 500 tys. zabytkowych
archiwali&#243;w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.</td>
</tr></table></td></tr>
</table></div>
<p>If this does not display correctly on your terminal, or if we want
to see the underlying numerical values (or &quot;codepoints&quot;) of the characters,
then we can convert all non-ASCII characters into their two-digit <tt class="doctest"><span class="pre">\x</span></tt><em>XX</em>
and four-digit <tt class="doctest"><span class="pre">\u</span></tt><em>XXXX</em> representations:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> f = open(path, encoding='latin2')
>>> for line in f:
...     line = line.strip()
...     print(line.encode('unicode_escape'))
b'Pruska Biblioteka Pa\\u0144stwowa. Jej dawne zbiory znane pod nazw\\u0105'
b'"Berlinka" to skarb kultury i sztuki niemieckiej. Przewiezione przez'
b'Niemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y'
b'odnalezione po 1945 r. na terytorium Polski. Trafi\\u0142y do Biblioteki'
b'Jagiello\\u0144skiej w Krakowie, obejmuj\\u0105 ponad 500 tys. zabytkowych'
b'archiwali\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'</td>
</tr></table></td></tr>
</table></div>
<p>The first line above illustrates a Unicode escape string
preceded by the <tt class="doctest"><span class="pre">\u</span></tt> escape string, namely <tt class="doctest"><span class="pre">\u0144</span></tt> . The relevant
Unicode character will be dislayed on the screen as the glyph
&#324;.  In the third line of the preceding example, we see
<tt class="doctest"><span class="pre">\xf3</span></tt>, which corresponds to the glyph &#243;, and is within the
128-255 range.</p>
<p>In Python 3, source code is encoded using UTF-8 by default, and you can
include Unicode characters in strings if you are using IDLE or another program editor
that supports Unicode.
Arbitrary Unicode characters can be included using the
<tt class="doctest"><span class="pre">\u</span></tt><em>XXXX</em> escape sequence.
We find the integer ordinal of a character using <tt class="doctest"><span class="pre">ord()</span></tt>. For example:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> ord('&#324;')
324</td>
</tr></table></td></tr>
</table></div>
<p>The hexadecimal 4 digit notation for 324 is 0144 (type <tt class="doctest"><span class="pre">hex(324)</span></tt> to discover this),
and we can define a string with the appropriate escape sequence.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> nacute = '\u0144'
>>> nacute
'&#324;'</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There are many factors determining what glyphs are rendered
on your screen. If you are sure that you have the correct encoding,
but your Python code is still failing to produce the glyphs you
expected, you should also check that you have the necessary fonts
installed on your system. It may be necessary to configure your locale
to render UTF-8 encoded characters, then use <tt class="doctest"><span class="pre">print(nacute.encode('utf8'))</span></tt>
in order to see the &#324; displayed in your terminal.</p>
</div>
<p>We can also see how this character is represented as a sequence of bytes inside
a text file:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> nacute.encode('utf8')
b'\xc5\x84'</td>
</tr></table></td></tr>
</table></div>
<p>The module <tt class="doctest"><span class="pre">unicodedata</span></tt> lets us inspect the properties of Unicode
characters. In the following example, we select all characters in the
third line of our Polish text outside the ASCII range and print their
UTF-8 byte sequence, followed by their code point integer using the
standard Unicode convention (i.e., prefixing the hex digits with
<tt class="doctest"><span class="pre">U+</span></tt>), followed by their Unicode name.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> import unicodedata
>>> lines = open(path, encoding='latin2').readlines()
>>> line = lines[2]
>>> print(line.encode('unicode_escape'))
b'Niemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y\\n'
>>> for c in line: # [_unicode-info]
...     if ord(c) > 127:
...         print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))
b'\xc3\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE
b'\xc5\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE
b'\xc5\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE
b'\xc4\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK
b'\xc5\x82' U+0142 LATIN SMALL LETTER L WITH STROKE</td>
</tr></table></td></tr>
</table></div>
<p>If you replace
<tt class="doctest"><span class="pre">c.encode('utf8')</span></tt> in <a class="reference internal" href="#unicode-info"><span id="ref-unicode-info"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> with <tt class="doctest"><span class="pre">c</span></tt>, and if your system supports UTF-8,
you should see an output like the following:</p>
<div class="line-block">
<div class="line">&#243; U+00f3 LATIN SMALL LETTER O WITH ACUTE</div>
<div class="line">&#347; U+015b LATIN SMALL LETTER S WITH ACUTE</div>
<div class="line">&#346; U+015a LATIN CAPITAL LETTER S WITH ACUTE</div>
<div class="line">&#261; U+0105 LATIN SMALL LETTER A WITH OGONEK</div>
<div class="line">&#322; U+0142 LATIN SMALL LETTER L WITH STROKE</div>
</div>
<p>Alternatively, you may need to replace the encoding <tt class="doctest"><span class="pre">'utf8'</span></tt> in the
example by <tt class="doctest"><span class="pre">'latin2'</span></tt>, again depending on the details of your system.</p>
<p>The next examples illustrate how Python string methods and the <tt class="doctest"><span class="pre">re</span></tt>
module can work with Unicode characters. (We will take a close look at
the <tt class="doctest"><span class="pre">re</span></tt> module in the following section. The <tt class="doctest"><span class="pre">\w</span></tt> matches a &quot;word
character&quot;, cf <a class="reference internal" href="#tab-re-symbols">3.4</a>).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> line.find('zosta\u0142y')
54
>>> line = line.lower()
>>> line
'niemc&#243;w pod koniec ii wojny &#347;wiatowej na dolny &#347;l&#261;sk, zosta&#322;y\n'
>>> line.encode('unicode_escape')
b'niemc\\xf3w pod koniec ii wojny \\u015bwiatowej na dolny \\u015bl\\u0105sk, zosta\\u0142y\\n'
>>> import re
>>> m = re.search('\u015b\w*', line)
>>> m.group()
'\u015bwiatowej'</td>
</tr></table></td></tr>
</table></div>
<p>NLTK tokenizers allow Unicode strings as input, and
correspondingly yield Unicode strings as output.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> word_tokenize(line)
['niemc&#243;w', 'pod', 'koniec', 'ii', 'wojny', '&#347;wiatowej', 'na', 'dolny', '&#347;l&#261;sk', ',', 'zosta&#322;y']</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="using-your-local-encoding-in-python">
<h3>Using your local encoding in Python</h3>
<p>If you are used to working with characters in a particular local
encoding, you probably want to be able to use your standard methods
for inputting and editing strings in a Python file. In order to do this,
you need to include the string <tt class="doctest"><span class="pre">'# -*- coding: <coding> -*-'</span></tt> as the
first or second line of your file. Note that <em>&lt;coding&gt;</em> has to be a
string like <tt class="doctest"><span class="pre">'latin-1'</span></tt>, <tt class="doctest"><span class="pre">'big5'</span></tt> or <tt class="doctest"><span class="pre">'utf-8'</span></tt> (see <a class="reference internal" href="#fig-polish-utf8">fig-polish-utf8</a>).</p>
<div class="system-message" id="fig-polish-utf8">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1076)</p>
<p>Error in &quot;figure&quot; directive:
invalid option value: (option: &quot;scale&quot;; value: '100:100:100')
invalid literal for int() with base 10: '100:100:100'.</p>
<pre class="literal-block">
.. figure:: ../images/polish-utf8.png
   :scale: 100:100:100

   Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor;
   this requires that an appropriate font is set in IDLE's preferences;
   here we have chosen Courier CE.

</pre>
</div>
<!-- cf http://mail.python.org/pipermail/python-list/2004-February/247783.html

It is also possible to enter non-ASCII characters in interactive
mode. IDLE will convert them to the locale's encoding before
evaluating the source code; if that fails, you get the message you
see. So where you trying to enter hangul characters in interactive
mode in a locale that does not support hangul, or are you lacking
a codec for your locale?

In interactive mode, UTF-8 is never used (unless you have an
UTF-8 locale). -->
<p>The above example also illustrates how regular expressions can use
encoded strings.</p>
<!-- If you are using Emacs as your editor, the coding specification
will also be interpreted as a specification of the editor's coding
for the file. Not all of the valid Python names for codings are
accepted by Emacs. -->
</div>
</div>
<div class="section" id="regular-expressions-for-detecting-word-patterns">
<span id="sec-regular-expressions-word-patterns"></span><h2>3.4&nbsp;&nbsp;&nbsp;Regular Expressions for Detecting Word Patterns</h2>
<p>Many linguistic processing tasks involve pattern matching.
For example, we can find words ending with <span class="example">ed</span> using
<tt class="doctest"><span class="pre">endswith('ed')</span></tt>.  We saw a variety of such &quot;word tests&quot;
in <a href="#id24"><span class="problematic" id="id25">tab-word-tests_</span></a>.
Regular expressions give us a more powerful and flexible
method for describing the character patterns we are interested in.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There are many other published introductions to regular expressions,
organized around the syntax of regular expressions and applied to searching
text files.  Instead of doing this again, we focus on the use of regular expressions
at different stages of linguistic processing.  As usual, we'll adopt
a problem-based approach and present new features only as they are
needed to solve practical problems.  In our discussion we will mark
regular expressions using chevrons like this: &#171;<tt class="doctest"><span class="pre">patt</span></tt>&#187;.</p>
</div>
<p>To use regular expressions in Python we need to import the <tt class="doctest"><span class="pre">re</span></tt>
library using: <tt class="doctest"><span class="pre">import re</span></tt>.  We also need a list of words to search;
we'll use the Words Corpus again (<a href="#id26"><span class="problematic" id="id27">sec-lexical-resources_</span></a>).  We
will preprocess it to remove any proper names.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> import re
>>> wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]</td>
</tr></table></td></tr>
</table></div>
<div class="section" id="using-basic-meta-characters">
<h3>Using Basic Meta-Characters</h3>
<p>Let's find words ending with <span class="example">ed</span> using the regular expression &#171;<tt class="doctest"><span class="pre">ed$</span></tt>&#187;.
We will use the <tt class="doctest"><span class="pre">re.search(p, s)</span></tt> function to check whether the pattern <tt class="doctest"><span class="pre">p</span></tt> can be found
somewhere inside the string <tt class="doctest"><span class="pre">s</span></tt>.
We need to specify the characters of interest, and use the dollar sign which has a
special behavior in the context of regular expressions in that it matches
the end of the word:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> [w for w in wordlist if re.search('ed$', w)]
['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', ...]</td>
</tr></table></td></tr>
</table></div>
<p>The <tt class="doctest"><span class="pre">.</span></tt> <a name="wildcard_index_term" /><span class="termdef">wildcard</span> symbol matches any single character.
Suppose we have room in a crossword puzzle for an 8-letter word
with <span class="example">j</span> as its third letter and <span class="example">t</span> as its sixth letter.
In place of each blank cell we use a period:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> [w for w in wordlist if re.search('^..j..t..$', w)]
['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', ...]</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
The caret symbol <tt class="doctest"><span class="pre">^</span></tt> matches the start of a string, just like the <tt class="doctest"><span class="pre">$</span></tt> matches
the end.  What results do we get with the above example if we leave out
both of these, and search for &#171;<tt class="doctest"><span class="pre">..j..t..</span></tt>&#187;?</p>
</div>
<p>Finally, the <tt class="doctest"><span class="pre">?</span></tt> symbol specifies that the previous character is optional.
Thus &#171;<tt class="doctest"><span class="pre">^e-?mail$</span></tt>&#187; will match both <span class="example">email</span> and <span class="example">e-mail</span>.
We could count the total number of occurrences of this word (in either spelling)
in a text using <tt class="doctest"><span class="pre">sum(1 for w in text if re.search('^e-?mail$', w))</span></tt>.</p>
</div>
<div class="section" id="ranges-and-closures">
<h3>Ranges and Closures</h3>
<span class="target" id="fig-t9"></span><div class="figure" id="fig-t9">
<img alt="../images/T9.png" src="../images/T9.png" />
<p class="caption"><span class="caption-label">Figure 3.1</span>: T9: Text on 9 Keys</p>
</div>
<p>The <a name="t9_index_term" /><span class="termdef">T9</span> system is used for entering text on mobile phones (see <a class="reference internal" href="#fig-t9">3.1</a>).  Two or more words that
are entered with the same sequence of keystrokes are known as <a name="textonyms_index_term" /><span class="termdef">textonyms</span>.
For example, both <span class="example">hole</span> and <span class="example">golf</span> are entered by pressing
the sequence 4653.  What other words
could be produced with the same sequence?  Here we use the regular expression
&#171;<tt class="doctest"><span class="pre">^[ghi][mno][jlk][def]$</span></tt>&#187;:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]
['gold', 'golf', 'hold', 'hole']</td>
</tr></table></td></tr>
</table></div>
<p>The first part of the expression, &#171;<tt class="doctest"><span class="pre">^[ghi]</span></tt>&#187;, matches the start of
a word followed by <span class="example">g</span>, <span class="example">h</span>, or <span class="example">i</span>.  The next part of the expression,
&#171;<tt class="doctest"><span class="pre">[mno]</span></tt>&#187;, constrains the second character to be
<span class="example">m</span>, <span class="example">n</span>, or <span class="example">o</span>.  The third and fourth characters are also constrained.
Only four words satisfy all these constraints.
Note that the order of characters inside the square brackets is not significant, so we
could have written &#171;<tt class="doctest"><span class="pre">^[hig][nom][ljk][fed]$</span></tt>&#187; and matched the same
words.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Look for some &quot;finger-twisters&quot;, by searching for words that only use part
of the number-pad.  For example &#171;<tt class="doctest"><span class="pre">^[ghijklmno]+$</span></tt>&#187;, or
more concisely, &#171;<tt class="doctest"><span class="pre">^[g-o]+$</span></tt>&#187;, will match words
that only use keys 4, 5, 6 in the center row, and &#171;<tt class="doctest"><span class="pre">^[a-fj-o]+$</span></tt>&#187;
will match words that use keys 2, 3, 5, 6 in the top-right corner.
What do <tt class="doctest"><span class="pre">-</span></tt> and <tt class="doctest"><span class="pre">+</span></tt> mean?</p>
</div>
<p>Let's explore the <tt class="doctest"><span class="pre">+</span></tt> symbol a bit further.  Notice that it can be applied to
individual letters, or to bracketed sets of letters:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))
>>> [w for w in chat_words if re.search('^m+i+n+e+$', w)]
['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine',
'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']
>>> [w for w in chat_words if re.search('^[ha]+$', w)]
['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh',
'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa',
'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', ...]</td>
</tr></table></td></tr>
</table></div>
<p>It should be clear that <tt class="doctest"><span class="pre">+</span></tt> simply means &quot;one or more instances of the preceding item&quot;,
which could be an individual character like <tt class="doctest"><span class="pre">m</span></tt>, a set like <tt class="doctest"><span class="pre">[fed]</span></tt> or a range like <tt class="doctest"><span class="pre">[d-f]</span></tt>.
Now let's replace <tt class="doctest"><span class="pre">+</span></tt> with <tt class="doctest"><span class="pre">*</span></tt>, which means &quot;zero or more instances of the preceding item&quot;.
The regular expression &#171;<tt class="doctest"><span class="pre">^m*i*n*e*$</span></tt>&#187; will match everything that we found using
&#171;<tt class="doctest"><span class="pre">^m+i+n+e+$</span></tt>&#187;, but also words where some of the letters don't appear at all,
e.g. <span class="example">me</span>, <span class="example">min</span>, and <span class="example">mmmmm</span>.
Note that the <tt class="doctest"><span class="pre">+</span></tt> and <tt class="doctest"><span class="pre">*</span></tt> symbols are sometimes referred to as <a name="kleene_closures_index_term" /><span class="termdef">Kleene closures</span>,
or simply <a name="closures_index_term" /><span class="termdef">closures</span>.</p>
<p>The <tt class="doctest"><span class="pre">^</span></tt> operator has another function when it appears as the first
character inside square brackets.  For
example &#171;<tt class="doctest"><span class="pre">[^aeiouAEIOU]</span></tt>&#187; matches any character other than a vowel.
We can search the NPS Chat Corpus for words that are made up entirely of non-vowel
characters using &#171;<tt class="doctest"><span class="pre">^[^aeiouAEIOU]+$</span></tt>&#187; to find items like these:
<tt class="doctest"><span class="pre">:):):)</span></tt>, <tt class="doctest"><span class="pre">grrr</span></tt>, <tt class="doctest"><span class="pre">cyb3r</span></tt> and <tt class="doctest"><span class="pre">zzzzzzzz</span></tt>.  Notice this includes
non-alphabetic characters.</p>
<p>Here are some more examples of regular expressions being used to find tokens
that match a particular pattern, illustrating the use of some new symbols:
<tt class="doctest"><span class="pre">\</span></tt>, <tt class="doctest"><span class="pre">{}</span></tt>, <tt class="doctest"><span class="pre">()</span></tt>, and <tt class="doctest"><span class="pre">|</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> wsj = sorted(set(nltk.corpus.treebank.words()))
>>> [w for w in wsj if re.search('^[0-9]+\.[0-9]+$', w)]
['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5',
'0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99',
'1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', ...]
>>> [w for w in wsj if re.search('^[A-Z]+\$$', w)]
['C$', 'US$']
>>> [w for w in wsj if re.search('^[0-9]{4}$', w)]
['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ...]
>>> [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]
['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', ...]
>>> [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]
['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting',
'savings-and-loan']
>>> [w for w in wsj if re.search('(ed|ing)$', w)]
['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', ...]</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Study the above examples and try to work out what the
<tt class="doctest"><span class="pre">\</span></tt>, <tt class="doctest"><span class="pre">{}</span></tt>, <tt class="doctest"><span class="pre">()</span></tt>, and <tt class="doctest"><span class="pre">|</span></tt> notations mean before
you read on.</p>
</div>
<p>You probably worked out that a backslash means that the following character is
deprived of its special powers and must literally match a specific character in the
word.  Thus, while <tt class="doctest"><span class="pre">.</span></tt> is special, <tt class="doctest"><span class="pre">\.</span></tt> only matches a period.
The braced expressions, like <tt class="doctest"><span class="pre">{3,5}</span></tt>, specify the number of repeats of the previous item.
The pipe character indicates a choice between the material on its left or its right.
Parentheses indicate the scope of an operator: they can be used together with
the pipe (or disjunction) symbol like this: &#171;<tt class="doctest"><span class="pre">w(i|e|ai|oo)t</span></tt>&#187;, matching <span class="example">wit</span>,
<span class="example">wet</span>, <span class="example">wait</span>, and <span class="example">woot</span>.  It is instructive to see what happens when
you omit the parentheses from the last expression above, and search for
&#171;<tt class="doctest"><span class="pre">ed|ing$</span></tt>&#187;.</p>
<p>The meta-characters we have seen are summarized in <a class="reference internal" href="#tab-regexp-meta-characters1">3.3</a>.</p>
<span class="target" id="tab-regexp-meta-characters1"></span><table border="1" class="docutils" id="tab-regexp-meta-characters1">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Operator</th>
<th class="head">Behavior</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><tt class="doctest"><span class="pre">.</span></tt></td>
<td>Wildcard, matches any character</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">^abc</span></tt></td>
<td>Matches some pattern <span class="math">abc</span> at the start of a string</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">abc$</span></tt></td>
<td>Matches some pattern <span class="math">abc</span> at the end of a string</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">[abc]</span></tt></td>
<td>Matches one of a set of characters</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">[A-Z0-9]</span></tt></td>
<td>Matches one of a range of characters</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">ed|ing|s</span></tt></td>
<td>Matches one of the specified strings (disjunction)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">*</span></tt></td>
<td>Zero or more of previous item, e.g. <tt class="doctest"><span class="pre">a*</span></tt>, <tt class="doctest"><span class="pre">[a-z]*</span></tt> (also known as <em>Kleene Closure</em>)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">+</span></tt></td>
<td>One or more of previous item, e.g. <tt class="doctest"><span class="pre">a+</span></tt>, <tt class="doctest"><span class="pre">[a-z]+</span></tt></td>
</tr>
<tr><td><tt class="doctest"><span class="pre">?</span></tt></td>
<td>Zero or one of the previous item (i.e. optional), e.g. <tt class="doctest"><span class="pre">a?</span></tt>, <tt class="doctest"><span class="pre">[a-z]?</span></tt></td>
</tr>
<tr><td><tt class="doctest"><span class="pre">{n}</span></tt></td>
<td>Exactly <span class="math">n</span> repeats where n is a non-negative integer</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">{n,}</span></tt></td>
<td>At least <span class="math">n</span> repeats</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">{,n}</span></tt></td>
<td>No more than <span class="math">n</span> repeats</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">{m,n}</span></tt></td>
<td>At least <span class="math">m</span> and no more than <span class="math">n</span> repeats</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">a(b|c)+</span></tt></td>
<td>Parentheses that indicate the scope of the operators</td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 3.3</span>: <p>Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures</p>
</p>
</table>
<p>To the Python interpreter, a regular expression is just like any other string.
If the string contains a backslash followed by particular characters, it will
interpret these specially.  For example <tt class="doctest"><span class="pre">\b</span></tt> would be interpreted as the
backspace character.  In general, when using regular expressions containing
backslash, we should instruct the interpreter not to look inside the string
at all, but simply to pass it directly to the <tt class="doctest"><span class="pre">re</span></tt> library for processing.
We do this by prefixing the string with the letter <tt class="doctest"><span class="pre">r</span></tt>, to indicate that
it is a <a name="raw_string_index_term" /><span class="termdef">raw string</span>.  For example, the raw string <tt class="doctest"><span class="pre">r'\band\b'</span></tt>
contains two <tt class="doctest"><span class="pre">\b</span></tt> symbols that are interpreted by the <tt class="doctest"><span class="pre">re</span></tt> library
as matching word boundaries instead of backspace characters.
If you get into the habit of using <tt class="doctest"><span class="pre">r'...'</span></tt> for regular expressions
&#8212; as we will do from now on &#8212; you will avoid having to think about
these complications.</p>
</div>
</div>
<div class="section" id="useful-applications-of-regular-expressions">
<span id="sec-useful-applications-of-regular-expressions"></span><h2>3.5&nbsp;&nbsp;&nbsp;Useful Applications of Regular Expressions</h2>
<p>The above examples all involved searching for words <span class="math">w</span>
that match some regular expression <span class="example">regexp</span> using <tt class="doctest"><span class="pre">re.search(regexp, w)</span></tt>.
Apart from checking if a regular expression matches a word, we can use
regular expressions to extract material from words, or to modify words
in specific ways.</p>
<div class="section" id="extracting-word-pieces">
<h3>Extracting Word Pieces</h3>
<p>The <tt class="doctest"><span class="pre">re.findall()</span></tt> (&quot;find all&quot;) method finds all (non-overlapping)
matches of the given regular expression.  Let's find all the vowels in
a word, then count them:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> word = 'supercalifragilisticexpialidocious'
>>> re.findall(r'[aeiou]', word)
['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']
>>> len(re.findall(r'[aeiou]', word))
16</td>
</tr></table></td></tr>
</table></div>
<p>Let's look for all sequences of two or more vowels in some text,
and determine their relative frequency:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> wsj = sorted(set(nltk.corpus.treebank.words()))
>>> fd = nltk.FreqDist(vs for word in wsj
...                       for vs in re.findall(r'[aeiou]{2,}', word))
>>> fd.most_common(12)
[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253),
('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95)]</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>Your Turn:</strong>
In the W3C Date Time Format, dates are represented like this: 2009-12-31.
Replace the <tt class="doctest"><span class="pre">?</span></tt> in the following Python code with a regular expression,
in order to convert the string <tt class="doctest"><span class="pre">'2009-12-31'</span></tt> to a list of integers
<tt class="doctest"><span class="pre">[2009, 12, 31]</span></tt>:</p>
<p class="last"><tt class="doctest"><span class="pre">[int(n) for n in re.findall(?, '2009-12-31')]</span></tt></p>
</div>
</div>
<div class="section" id="doing-more-with-word-pieces">
<h3>Doing More with Word Pieces</h3>
<p>Once we can use <tt class="doctest"><span class="pre">re.findall()</span></tt> to extract material from words, there's
interesting things to do with the pieces, like glue them back together or
plot them.</p>
<p>It is sometimes noted that English text is highly redundant, and it is still
easy to read when word-internal vowels are left out.  For example,
<span class="example">declaration</span> becomes <span class="example">dclrtn</span>, and <span class="example">inalienable</span> becomes <span class="example">inlnble</span>,
retaining any initial or final vowel sequences.   The regular expression
in our next example matches initial vowel sequences, final vowel sequences, and all consonants;
everything else is ignored.  This three-way disjunction is processed left-to-right,
if one of the three parts matches the word, any later parts of the regular
expression are ignored.
We use <tt class="doctest"><span class="pre">re.findall()</span></tt> to extract all the matching
pieces, and <tt class="doctest"><span class="pre">''.join()</span></tt> to join them together (see <a class="reference internal" href="#sec-formatting">3.9</a> for
more about the join operation).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'
>>> def compress(word):
...     pieces = re.findall(regexp, word)
...     return ''.join(pieces)
...
>>> english_udhr = nltk.corpus.udhr.words('English-Latin1')
>>> print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))
Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and
of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn
of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn
rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,
and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and</td>
</tr></table></td></tr>
</table></div>
<p>Next, let's combine regular expressions with conditional frequency
distributions.  Here we will extract all consonant-vowel sequences
from the words of Rotokas, such as <span class="example">ka</span> and <span class="example">si</span>.  Since each of
these is a pair, it can be used to initialize a conditional frequency
distribution.  We then tabulate the frequency of each pair:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')
>>> cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]
>>> cfd = nltk.ConditionalFreqDist(cvs)
>>> cfd.tabulate()
    a    e    i    o    u
k  418  148   94  420  173
p   83   31  105   34   51
r  187   63   84   89   79
s    0    0  100    2    1
t   47    8    0  148   37
v   93   27  105   48   49</td>
</tr></table></td></tr>
</table></div>
<p>Examining the rows for <span class="example">s</span> and <span class="example">t</span>, we see they are in partial
&quot;complementary distribution&quot;, which is evidence that they are not
distinct phonemes in the language.  Thus, we could conceivably drop
<span class="example">s</span> from the Rotokas alphabet and simply have a pronunciation rule
that the letter <span class="example">t</span> is pronounced <span class="example">s</span> when followed by
<span class="example">i</span>.  (Note that the single entry having <em>su</em>, namely <em>kasuari</em>,
'cassowary' is borrowed from English.)</p>
<p>If we want to be able to inspect the words behind the numbers in the above table,
it would be helpful to have an index, allowing us to quickly find the list of words
that contains a given consonant-vowel pair, e.g. <tt class="doctest"><span class="pre">cv_index['su']</span></tt> should give us
all words containing <span class="example">su</span>.  Here's how we can do this:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> cv_word_pairs = [(cv, w) for w in rotokas_words
...                          for cv in re.findall(r'[ptksvr][aeiou]', w)]
>>> cv_index = nltk.Index(cv_word_pairs)
>>> cv_index['su']
['kasuari']
>>> cv_index['po']
['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa',
'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', ...]</td>
</tr></table></td></tr>
</table></div>
<p>This program processes each word <tt class="doctest"><span class="pre">w</span></tt> in turn, and for each one, finds every
substring that matches the regular expression &#171;<tt class="doctest"><span class="pre">[ptksvr][aeiou]</span></tt>&#187;.
In the case of the word <span class="example">kasuari</span>, it finds <span class="example">ka</span>, <span class="example">su</span> and <span class="example">ri</span>.
Therefore, the <tt class="doctest"><span class="pre">cv_word_pairs</span></tt> list will contain <tt class="doctest"><span class="pre">('ka', 'kasuari')</span></tt>,
<tt class="doctest"><span class="pre">('su', 'kasuari')</span></tt> and <tt class="doctest"><span class="pre">('ri', 'kasuari')</span></tt>.  One further step, using
<tt class="doctest"><span class="pre">nltk.Index()</span></tt>, converts this into a useful index.</p>
</div>
<div class="section" id="finding-word-stems">
<h3>Finding Word Stems</h3>
<p>When we use a web search engine, we usually don't mind (or even notice)
if the words in the document differ from our search terms in having
different endings.  A query for <span class="example">laptops</span> finds documents
containing <span class="example">laptop</span> and vice versa.
Indeed, <span class="example">laptop</span> and <span class="example">laptops</span> are just two forms of the
same dictionary word (or lemma).
For some language processing tasks we want to ignore word endings, and just
deal with word stems.</p>
<p>There are various ways we can pull out the stem of a word.  Here's a simple-minded
approach which just strips off anything that looks like a suffix:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def stem(word):
...     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:
...         if word.endswith(suffix):
...             return word[:-len(suffix)]
...     return word</td>
</tr></table></td></tr>
</table></div>
<p>Although we will ultimately use NLTK's built-in stemmers, it's interesting
to see how we can use regular expressions for this task.  Our first step is
to build up a disjunction of all the suffixes.  We need to enclose it in parentheses
in order to limit the scope of the disjunction.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
['ing']</td>
</tr></table></td></tr>
</table></div>
<p>Here, <tt class="doctest"><span class="pre">re.findall()</span></tt> just gave us the suffix even though the regular expression
matched the entire word.  This is because the parentheses have a second function,
to select substrings to be extracted.  If we want to use the parentheses to
specify the scope of the disjunction, but not to select the material to be output,
we have to add <tt class="doctest"><span class="pre">?:</span></tt>,
which is just one of many arcane subtleties of regular expressions.
Here's the revised version.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
['processing']</td>
</tr></table></td></tr>
</table></div>
<p>However, we'd actually like to split the word into stem and suffix.
So we should just parenthesize both parts of the regular expression:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
[('process', 'ing')]</td>
</tr></table></td></tr>
</table></div>
<p>This looks promising, but still has a problem.  Let's look at a different
word, <span class="example">processes</span>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')
[('processe', 's')]</td>
</tr></table></td></tr>
</table></div>
<p>The regular expression incorrectly found an <span class="example">-s</span> suffix instead of
an <span class="example">-es</span> suffix.  This demonstrates another subtlety: the star operator
is &quot;greedy&quot; and the <tt class="doctest"><span class="pre">.*</span></tt> part of the expression tries to consume as much of the input
as possible.  If we use the &quot;non-greedy&quot; version of the star operator, written <tt class="doctest"><span class="pre">*?</span></tt>,
we get what we want:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')
[('process', 'es')]</td>
</tr></table></td></tr>
</table></div>
<p>This works even when we allow an empty suffix, by making the content of the
second parentheses optional:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')
[('language', '')]</td>
</tr></table></td></tr>
</table></div>
<p>This approach still has many problems (can you spot them?) but we will move
on to define a function to perform stemming, and apply it to a whole text:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> def stem(word):
...     regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'
...     stem, suffix = re.findall(regexp, word)[0]
...     return stem
...
>>> raw = """DENNIS: Listen, strange women lying in ponds distributing swords
... is no basis for a system of government.  Supreme executive power derives from
... a mandate from the masses, not from some farcical aquatic ceremony."""
>>> tokens = word_tokenize(raw)
>>> [stem(t) for t in tokens]
['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut',
'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme',
'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',',
'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']</td>
</tr></table></td></tr>
</table></div>
<p>Notice that our regular expression removed the <span class="example">s</span> from <span class="example">ponds</span> but also from <span class="example">is</span>
and <span class="example">basis</span>.  It produced some non-words like <span class="example">distribut</span> and <span class="example">deriv</span>, but these
are acceptable stems in some applications.</p>
</div>
<div class="section" id="searching-tokenized-text">
<h3>Searching Tokenized Text</h3>
<p>You can use a special kind of regular expression for searching across multiple words
in a text (where a text is a list of tokens).  For example, <tt class="doctest"><span class="pre">"<a> <man>"</span></tt> finds all
instances of <span class="example">a man</span> in the text.  The angle brackets are used to mark token boundaries,
and any whitespace between the angle brackets is ignored (behaviors that are unique
to NLTK's <tt class="doctest"><span class="pre">findall()</span></tt> method for texts).  In the following example, we include
<tt class="doctest"><span class="pre"><.*></span></tt> <a class="reference internal" href="#single-token-wildcard"><span id="ref-single-token-wildcard"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> which will match any single token, and enclose it in parentheses so only the
matched word (e.g. <span class="example">monied</span>) and not the matched phrase (e.g. <span class="example">a monied man</span>)
is produced.  The second example finds three-word phrases ending with the word <span class="example">bro</span>
<a class="reference internal" href="#three-word-phrases"><span id="ref-three-word-phrases"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.  The last example finds sequences of three or more words starting with
the letter <span class="example">l</span> <a class="reference internal" href="#letter-l"><span id="ref-letter-l"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import gutenberg, nps_chat
>>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))
>>> moby.findall(r"<a> (<.*>) <man>") # [_single-token-wildcard]
monied; nervous; dangerous; white; white; white; pious; queer; good;
mature; white; Cape; great; wise; wise; butterless; white; fiendish;
pale; furious; better; certain; complete; dismasted; younger; brave;
brave; brave; brave
>>> chat = nltk.Text(nps_chat.words())
>>> chat.findall(r"<.*> <.*> <bro>") # [_three-word-phrases]
you rule bro; telling you bro; u twizted bro
>>> chat.findall(r"<l.*>{3,}") # [_letter-l]
lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la
la la; lovely lol lol love; lol lol lol.; la la la; la la la</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Consolidate your understanding of regular expression patterns and substitutions using
<tt class="doctest"><span class="pre">nltk.re_show(</span></tt><em>p, s</em><tt class="doctest"><span class="pre">)</span></tt> which annotates the string <em>s</em> to show every place where
pattern <em>p</em> was matched, and <tt class="doctest"><span class="pre">nltk.app.nemo()</span></tt> which provides a graphical
interface for exploring regular expressions.  For more practice, try some
of the exercises on regular expressions at the end of this chapter.</p>
</div>
<!-- TODO: Add code example for type-instance relations. -->
<p>It is easy to build search patterns when the linguistic phenomenon we're
studying is tied to particular words.  In some cases, a little creativity
will go a long way.  For instance, searching a large text corpus for
expressions of the form <span class="example">x and other ys</span> allows us to discover
hypernyms (cf <a href="#id28"><span class="problematic" id="id29">sec-wordnet_</span></a>):</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from nltk.corpus import brown
>>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))
>>> hobbies_learned.findall(r"<\w*> <and> <other> <\w*s>")
speed and other activities; water and other liquids; tomb and other
landmarks; Statues and other monuments; pearls and other jewels;
charts and other items; roads and other features; figures and other
objects; military and other areas; demands and other factors;
abstracts and other compilations; iron and other metals</td>
</tr></table></td></tr>
</table></div>
<p>With enough text, this approach would give us a useful store
of information about the taxonomy of objects, without the need for
any manual labor.  However, our search results will usually
contain false positives, i.e. cases that we would want to exclude.
For example, the result: <span class="example">demands and other factors</span> suggests
that <span class="example">demand</span> is an instance of the type <span class="example">factor</span>, but this
sentence is actually about wage demands.  Nevertheless, we could
construct our own ontology of English concepts by manually correcting
the output of such searches.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This combination of automatic and manual processing is the most common
way for new corpora to be constructed.  We will return to this in
<a href="#id30"><span class="problematic" id="id31">chap-data_</span></a>.</p>
</div>
<p>Searching corpora also suffers from the problem of false negatives,
i.e. omitting cases that we would want to include.  It is risky to
conclude that some linguistic phenomenon doesn't exist in a corpus
just because we couldn't find any instances of a search pattern.
Perhaps we just didn't think carefully enough about suitable patterns.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Look for instances of the pattern <span class="example">as x as y</span> to discover
information about entities and their properties.</p>
</div>
<!-- searching for doubled final consonants: .*([bdgptk])\1ed
transforming date strings
HTML stripping
spelling correction
textonyms -->
</div>
</div>
<div class="section" id="normalizing-text">
<span id="sec-normalizing-text"></span><h2>3.6&nbsp;&nbsp;&nbsp;Normalizing Text</h2>
<p>In earlier program examples we have often converted text to lowercase before
doing anything with its words, e.g. <tt class="doctest"><span class="pre">set(w.lower() for w in text)</span></tt>.
By using <tt class="doctest"><span class="pre">lower()</span></tt>, we have <a name="normalized_index_term" /><span class="termdef">normalized</span> the text to lowercase so that
the distinction between <span class="example">The</span> and <span class="example">the</span> is ignored.  Often we want
to go further than this, and strip off any affixes, a task known as stemming.
A further step is to make sure that the resulting form is a known word in a dictionary,
a task known as lemmatization.  We discuss each of these in turn.  First, we need
to define the data we will use in this section:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> raw = """DENNIS: Listen, strange women lying in ponds distributing swords
... is no basis for a system of government.  Supreme executive power derives from
... a mandate from the masses, not from some farcical aquatic ceremony."""
>>> tokens = word_tokenize(raw)</td>
</tr></table></td></tr>
</table></div>
<div class="section" id="stemmers">
<h3>Stemmers</h3>
<p>NLTK includes several off-the-shelf stemmers, and if you ever need
a stemmer you should use one of these in preference to crafting your own
using regular expressions, since these handle a wide range of irregular cases.
The Porter and Lancaster stemmers follow their own rules for stripping affixes.
Observe that the Porter stemmer correctly handles the word <span class="example">lying</span>
(mapping it to <span class="example">lie</span>), while the Lancaster stemmer does not.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> porter = nltk.PorterStemmer()
>>> lancaster = nltk.LancasterStemmer()
>>> [porter.stem(t) for t in tokens]
['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond',
'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',
'.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from',
'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']
>>> [lancaster.stem(t) for t in tokens]
['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut',
'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem',
'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not',
'from', 'som', 'farc', 'aqu', 'ceremony', '.']</td>
</tr></table></td></tr>
</table></div>
<p>Stemming is not a well-defined process, and we typically pick the stemmer that best
suits the application we have in mind.  The Porter Stemmer is a good choice if you
are indexing some texts and want to support search using alternative forms of
words (illustrated in <a class="reference internal" href="#code-stemmer-indexing">3.2</a>, which uses <em>object oriented</em>
programming techniques that are outside the scope of this book, string formatting
techniques to be covered in <a class="reference internal" href="#sec-formatting">3.9</a>, and the <tt class="doctest"><span class="pre">enumerate()</span></tt> function
to be explained in <a href="#id32"><span class="problematic" id="id33">sec-sequences_</span></a>).</p>
<span class="target" id="code-stemmer-indexing"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">class IndexedText(object):

    def __init__(self, stemmer, text):
        self._text = text
        self._stemmer = stemmer
        self._index = nltk.Index((self._stem(word), i)
                                 for (i, word) in enumerate(text))

    def concordance(self, word, width=40):
        key = self._stem(word)
        wc = int(width/4)                # words of context
        for i in self._index[key]:
            lcontext = ' '.join(self._text[i-wc:i])
            rcontext = ' '.join(self._text[i:i+wc])
            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)
            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)
            print(ldisplay, rdisplay)

    def _stem(self, word):
        return self._stemmer.stem(word).lower()</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> porter = nltk.PorterStemmer()
>>> grail = nltk.corpus.webtext.words('grail.txt')
>>> text = IndexedText(porter, grail)
>>> text.concordance('lie')
r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no
 beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of
       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !
doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well
ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which
   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --
h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k
not stop our fight ' til each one of you lies dead , and the Holy Grail returns t</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_stemmer_indexing.py" type="text/x-python"><span class="caption-label">Example 3.2 (code_stemmer_indexing.py)</span></a>: <span class="caption-label">Figure 3.2</span>: Indexing a Text Using a Stemmer</td></tr></p>
</table></div>
</div>
<div class="section" id="lemmatization">
<h3>Lemmatization</h3>
<p>The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary.
This additional checking process makes the lemmatizer slower than the above stemmers.
Notice that it doesn't handle <span class="example">lying</span>, but it converts <span class="example">women</span> to <span class="example">woman</span>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> wnl = nltk.WordNetLemmatizer()
>>> [wnl.lemmatize(t) for t in tokens]
['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond',
'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of',
'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a',
'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical',
'aquatic', 'ceremony', '.']</td>
</tr></table></td></tr>
</table></div>
<p>The WordNet lemmatizer is a good choice if you want to compile the vocabulary
of some texts and want a list of valid lemmas (or lexicon headwords).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Another normalization task involves identifying <a name="non_standard_words_index_term" /><span class="termdef">non-standard words</span>
including numbers, abbreviations, and dates, and mapping any such tokens
to a special vocabulary.  For example, every decimal number could be
mapped to a single token <tt class="doctest"><span class="pre">0.0</span></tt>, and every acronym could be mapped to <tt class="doctest"><span class="pre">AAA</span></tt>.
This keeps the vocabulary small and improves the accuracy of many
language modeling tasks.</p>
</div>
<!-- Non-Standard Words
- - - - - - - - - - - - - - - - - -

[Discuss the practice of mapping words such as numbers, abbreviations, dates to
a special vocabulary, based on Sproat et al 2001; new NLTK support planned...] -->
</div>
</div>
<div class="section" id="regular-expressions-for-tokenizing-text">
<span id="sec-tokenization"></span><h2>3.7&nbsp;&nbsp;&nbsp;Regular Expressions for Tokenizing Text</h2>
<p>Tokenization is the task of cutting a string into
identifiable linguistic units that constitute a piece of language data.
Although it is a fundamental task, we have been able to
delay it until now because many corpora are already tokenized,
and because NLTK includes some tokenizers.
Now that you are familiar with regular expressions,
you can learn how to use them to tokenize text, and to
have much more control over the process.</p>
<div class="section" id="simple-approaches-to-tokenization">
<h3>Simple Approaches to Tokenization</h3>
<p>The very simplest method for tokenizing text is to split on whitespace.
Consider the following text from <em>Alice's Adventures in Wonderland</em>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> raw = """'When I'M a Duchess,' she said to herself, (not in a very hopeful tone
... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very
... well without--Maybe it's always pepper that makes people hot-tempered,'..."""</td>
</tr></table></td></tr>
</table></div>
<p>We could split this raw text on whitespace using <tt class="doctest"><span class="pre">raw.split()</span></tt>.
To do the same using a regular expression, it is not enough to match
any space characters in the string <a class="reference internal" href="#split-space"><span id="ref-split-space"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> since this results
in tokens that contain a <tt class="doctest"><span class="pre">\n</span></tt> newline character; instead we need
to match any number of spaces, tabs, or newlines <a class="reference internal" href="#split-whitespace"><span id="ref-split-whitespace"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.split(r' ', raw) # [_split-space]
["'When", "I'M", 'a', "Duchess,'", 'she', 'said', 'to', 'herself,', '(not', 'in',
'a', 'very', 'hopeful', 'tone\nthough),', "'I", "won't", 'have', 'any', 'pepper',
'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\nwell', 'without--Maybe',
"it's", 'always', 'pepper', 'that', 'makes', 'people', "hot-tempered,'..."]
>>> re.split(r'[ \t\n]+', raw) # [_split-whitespace]
["'When", "I'M", 'a', "Duchess,'", 'she', 'said', 'to', 'herself,', '(not', 'in',
'a', 'very', 'hopeful', 'tone', 'though),', "'I", "won't", 'have', 'any', 'pepper',
'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe',
"it's", 'always', 'pepper', 'that', 'makes', 'people', "hot-tempered,'..."]</td>
</tr></table></td></tr>
</table></div>
<p>The regular expression &#171;<tt class="doctest"><span class="pre">[ \t\n]+</span></tt>&#187; matches one or more space, tab (<tt class="doctest"><span class="pre">\t</span></tt>)
or newline (<tt class="doctest"><span class="pre">\n</span></tt>).  Other whitespace characters, such as carriage-return and
form-feed should really be included too.  Instead, we will use a built-in
<tt class="doctest"><span class="pre">re</span></tt> abbreviation, <tt class="doctest"><span class="pre">\s</span></tt>, which means any whitespace character.  The above
statement can be rewritten as <tt class="doctest"><span class="pre">re.split(r'\s+', raw)</span></tt>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Important:</strong>
Remember to prefix regular expressions with the letter <tt class="doctest"><span class="pre">r</span></tt>
(meaning &quot;raw&quot;), which instructs the Python
interpreter to treat the string literally, rather than
processing any backslashed characters it contains.</p>
</div>
<p>Splitting on whitespace gives us tokens like <tt class="doctest"><span class="pre">'(not'</span></tt> and <tt class="doctest"><span class="pre">'herself,'</span></tt>.
An alternative is to use the fact that Python provides us with a
character class <tt class="doctest"><span class="pre">\w</span></tt> for word characters, equivalent to <tt class="doctest"><span class="pre">[a-zA-Z0-9_]</span></tt>.
It also defines the complement of this class <tt class="doctest"><span class="pre">\W</span></tt>, i.e. all characters
other than letters, digits or underscore.  We can use <tt class="doctest"><span class="pre">\W</span></tt> in a simple
regular expression to split the input on anything <em>other</em> than a word character:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.split(r'\W+', raw)
['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in',
'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper',
'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without',
'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered',
'']</td>
</tr></table></td></tr>
</table></div>
<p>Observe that this gives us empty strings at the start and the end (to understand
why, try doing <tt class="doctest"><span class="pre">'xx'.split('x')</span></tt>).  We get the same tokens, but without the empty strings,
with <tt class="doctest"><span class="pre">re.findall(r'\w+', raw)</span></tt>, using a pattern that matches the words instead of the spaces.
Now that we're matching the words, we're in a position to extend the regular expression
to cover a wider range of cases.
The regular expression &#171;<tt class="doctest"><span class="pre">\w+|\S\w*</span></tt>&#187; will first try to match any sequence
of word characters.  If no match is found, it will try to match any
<em>non</em>-whitespace character (<tt class="doctest"><span class="pre">\S</span></tt> is the complement of <tt class="doctest"><span class="pre">\s</span></tt>) followed by
further word characters.  This means that punctuation is grouped with any following
letters (e.g. <span class="example">'s</span>) but that sequences of two or more punctuation
characters are separated.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> re.findall(r'\w+|\S\w*', raw)
["'When", 'I', "'M", 'a', 'Duchess', ',', "'", 'she', 'said', 'to', 'herself', ',',
'(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', "'I", 'won', "'t",
'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does',
'very', 'well', 'without', '-', '-Maybe', 'it', "'s", 'always', 'pepper', 'that',
'makes', 'people', 'hot', '-tempered', ',', "'", '.', '.', '.']</td>
</tr></table></td></tr>
</table></div>
<p>Let's generalize the <tt class="doctest"><span class="pre">\w+</span></tt> in the above expression
to permit word-internal hyphens and apostrophes: &#171;<tt class="doctest"><span class="pre">\w+([-']\w+)*</span></tt>&#187;.
This expression means <tt class="doctest"><span class="pre">\w+</span></tt> followed by zero or more instances of <tt class="doctest"><span class="pre">[-']\w+</span></tt>;
it would match <span class="example">hot-tempered</span> and <span class="example">it's</span>.
(We need to include <tt class="doctest"><span class="pre">?:</span></tt> in this expression for reasons discussed earlier.)
We'll also add a pattern to match quote characters so these are kept separate
from the text they enclose.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> print(re.findall(r"\w+(?:[-']\w+)*|'|[-.(]+|\S\w*", raw))
["'", 'When', "I'M", 'a', 'Duchess', ',', "'", 'she', 'said', 'to', 'herself', ',',
'(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', "'", 'I',
"won't", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup',
'does', 'very', 'well', 'without', '--', 'Maybe', "it's", 'always', 'pepper',
'that', 'makes', 'people', 'hot-tempered', ',', "'", '...']</td>
</tr></table></td></tr>
</table></div>
<p>The above expression also included &#171;<tt class="doctest"><span class="pre">[-.(]+</span></tt>&#187; which causes the double hyphen,
ellipsis, and open parenthesis to be tokenized separately.</p>
<p><a class="reference internal" href="#tab-re-symbols">3.4</a> lists the regular expression character class symbols we have
seen in this section, in addition to some other useful symbols.</p>
<span class="target" id="tab-re-symbols"></span><table border="1" class="docutils" id="tab-re-symbols">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Symbol</th>
<th class="head">Function</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><tt class="doctest"><span class="pre">\b</span></tt></td>
<td>Word boundary (zero width)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\d</span></tt></td>
<td>Any decimal digit (equivalent to <tt class="doctest"><span class="pre">[0-9]</span></tt>)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\D</span></tt></td>
<td>Any non-digit character (equivalent to <tt class="doctest"><span class="pre">[^0-9]</span></tt>)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\s</span></tt></td>
<td>Any whitespace character (equivalent to <tt class="doctest"><span class="pre">[ \t\n\r\f\v]</span></tt>)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\S</span></tt></td>
<td>Any non-whitespace character (equivalent to <tt class="doctest"><span class="pre">[^ \t\n\r\f\v]</span></tt>)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\w</span></tt></td>
<td>Any alphanumeric character (equivalent to <tt class="doctest"><span class="pre">[a-zA-Z0-9_]</span></tt>)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\W</span></tt></td>
<td>Any non-alphanumeric character (equivalent to <tt class="doctest"><span class="pre">[^a-zA-Z0-9_]</span></tt>)</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\t</span></tt></td>
<td>The tab character</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">\n</span></tt></td>
<td>The newline character</td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 3.4</span>: <p>Regular Expression Symbols</p>
</p>
</table>
</div>
<div class="section" id="nltk-s-regular-expression-tokenizer">
<h3>NLTK's Regular Expression Tokenizer</h3>
<p>The function <tt class="doctest"><span class="pre">nltk.regexp_tokenize()</span></tt> is similar to <tt class="doctest"><span class="pre">re.findall()</span></tt> (as
we've been using it for tokenization).  However, <tt class="doctest"><span class="pre">nltk.regexp_tokenize()</span></tt>
is more efficient for this task, and avoids the need for special treatment of parentheses.
For readability we break up the regular expression over several lines
and add a comment about each line.  The special <tt class="doctest"><span class="pre">(?x)</span></tt> &quot;verbose flag&quot;
tells Python to strip out the embedded whitespace and comments.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = 'That U.S.A. poster-print costs $12.40...'
>>> pattern = r'''(?x)    # set flag to allow verbose regexps
...     ([A-Z]\.)+        # abbreviations, e.g. U.S.A.
...   | \w+(-\w+)*        # words with optional internal hyphens
...   | \$?\d+(\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
...   | \.\.\.            # ellipsis
...   | [][.,;"'?():-_`]  # these are separate tokens; includes ], [
... '''
>>> nltk.regexp_tokenize(text, pattern)
['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']</td>
</tr></table></td></tr>
</table></div>
<p>When using the verbose flag, you can no longer use <tt class="doctest"><span class="pre">' '</span></tt> to match
a space character; use <tt class="doctest"><span class="pre">\s</span></tt> instead.
The <tt class="doctest"><span class="pre">regexp_tokenize()</span></tt> function has an optional <tt class="doctest"><span class="pre">gaps</span></tt> parameter.
When set to <tt class="doctest"><span class="pre">True</span></tt>, the regular expression specifies the gaps
between tokens, as with <tt class="doctest"><span class="pre">re.split()</span></tt>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We can evaluate a tokenizer by comparing the resulting tokens with a
wordlist, and reporting any tokens that don't appear in the wordlist,
using <tt class="doctest"><span class="pre">set(tokens).difference(wordlist)</span></tt>.  You'll probably want to
lowercase all the tokens first.</p>
</div>
</div>
<div class="section" id="further-issues-with-tokenization">
<h3>Further Issues with Tokenization</h3>
<p>Tokenization turns out to be a far more difficult task than you might have expected.
No single solution works well across-the-board, and we
must decide what counts as a token depending on the application domain.</p>
<p>When developing a tokenizer it helps to have access to raw text which
has been manually tokenized, in order to compare the output of your tokenizer
with high-quality (or &quot;gold-standard&quot;) tokens.  The NLTK corpus
collection includes a sample of Penn Treebank data, including the raw
Wall Street Journal text (<tt class="doctest"><span class="pre">nltk.corpus.treebank_raw.raw()</span></tt>) and
the tokenized version (<tt class="doctest"><span class="pre">nltk.corpus.treebank.words()</span></tt>).</p>
<p>A final issue for tokenization is the presence of contractions, such
as <span class="example">didn't</span>.  If we are analyzing the meaning
of a sentence, it would probably be more useful to normalize this
form to two separate forms: <span class="example">did</span> and <span class="example">n't</span> (or <span class="example">not</span>).
We can do this work with the help of a lookup table.</p>
</div>
</div>
<div class="section" id="segmentation">
<span id="sec-segmentation"></span><h2>3.8&nbsp;&nbsp;&nbsp;Segmentation</h2>
<p>This section discusses more advanced concepts, which you may prefer to skip on the
first time through this chapter.</p>
<p>Tokenization is an instance of a more general problem of <a name="segmentation_index_term" /><span class="termdef">segmentation</span>.
In this section we will look at two other instances of this problem,
which use radically different techniques to the ones we have seen so far
in this chapter.</p>
<div class="section" id="sentence-segmentation">
<h3>Sentence Segmentation</h3>
<p>Manipulating texts at the level of individual words often presupposes
the ability to divide a text into individual sentences.  As we have
seen, some corpora already provide access at the sentence level.  In
the following example, we compute the average number of words per
sentence in the Brown Corpus:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())
20.250994070456922</td>
</tr></table></td></tr>
</table></div>
<p>In other cases, the text is only available as a stream of characters.  Before
tokenizing the text into words, we need to segment it into sentences.  NLTK facilitates
this by including the Punkt sentence segmenter <a href="#id34"><span class="problematic" id="id1">[KissStrunk2006]_</span></a>.
Here is an example of its use in segmenting the text of a novel.
(Note that if the segmenter's internal data has been updated by the time you read this,
you will see different output):</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
>>> sents = nltk.sent_tokenize(text)
>>> pprint.pprint(sents[79:89])
['"Nonsense!"',
 'said Gregory, who was very rational when anyone else\nattempted paradox.',
 '"Why do all the clerks and navvies in the\n'
 'railway trains look so sad and tired, so very sad and tired?',
 'I will\ntell you.',
 'It is because they know that the train is going right.',
 'It\n'
 'is because they know that whatever place they have taken a ticket\n'
 'for that place they will reach.',
 'It is because after they have\n'
 'passed Sloane Square they know that the next station must be\n'
 'Victoria, and nothing but Victoria.',
 'Oh, their wild rapture!',
 'oh,\n'
 'their eyes like stars and their souls again in Eden, if the next\n'
 'station were unaccountably Baker Street!"',
 '"It is you who are unpoetical," replied the poet Syme.']</td>
</tr></table></td></tr>
</table></div>
<p>Notice that this example is really a single sentence, reporting the speech of Mr Lucian Gregory.
However, the quoted speech contains several sentences, and these have been split into individual
strings.  This is reasonable behavior for most applications.</p>
<p>Sentence segmentation is difficult because period is used to mark abbreviations,
and some periods simultaneously mark an abbreviation and terminate a sentence,
as often happens with acronyms like <span class="example">U.S.A.</span></p>
<p>For another approach to sentence segmentation, see <a href="#id35"><span class="problematic" id="id36">sec-further-examples-of-supervised-classification_</span></a>.</p>
</div>
<div class="section" id="word-segmentation">
<h3>Word Segmentation</h3>
<p>For some writing systems, tokenizing text is made more difficult by the fact that there
is no visual representation of word boundaries.
For example, in Chinese, the three-character string: &#29233;&#22269;&#20154;
(ai4 &quot;love&quot; (verb), guo2 &quot;country&quot;, ren2 &quot;person&quot;) could
be tokenized as &#29233;&#22269; / &#20154;, &quot;country-loving person&quot;
or as &#29233; / &#22269;&#20154;, &quot;love country-person.&quot;</p>
<p>A similar problem arises in the processing of spoken language, where the
hearer must segment a continuous speech stream into individual words.
A particularly challenging version of this problem arises when we don't
know the words in advance.  This is the problem faced by a language learner,
such as a child hearing utterances from a parent.  Consider the
following artificial example, where word boundaries have been removed:</p>
<span class="target" id="ex-kitty"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(1)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>doyouseethekitty</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>seethedoggy</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>doyoulikethekitty</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">d.</td><td width="15"></td><td>likethedoggy</td></tr></table></p>
</td></tr></table></p>
<p>Our first challenge is simply to represent the problem: we need to find
a way to separate text content from the segmentation.  We can do this by
annotating each character with a boolean value to indicate whether or
not a word-break appears after the character (an idea that will be used
heavily for &quot;chunking&quot; in <a href="#id37"><span class="problematic" id="id38">chap-chunk_</span></a>).
Let's assume that the learner is given the utterance breaks,
since these often correspond to extended pauses.  Here is a possible representation,
including the initial and target segmentations:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = "doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"
>>> seg1 = "0000000000000001000000000010000000000000000100000000000"
>>> seg2 = "0100100100100001001001000010100100010010000100010010000"</td>
</tr></table></td></tr>
</table></div>
<p>Observe that the segmentation strings consist of zeros and ones.  They
are one character shorter than the source text, since a text of length
<span class="math">n</span> can only be broken up in <span class="math">n-1</span> places.
The <tt class="doctest"><span class="pre">segment()</span></tt> function in <a class="reference internal" href="#code-segment">3.3</a> demonstrates that we can
get back to the original segmented text from the above representation.</p>
<span class="target" id="code-segment"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def segment(text, segs):
    words = []
    last = 0
    for i in range(len(segs)):
        if segs[i] == '1':
            words.append(text[last:i+1])
            last = i+1
    words.append(text[last:])
    return words</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = "doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"
>>> seg1 = "0000000000000001000000000010000000000000000100000000000"
>>> seg2 = "0100100100100001001001000010100100010010000100010010000"
>>> segment(text, seg1)
['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']
>>> segment(text, seg2)
['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you',
'like', 'the', 'kitty', 'like', 'the', 'doggy']</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_segment.py" type="text/x-python"><span class="caption-label">Example 3.3 (code_segment.py)</span></a>: <span class="caption-label">Figure 3.3</span>: Reconstruct Segmented Text from String Representation:
<tt class="doctest"><span class="pre">seg1</span></tt> and <tt class="doctest"><span class="pre">seg2</span></tt> represent the initial and final
segmentations of some hypothetical child-directed speech;
the <tt class="doctest"><span class="pre">segment()</span></tt> function can use them to reproduce the
segmented text.</td></tr></p>
</table></div>
<p>Now the segmentation task becomes a search problem: find the bit string that causes
the text string to be correctly segmented into words.
We assume the learner is acquiring words and storing them in an internal lexicon.
Given a suitable lexicon, it is possible to reconstruct the source text as a sequence of
lexical items.  Following <a href="#id39"><span class="problematic" id="id2">[Brent1995]_</span></a>, we can define an <a name="objective_function_index_term" /><span class="termdef">objective function</span>,
a scoring function whose value we will try to optimize, based on
the size of the lexicon (number of characters in the words plus an extra
delimiter character to mark the end of each word) and the amount of information needed to
reconstruct the source text from the lexicon.  We illustrate this in
<a class="reference internal" href="#fig-brent">3.4</a>.</p>
<span class="target" id="fig-brent"></span><div class="figure" id="fig-brent">
<img alt="../images/brent.png" src="../images/brent.png" />
<p class="caption"><span class="caption-label">Figure 3.4</span>: Calculation of Objective Function: Given a hypothetical segmentation
of the source text (on the left), derive a lexicon and a derivation
table that permit the source text to be reconstructed, then total
up the number of characters used by each lexical item (including a boundary
marker) and the number of lexical items used by each derivation, to serve as a score of the quality of
the segmentation; smaller values of the score indicate a better segmentation.</p>
</div>
<p>It is a simple matter to implement this objective function, as shown in
<a class="reference internal" href="#code-evaluate">3.5</a>.</p>
<span class="target" id="code-evaluate"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def evaluate(text, segs):
    words = segment(text, segs)
    text_size = len(words)
    lexicon_size = sum(len(word) + 1 for word in set(words))
    return text_size + lexicon_size</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = "doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"
>>> seg1 = "0000000000000001000000000010000000000000000100000000000"
>>> seg2 = "0100100100100001001001000010100100010010000100010010000"
>>> seg3 = "0000100100000011001000000110000100010000001100010000001"
>>> segment(text, seg3)
['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like',
 'thekitt', 'y', 'like', 'thedogg', 'y']
>>> evaluate(text, seg3)
47
>>> evaluate(text, seg2)
48
>>> evaluate(text, seg1)
64</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_evaluate.py" type="text/x-python"><span class="caption-label">Example 3.5 (code_evaluate.py)</span></a>: <span class="caption-label">Figure 3.5</span>: Computing the Cost of Storing the Lexicon and Reconstructing the Source Text</td></tr></p>
</table></div>
<p>The final step is to search for the pattern of zeros and ones that minimizes this objective
function, shown in <a class="reference internal" href="#code-anneal">3.6</a>.  Notice that the best segmentation includes &quot;words&quot; like
<span class="example">thekitty</span>, since there's not enough evidence in the data to split this any further.</p>
<span class="target" id="code-anneal"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">from random import randint

def flip(segs, pos):
    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]

def flip_n(segs, n):
    for i in range(n):
        segs = flip(segs, randint(0, len(segs)-1))
    return segs

def anneal(text, segs, iterations, cooling_rate):
    temperature = float(len(segs))
    while temperature > 0.5:
        best_segs, best = segs, evaluate(text, segs)
        for i in range(iterations):
            guess = flip_n(segs, round(temperature))
            score = evaluate(text, guess)
            if score < best:
                best, best_segs = score, guess
        score, segs = best, best_segs
        temperature = temperature / cooling_rate
        print(evaluate(text, segs), segment(text, segs))
    print()
    return segs</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> text = "doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"
>>> seg1 = "0000000000000001000000000010000000000000000100000000000"
>>> anneal(text, seg1, 5000, 1.2)
61 ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']
59 ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']
57 ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']
55 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']
54 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']
52 ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']
43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']
'0000100100000001001000000010000100010000000100010000000'</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_anneal.py" type="text/x-python"><span class="caption-label">Example 3.6 (code_anneal.py)</span></a>: <span class="caption-label">Figure 3.6</span>: Non-Deterministic Search Using Simulated Annealing: begin searching
with phrase segmentations only; randomly perturb the zeros and ones
proportional to the &quot;temperature&quot;; with each iteration the temperature
is lowered and the perturbation of boundaries is reduced. As this
search algorithm is non-deterministic, you may see a slightly different
result.</td></tr></p>
</table></div>
<p>With enough data, it is possible to automatically segment text into words with a reasonable
degree of accuracy.  Such methods can be applied to tokenization for writing systems that
don't have any visual representation of word boundaries.</p>
</div>
</div>
<div class="section" id="formatting-from-lists-to-strings">
<span id="sec-formatting"></span><h2>3.9&nbsp;&nbsp;&nbsp;Formatting: From Lists to Strings</h2>
<p>Often we write a program to report a single data item, such as a particular element
in a corpus that meets some complicated criterion, or a single summary statistic
such as a word-count or the performance of a tagger.  More often, we write a program
to produce a structured result; for example, a tabulation of numbers or linguistic forms,
or a reformatting of the original data.  When the results to be presented are linguistic,
textual output is usually the most natural choice.  However, when the results are numerical,
it may be preferable to produce graphical output.  In this section you will learn about
a variety of ways to present program output.</p>
<div class="section" id="from-lists-to-strings">
<h3>From Lists to Strings</h3>
<p>The simplest kind of structured object we use for text processing is lists of words.
When we want to output these to a display or a file, we must convert
these lists into strings.  To do this in Python we use the <tt class="doctest"><span class="pre">join()</span></tt> method, and specify
the string to be used as the &quot;glue&quot;.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']
>>> ' '.join(silly)
'We called him Tortoise because he taught us .'
>>> ';'.join(silly)
'We;called;him;Tortoise;because;he;taught;us;.'
>>> ''.join(silly)
'WecalledhimTortoisebecausehetaughtus.'</td>
</tr></table></td></tr>
</table></div>
<p>So <tt class="doctest"><span class="pre">' '.join(silly)</span></tt> means: take all the items in <tt class="doctest"><span class="pre">silly</span></tt> and
concatenate them as one big string, using <tt class="doctest"><span class="pre">' '</span></tt> as a spacer between
the items.  I.e. <tt class="doctest"><span class="pre">join()</span></tt> is a method of the string that you want to
use as the glue.  (Many people find this notation for <tt class="doctest"><span class="pre">join()</span></tt> counter-intuitive.)
The <tt class="doctest"><span class="pre">join()</span></tt> method only works on a list of strings &#8212; what we have been calling a text
&#8212; a complex type that enjoys some privileges in Python.</p>
</div>
<div class="section" id="strings-and-formats">
<h3>Strings and Formats</h3>
<p>We have seen that there are two ways to display the contents of an object:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> word = 'cat'
>>> sentence = """hello
... world"""
>>> print(word)
cat
>>> print(sentence)
hello
world
>>> word
'cat'
>>> sentence
'hello\nworld'</td>
</tr></table></td></tr>
</table></div>
<p>The <tt class="doctest"><span class="pre">print</span></tt> command yields Python's attempt to produce the most human-readable form of an object.
The second method &#8212; naming the variable at a prompt &#8212; shows us a string
that can be used to recreate this object.  It is important to keep in mind that both of
these are just strings, displayed for the benefit of you, the user.  They do not give
us any clue as to the actual internal representation of the object.</p>
<p>There are many other useful ways to display an object as a string of
characters.  This may be for the benefit of a human reader, or because
we want to <a name="export_index_term" /><span class="termdef">export</span> our data to a particular file format for use
in an external program.</p>
<p>Formatted output typically contains a combination of variables and
pre-specified strings, e.g. given a frequency distribution <tt class="doctest"><span class="pre">fdist</span></tt>
we could do:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])
>>> for word in sorted(fdist):
...     print(word, '->', fdist[word], end='; ')
cat -> 3; dog -> 4; snake -> 1;</td>
</tr></table></td></tr>
</table></div>
<p>Print statements that contain alternating variables and constants can be difficult to read and
maintain.  Another solution is to use <a name="string_formatting_index_term" /><span class="termdef">string formatting</span>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> for word in sorted(fdist):
...    print('{}->{};'.format(word, fdist[word]), end=' ')
cat->3; dog->4; snake->1;</td>
</tr></table></td></tr>
</table></div>
<p>To understand what is going on here, let's test out the
format string on its own.  (By now this will be
your usual method of exploring new syntax.)</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> '{}->{};'.format ('cat', 3)
'cat->3;'</td>
</tr></table></td></tr>
</table></div>
<p>The curly brackets <tt class="doctest"><span class="pre">'{}'</span></tt>  mark the presence of a <a name="replacement_field_index_term" /><span class="termdef">replacement
field</span>: this acts as a
placeholder for the string values of objects that are
passed to the <tt class="doctest"><span class="pre">str.format()</span></tt> method. We can embed occurrences of <tt class="doctest"><span class="pre">'{}'</span></tt>
inside a string, then replacet them with strings by calling <tt class="doctest"><span class="pre">format()</span></tt> with
appropriate arguments.  A string containing
replacement fields is called a <a name="format_string_index_term" /><span class="termdef">format string</span>.</p>
<p>Let's unpack the above code further, in order to see this
behavior up close:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> '{}->'.format('cat')
'cat->'
>>> '{}'.format(3)
'3'
>>> 'I want a {} right now'.format('coffee')
'I want a coffee right now'</td>
</tr></table></td></tr>
</table></div>
<p>We can have any number of placeholders, but  the <tt class="doctest"><span class="pre">str.format</span></tt> method
must be called with exactly the same number of arguments.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> '{} wants a {} {}'.format ('Lee', 'sandwich', 'for lunch')
'Lee wants a sandwich for lunch'
>>> '{} wants a {} {}'.format ('sandwich', 'for lunch')
Traceback (most recent call last):
...
    '{} wants a {} {}'.format ('sandwich', 'for lunch')
IndexError: tuple index out of range</td>
</tr></table></td></tr>
</table></div>
<p>Arguments to <tt class="doctest"><span class="pre">format()</span></tt> are consumed left to right, and any
superfluous arguments are simply ignored.</p>
<div class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2262)</p>
Unexpected indentation.</div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> '{} wants a {}'.format ('Lee', 'sandwich', 'for lunch')
'Lee wants a sandwich'</td>
</tr></table></td></tr>
</table></div>
<p>The field name in a format string can start with a number, which
refers to a positional argument of <tt class="doctest"><span class="pre">format()</span></tt>. Something like
<tt class="doctest"><span class="pre">'from {} to {}'</span></tt>
is equivalent to  <tt class="doctest"><span class="pre">'from {0} to {1}'</span></tt>, but we can use the numbers to get
non-default orders:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> 'from {1} to {0}'.format('A', 'B')
'from B to A'</td>
</tr></table></td></tr>
</table></div>
<p>We can also provide the values for the placeholders indirectly. Here's
an example using a <tt class="doctest"><span class="pre">for</span></tt> loop:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> template = 'Lee wants a {} right now'
>>> menu = ['sandwich', 'spam fritter', 'pancake']
>>> for snack in menu:
...     print(template.format(snack))
...
Lee wants a sandwich right now
Lee wants a spam fritter right now
Lee wants a pancake right now</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="lining-things-up">
<h3>Lining Things Up</h3>
<!-- In case we don't know in advance how wide a displayed value should be,
the width value can be replaced with a star in the formatting string,
then specified using a variable width-variable_.
width = 6
'%-*s' % (width, 'dog') # [_width-variable]
 'dog   ' -->
<p>So far our format strings generated output of arbitrary width
on the page (or screen).  We can add padding to obtain output of a given
width by inserting into the brackets a colon <tt class="doctest"><span class="pre">':'</span></tt> followed by an integer. So <tt class="doctest"><span class="pre">{:6}</span></tt>
specifies that we want a string that is
padded to width 6.  It is right-justified by default for numbers <a class="reference internal" href="#right-justified"><span id="ref-right-justified"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>,
but we can precede the width specifier with a <tt class="doctest"><span class="pre">'<'</span></tt> alignment option to make numbers left-justified <a class="reference internal" href="#left-justified"><span id="ref-left-justified"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> '{:6}'.format(41) # [_right-justified]
'    41'
>>> '{:<6}' .format(41) # [_left-justified]
'41    '</td>
</tr></table></td></tr>
</table></div>
<p>Strings are left-justified by default, but can be right-justified with
the <tt class="doctest"><span class="pre">'>'</span></tt> alignment option.</p>
<div class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2310)</p>
Unexpected indentation.</div>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> '{:6}'.format('dog') # [_left-justified-str]
'dog   '
>>> '{:>6}'.format('dog') # [_right-justified-str]
 '   dog'</td>
</tr></table></td></tr>
</table></div>
<p>Other control characters can be used to specify the sign and precision
of floating point numbers; for example <tt class="doctest"><span class="pre">{:.4f}</span></tt> indicates that four
digits should be displayed after the decimal point for a floating
point number.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> import math
>>> '{:.4f}'.format(math.pi)
'3.1416'</td>
</tr></table></td></tr>
</table></div>
<p>The string formatting is smart enough to know that if you include a
<tt class="doctest"><span class="pre">'%'</span></tt> in your format specification, then you want to represent the
value as a percentage; there's no need to multiply by 100.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> count, total = 3205, 9375
>>> "accuracy for {} words: {:.4%}".format(total, count / total)
'accuracy for 9375 words: 34.1867%'</td>
</tr></table></td></tr>
</table></div>
<p>An important use of formatting strings is for tabulating data.
Recall that in <a href="#id40"><span class="problematic" id="id41">sec-extracting-text-from-corpora_</span></a> we saw
data being tabulated from a conditional frequency distribution.
Let's perform the tabulation ourselves, exercising full control
of headings and column widths, as shown in <a class="reference internal" href="#code-modal-tabulate">3.7</a>.
Note the clear separation between the language processing work,
and the tabulation of results.</p>
<span class="target" id="code-modal-tabulate"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">def tabulate(cfdist, words, categories):
    print('{:16}'.format('Category'), end=' ')                    # column headings
    for word in words:
        print('{:>6}'.format(word), end=' ')
    print()
    for category in categories:
        print('{:16}'.format(category), end=' ')                  # row heading
        for word in words:                                        # for each word
            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell
        print()                                                   # end the row

>>> from nltk.corpus import brown
>>> cfd = nltk.ConditionalFreqDist(
...           (genre, word)
...           for genre in brown.categories()
...           for word in brown.words(categories=genre))
>>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> tabulate(cfd, modals, genres)
Category            can  could    may  might   must   will
news                 93     86     66     38     50    389
religion             82     59     78     12     54     71
hobbies             268     58    131     22     83    264
science_fiction      16     49      4     12      8     16
romance              74    193     11     51     45     43
humor                16     30      8      8      9     13</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_modal_tabulate.py" type="text/x-python"><span class="caption-label">Example 3.7 (code_modal_tabulate.py)</span></a>: <span class="caption-label">Figure 3.7</span>: Frequency of Modals in Different Sections of the Brown Corpus</td></tr></p>
</table></div>
<p>Recall from the listing in <a class="reference internal" href="#code-stemmer-indexing">3.2</a> that we used a format string
<tt class="doctest"><span class="pre">'{:{width}}'</span></tt> and bound a value to the <tt class="doctest"><span class="pre">width</span></tt> parameter in
<tt class="doctest"><span class="pre">format()</span></tt>.  This allows us to specify the width of a field using a
variable.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> '{:{width}}' % ("Monty Python", width=15)
'Monty Python   '</td>
</tr></table></td></tr>
</table></div>
<p>We could use this to automatically customize the column to be
just wide enough to accommodate all the words, using
<tt class="doctest"><span class="pre">width = max(len(w) for w in words)</span></tt>.</p>
</div>
<div class="section" id="writing-results-to-a-file">
<h3>Writing Results to a File</h3>
<p>We have seen how to read text from files (<a class="reference internal" href="#sec-accessing-text">3.1</a>).
It is often useful to write output to files as well.  The following
code opens a file <tt class="doctest"><span class="pre">output.txt</span></tt> for writing, and saves the program
output to the file.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> output_file = open('output.txt', 'w')
>>> words = set(nltk.corpus.genesis.words('english-kjv.txt'))
>>> for word in sorted(words):
...     print(word, file=output_file)</td>
</tr></table></td></tr>
</table></div>
<p>When we write non-text data to a file we must convert it to a string first.
We can do this conversion using formatting strings, as we saw above.
Let's write the total number of words to our file:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> len(words)
2789
>>> str(len(words))
'2789'
>>> print(str(len(words)), file=output_file)</td>
</tr></table></td></tr>
</table></div>
<div class="admonition caution">
<p class="first admonition-title">Caution!</p>
<p class="last">You should avoid filenames that contain space characters like
<tt class="doctest"><span class="pre">output file.txt</span></tt>, or that are identical except for case
distinctions, e.g. <tt class="doctest"><span class="pre">Output.txt</span></tt> and <tt class="doctest"><span class="pre">output.TXT</span></tt>.</p>
</div>
</div>
<div class="section" id="text-wrapping">
<h3>Text Wrapping</h3>
<p>When the output of our program is text-like, instead of tabular,
it will usually be necessary to wrap it so that it can be displayed
conveniently.  Consider the following output, which overflows its line,
and which uses a complicated <tt class="doctest"><span class="pre">print</span></tt> statement:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',
...           'more', 'is', 'said', 'than', 'done', '.']
>>> for word in saying:
...     print(word, '(' + str(len(word)) + '),', end=' ')
After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1),</td>
</tr></table></td></tr>
</table></div>
<p>We can take care of line wrapping with the help of Python's <tt class="doctest"><span class="pre">textwrap</span></tt> module.
For maximum clarity we will separate each step onto its own line:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from textwrap import fill
>>> format = '%s (%d),'
>>> pieces = [format % (word, len(word)) for word in saying]
>>> output = ' '.join(pieces)
>>> wrapped = fill(output)
>>> print(wrapped)
After (5), all (3), is (2), said (4), and (3), done (4), , (1), more
(4), is (2), said (4), than (4), done (4), . (1),</td>
</tr></table></td></tr>
</table></div>
<p>Notice that there is a linebreak between <tt class="doctest"><span class="pre">more</span></tt> and its following number.
If we wanted to avoid this, we could
redefine the formatting string so that it contained no spaces,
e.g. <tt class="doctest"><span class="pre">'%s_(%d),'</span></tt>, then instead of printing the value of <tt class="doctest"><span class="pre">wrapped</span></tt>,
we could print <tt class="doctest"><span class="pre">wrapped.replace('_', ' ')</span></tt>.</p>
</div>
</div>
<div class="section" id="summary">
<h2>3.10&nbsp;&nbsp;&nbsp;Summary</h2>
<ul class="simple">
<li>In this book we view a text as a list of words.  A &quot;raw text&quot; is a potentially
long string containing words and whitespace formatting, and is how we
typically store and visualize a text.</li>
<li>A string is specified in Python using single or double quotes: <tt class="doctest"><span class="pre">'Monty Python'</span></tt>, <tt class="doctest"><span class="pre">"Monty Python"</span></tt>.</li>
<li>The characters of a string are accessed using indexes, counting from zero:
<tt class="doctest"><span class="pre">'Monty Python'[0]</span></tt> gives the value <tt class="doctest"><span class="pre">M</span></tt>.  The length of a string is
found using <tt class="doctest"><span class="pre">len()</span></tt>.</li>
<li>Substrings are accessed using slice notation: <tt class="doctest"><span class="pre">'Monty Python'[1:5]</span></tt>
gives the value <tt class="doctest"><span class="pre">onty</span></tt>.  If the start index is omitted, the
substring begins at the start of the string; if the end index is omitted,
the slice continues to the end of the string.</li>
<li>Strings can be split into lists: <tt class="doctest"><span class="pre">'Monty Python'.split()</span></tt> gives
<tt class="doctest"><span class="pre">['Monty', 'Python']</span></tt>.  Lists can be joined into strings:
<tt class="doctest"><span class="pre">'/'.join(['Monty', 'Python'])</span></tt> gives <tt class="doctest"><span class="pre">'Monty/Python'</span></tt>.</li>
<li>We can read text from a file <tt class="doctest"><span class="pre">input.txt</span></tt> using <tt class="doctest"><span class="pre">text = open('input.txt').read()</span></tt>.
We can read text from <tt class="doctest"><span class="pre">url</span></tt> using <tt class="doctest"><span class="pre">text = request.urlopen(url).read().decode('utf8')</span></tt>.
We can iterate over the lines of a text file using <tt class="doctest"><span class="pre">for line in open(f)</span></tt>.</li>
<li>We can write text to a file by opening the file for writing
<tt class="doctest"><span class="pre">output_file = open('output.txt', 'w')</span></tt>, then adding content to the
file <tt class="doctest"><span class="pre">print("Monty Python", file=output_file)</span></tt>.</li>
<li>Texts found on the web may contain unwanted material (such as headers, footers, markup),
that need to be removed before we do any linguistic processing.</li>
<li>Tokenization is the segmentation of a text into basic units &#8212; or tokens &#8212;
such as words and punctuation.
Tokenization based on whitespace is inadequate for many applications because it
bundles punctuation together with words.
NLTK provides an off-the-shelf tokenizer <tt class="doctest"><span class="pre">nltk.word_tokenize()</span></tt>.</li>
<li>Lemmatization is a process that maps the various forms of a word (such as <span class="example">appeared</span>, <span class="example">appears</span>)
to the canonical or citation form of the word, also known as the lexeme or lemma (e.g. <span class="lex">appear</span>).</li>
<li>Regular expressions are a powerful and flexible method of specifying
patterns. Once we have imported the <tt class="doctest"><span class="pre">re</span></tt> module, we can use
<tt class="doctest"><span class="pre">re.findall()</span></tt> to find all substrings in a string that match a pattern.</li>
<li>If a regular expression string includes a backslash, you should tell Python not to
preprocess the string, by using a raw string with an <tt class="doctest"><span class="pre">r</span></tt> prefix: <tt class="doctest"><span class="pre">r'regexp'</span></tt>.</li>
<li>When backslash is used before certain characters, e.g. <tt class="doctest"><span class="pre">\n</span></tt>, this takes on
a special meaning (newline character); however, when backslash is used
before regular expression wildcards and operators, e.g. <tt class="doctest"><span class="pre">\.</span></tt>, <tt class="doctest"><span class="pre">\|</span></tt>, <tt class="doctest"><span class="pre">\$</span></tt>,
these characters <span class="emphasis">lose</span> their special meaning and are matched literally.</li>
<li>A string formatting expression <tt class="doctest"><span class="pre">template % arg_tuple</span></tt> consists of a
format string <tt class="doctest"><span class="pre">template</span></tt> that contains conversion specifiers
like <tt class="doctest"><span class="pre">%-6s</span></tt> and <tt class="doctest"><span class="pre">%0.2d</span></tt>.</li>
</ul>
</div>
<div class="section" id="further-reading">
<h2>3.11&nbsp;&nbsp;&nbsp;Further Reading</h2>
<p>Extra materials for this chapter are posted at <tt class="doctest"><span class="pre">http://nltk.org/</span></tt>, including links to freely
available resources on the web.  Remember to consult the Python reference materials
at <tt class="doctest"><span class="pre">http://docs.python.org/</span></tt>.  (For example, this documentation covers &quot;universal newline support,&quot;
explaining how to work with the different newline conventions used by various
operating systems.)</p>
<p>For more examples of processing words with NLTK, see the
tokenization, stemming and corpus HOWTOs at <tt class="doctest"><span class="pre">http://nltk.org/howto</span></tt>.
Chapters 2 and 3 of <a href="#id42"><span class="problematic" id="id3">[JurafskyMartin2008]_</span></a> contain more advanced
material on regular expressions and morphology.  For more extensive
discussion of text processing with Python see <a href="#id43"><span class="problematic" id="id4">[Mertz2003TPP]_</span></a>.
For information about normalizing non-standard words see <a href="#id44"><span class="problematic" id="id5">[Sproat2001NOR]_</span></a></p>
<p>There are many references for regular expressions, both practical and
theoretical.  For an introductory
tutorial to using regular expressions in Python,
see Kuchling's <em>Regular Expression HOWTO</em>,
<tt class="doctest"><span class="pre">http://www.amk.ca/python/howto/regex/</span></tt>.
For a comprehensive and detailed manual
in using regular expressions, covering their syntax in most major
programming languages, including Python, see <a href="#id45"><span class="problematic" id="id6">[Friedl2002MRE]_</span></a>.
Other presentations include Section 2.1 of <a href="#id46"><span class="problematic" id="id7">[JurafskyMartin2008]_</span></a>,
and Chapter 3 of <a href="#id47"><span class="problematic" id="id8">[Mertz2003TPP]_</span></a>.</p>
<p>There are many online resources for Unicode.  Useful discussions
of Python's facilities for handling Unicode are:</p>
<ul class="simple">
<li>Ned Batchelder, <em>Pragmatic Unicode</em>, <tt class="doctest"><span class="pre">http://nedbatchelder.com/text/unipain.html</span></tt></li>
<li><em>Unicode HOWTO</em>, Python Documentation,
<tt class="doctest"><span class="pre">http://docs.python.org/3/howto/unicode.html</span></tt></li>
<li>David Beazley, Mastering Python 3 I/O,
<tt class="doctest"><span class="pre">http://pyvideo.org/video/289/pycon-2010--mastering-python-3-i-o</span></tt></li>
<li>Joel Spolsky, <em>The Absolute Minimum Every Software Developer
Absolutely, Positively Must Know About Unicode and Character Sets
(No Excuses!)</em>, <tt class="doctest"><span class="pre">http://www.joelonsoftware.com/articles/Unicode.html</span></tt></li>
</ul>
<p>The problem of tokenizing Chinese text is a major focus of SIGHAN,
the ACL Special Interest Group on Chinese Language Processing
<tt class="doctest"><span class="pre">http://sighan.org/</span></tt>.  Our method for segmenting English text
follows <a href="#id48"><span class="problematic" id="id9">[Brent1995]_</span></a>; this work falls in the area of
language acquisition <a href="#id49"><span class="problematic" id="id10">[Niyogi2006]_</span></a>.</p>
<p>Collocations are a special case of multiword expressions.
A <a name="multiword_expression_index_term" /><span class="termdef">multiword expression</span> is a small phrase whose meaning
and other properties cannot be predicted from its words alone,
e.g. <span class="example">part of speech</span> <a href="#id50"><span class="problematic" id="id11">[BaldwinKim2010]_</span></a>.</p>
<p>Simulated annealing is a heuristic for finding
a good approximation to the optimum value of
a function in a large, discrete search space,
based on an analogy with annealing in metallurgy.
The technique is described in many Artificial Intelligence texts.</p>
<p>The approach to discovering hyponyms in text using search
patterns like <span class="example">x and other ys</span> is described by <a href="#id51"><span class="problematic" id="id12">[Hearst1992HYP]_</span></a>.</p>
</div>
<div class="section" id="exercises">
<h2>3.12&nbsp;&nbsp;&nbsp;Exercises</h2>
<ol class="arabic">
<li><p class="first">&#9788; Define a string <tt class="doctest"><span class="pre">s = 'colorless'</span></tt>.  Write a Python statement
that changes this to &quot;colourless&quot; using only the slice and
concatenation operations.</p>
</li>
<li><p class="first">&#9788; We can use the slice notation to remove morphological endings on
words.  For example, <tt class="doctest"><span class="pre">'dogs'[:-1]</span></tt> removes the last character of
<tt class="doctest"><span class="pre">dogs</span></tt>, leaving <tt class="doctest"><span class="pre">dog</span></tt>.  Use slice notation to remove the
affixes from these words (we've inserted a hyphen to
indicate the affix boundary, but omit this from your strings):
<tt class="doctest"><span class="pre">dish-es</span></tt>, <tt class="doctest"><span class="pre">run-ning</span></tt>, <tt class="doctest"><span class="pre">nation-ality</span></tt>, <tt class="doctest"><span class="pre">un-do</span></tt>,
<tt class="doctest"><span class="pre">pre-heat</span></tt>.</p>
</li>
<li><p class="first">&#9788; We saw how we can generate an <tt class="doctest"><span class="pre">IndexError</span></tt> by indexing beyond the end
of a string.  Is it possible to construct an index that goes too far to
the left, before the start of the string?</p>
</li>
<li><p class="first">&#9788; We can specify a &quot;step&quot; size for the slice. The following
returns every second character within the slice: <tt class="doctest"><span class="pre">monty[6:11:2]</span></tt>.
It also works in the reverse direction: <tt class="doctest"><span class="pre">monty[10:5:-2]</span></tt>
Try these for yourself, then experiment with different step values.</p>
</li>
<li><p class="first">&#9788; What happens if you ask the interpreter to evaluate <tt class="doctest"><span class="pre">monty[::-1]</span></tt>?
Explain why this is a reasonable result.</p>
</li>
<li><p class="first">&#9788; Describe the class of strings matched by the following regular
expressions.</p>
<ol class="loweralpha simple">
<li><tt class="doctest"><span class="pre">[a-zA-Z]+</span></tt></li>
<li><tt class="doctest"><span class="pre">[A-Z][a-z]*</span></tt></li>
<li><tt class="doctest"><span class="pre">p[aeiou]{,2}t</span></tt></li>
<li><tt class="doctest"><span class="pre">\d+(\.\d+)?</span></tt></li>
<li><tt class="doctest"><span class="pre">([^aeiou][aeiou][^aeiou])*</span></tt></li>
<li><tt class="doctest"><span class="pre">\w+|[^\w\s]+</span></tt></li>
</ol>
<p>Test your answers using <tt class="doctest"><span class="pre">nltk.re_show()</span></tt>.</p>
</li>
<li><p class="first">&#9788; Write regular expressions to match the following classes of strings:</p>
<blockquote>
<ol class="loweralpha simple">
<li>A single determiner (assume that <span class="example">a</span>, <span class="example">an</span>, and <span class="example">the</span>
are the only determiners).</li>
<li>An arithmetic expression using integers, addition, and
multiplication, such as <tt class="doctest"><span class="pre">2*3+8</span></tt>.</li>
</ol>
</blockquote>
</li>
<li><p class="first">&#9788; Write a utility function that takes a URL as its argument, and returns
the contents of the URL, with all HTML markup removed.  Use <tt class="doctest"><span class="pre">from urllib import request</span></tt>
and then <tt class="doctest"><span class="pre">request.urlopen('http://nltk.org/').read().decode('utf8')</span></tt> to access the contents of the URL.</p>
</li>
<li><p class="first">&#9788;
Save some text into a file <tt class="doctest"><span class="pre">corpus.txt</span></tt>.  Define a function <tt class="doctest"><span class="pre">load(f)</span></tt>
that reads from the file named in its sole argument, and returns a string
containing the text of the file.</p>
<ol class="loweralpha simple">
<li>Use <tt class="doctest"><span class="pre">nltk.regexp_tokenize()</span></tt> to create a tokenizer that tokenizes
the various kinds of punctuation in this text.  Use one multi-line
regular expression, with inline comments, using the verbose flag <tt class="doctest"><span class="pre">(?x)</span></tt>.</li>
<li>Use <tt class="doctest"><span class="pre">nltk.regexp_tokenize()</span></tt> to create a tokenizer that tokenizes
the following kinds of expression: monetary amounts; dates; names
of people and organizations.</li>
</ol>
</li>
<li><p class="first">&#9788; Rewrite the following loop as a list comprehension:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
>>> result = []
>>> for word in sent:
...     word_len = (word, len(word))
...     result.append(word_len)
>>> result
[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]</td>
</tr></table></td></tr>
</table></div>
</li>
<li><p class="first">&#9788; Define a string <tt class="doctest"><span class="pre">raw</span></tt> containing a sentence of your own choosing.
Now, split <tt class="doctest"><span class="pre">raw</span></tt> on some character other than space, such as <tt class="doctest"><span class="pre">'s'</span></tt>.</p>
</li>
<li><p class="first">&#9788; Write a <tt class="doctest"><span class="pre">for</span></tt> loop to print out the characters of a string, one per line.</p>
</li>
<li><p class="first">&#9788; What is the difference between calling <tt class="doctest"><span class="pre">split</span></tt> on a string
with no argument or with <tt class="doctest"><span class="pre">' '</span></tt> as the argument,
e.g. <tt class="doctest"><span class="pre">sent.split()</span></tt> versus <tt class="doctest"><span class="pre">sent.split(' ')</span></tt>?  What happens
when the string being split contains tab characters, consecutive
space characters, or a sequence of tabs and spaces?  (In IDLE you
will need to use <tt class="doctest"><span class="pre">'\t'</span></tt> to enter a tab character.)</p>
</li>
<li><p class="first">&#9788; Create a variable <tt class="doctest"><span class="pre">words</span></tt> containing a list of words.
Experiment with <tt class="doctest"><span class="pre">words.sort()</span></tt> and <tt class="doctest"><span class="pre">sorted(words)</span></tt>.
What is the difference?</p>
</li>
<li><p class="first">&#9788; Explore the difference between strings and integers by typing
the following at a Python prompt: <tt class="doctest"><span class="pre">"3" * 7</span></tt> and <tt class="doctest"><span class="pre">3 * 7</span></tt>.
Try converting between strings and integers using
<tt class="doctest"><span class="pre">int("3")</span></tt> and <tt class="doctest"><span class="pre">str(3)</span></tt>.</p>
</li>
<li><p class="first">&#9788; Use a text editor to create a file
called <tt class="doctest"><span class="pre">prog.py</span></tt> containing the single line <tt class="doctest"><span class="pre">monty = 'Monty Python'</span></tt>.
Next, start up a new session with the
Python interpreter, and enter the expression <tt class="doctest"><span class="pre">monty</span></tt> at the prompt.
You will get an error from the interpreter. Now, try the following
(note that you have to leave off the <tt class="doctest"><span class="pre">.py</span></tt> part of the filename):</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> from prog import monty
>>> monty</td>
</tr></table></td></tr>
</table></div>
<p>This time, Python should return with a value. You can also try
<tt class="doctest"><span class="pre">import prog</span></tt>, in which case Python should be able to
evaluate the expression <tt class="doctest"><span class="pre">prog.monty</span></tt> at the prompt.</p>
</li>
<li><p class="first">&#9788; What happens when the formatting strings <tt class="doctest"><span class="pre">%6s</span></tt> and <tt class="doctest"><span class="pre">%-6s</span></tt>
are used to display strings that are longer than six characters?</p>
</li>
<li><p class="first">&#9681; Read in some text from a corpus, tokenize it, and print the list of
all <span class="example">wh</span>-word types that occur.  (<span class="example">wh</span>-words in English
are used in questions, relative clauses and exclamations:
<span class="example">who</span>, <span class="example">which</span>, <span class="example">what</span>, and so on.) Print
them in order.  Are any words duplicated in this list, because of
the presence of case distinctions or punctuation?</p>
</li>
<li><p class="first">&#9681; Create a file consisting of words and (made up) frequencies, where each
line consists of a word, the space character, and a positive integer,
e.g. <tt class="doctest"><span class="pre">fuzzy 53</span></tt>.  Read the file into a Python list using <tt class="doctest"><span class="pre">open(filename).readlines()</span></tt>.
Next, break each line into its two fields using <tt class="doctest"><span class="pre">split()</span></tt>, and
convert the number into an integer using <tt class="doctest"><span class="pre">int()</span></tt>.  The result should
be a list of the form: <tt class="doctest"><span class="pre">[['fuzzy', 53], ...]</span></tt>.</p>
</li>
<li><p class="first">&#9681; Write code to access a favorite webpage and extract some text from it.
For example, access a weather site and extract the forecast top
temperature for your town or city today.</p>
</li>
<li><p class="first">&#9681; Write a function <tt class="doctest"><span class="pre">unknown()</span></tt> that takes a URL as its argument,
and returns a list of unknown words that occur on that webpage.
In order to do this, extract all substrings consisting of lowercase letters
(using <tt class="doctest"><span class="pre">re.findall()</span></tt>) and remove any items from this set that occur
in the Words Corpus (<tt class="doctest"><span class="pre">nltk.corpus.words</span></tt>).  Try to categorize these words
manually and discuss your findings.</p>
</li>
<li><p class="first">&#9681; Examine the results of processing the URL
<tt class="doctest"><span class="pre">http://news.bbc.co.uk/</span></tt> using the regular expressions suggested
above. You will see that there is still a fair amount of
non-textual data there, particularly Javascript commands. You may
also find that sentence breaks have not been properly
preserved. Define further regular expressions that improve the
extraction of text from this web page.</p>
</li>
<li><p class="first">&#9681; Are you able to write a regular expression to tokenize text in such
a way that the word <span class="example">don't</span> is tokenized into <span class="example">do</span> and <span class="example">n't</span>?
Explain why this regular expression won't work: &#171;<tt class="doctest"><span class="pre">n't|\w+</span></tt>&#187;.</p>
</li>
<li><p class="first">&#9681; Try to write code to convert text into <em>hAck3r</em>, using regular expressions
and substitution, where
<tt class="doctest"><span class="pre">e</span></tt> &#8594; <tt class="doctest"><span class="pre">3</span></tt>,
<tt class="doctest"><span class="pre">i</span></tt> &#8594; <tt class="doctest"><span class="pre">1</span></tt>,
<tt class="doctest"><span class="pre">o</span></tt> &#8594; <tt class="doctest"><span class="pre">0</span></tt>,
<tt class="doctest"><span class="pre">l</span></tt> &#8594; <tt class="doctest"><span class="pre">|</span></tt>,
<tt class="doctest"><span class="pre">s</span></tt> &#8594; <tt class="doctest"><span class="pre">5</span></tt>,
<tt class="doctest"><span class="pre">.</span></tt> &#8594; <tt class="doctest"><span class="pre">5w33t!</span></tt>,
<tt class="doctest"><span class="pre">ate</span></tt> &#8594; <tt class="doctest"><span class="pre">8</span></tt>.
Normalize the text to lowercase before converting it.
Add more substitutions of your own.  Now try to map
<tt class="doctest"><span class="pre">s</span></tt> to two different values: <tt class="doctest"><span class="pre">$</span></tt> for word-initial <tt class="doctest"><span class="pre">s</span></tt>,
and <tt class="doctest"><span class="pre">5</span></tt> for word-internal <tt class="doctest"><span class="pre">s</span></tt>.</p>
</li>
<li><p class="first">&#9681; <em>Pig Latin</em> is a simple transformation of English text.  Each word of the
text is converted as follows: move any consonant (or consonant cluster)
that appears at the start of the word to the end,
then append <span class="example">ay</span>, e.g. <span class="example">string</span> &#8594; <span class="example">ingstray</span>,
<span class="example">idle</span> &#8594; <span class="example">idleay</span>.  <tt class="doctest"><span class="pre">http://en.wikipedia.org/wiki/Pig_Latin</span></tt></p>
<ol class="loweralpha simple">
<li>Write a function to convert a word to Pig Latin.</li>
<li>Write code that converts text, instead of individual words.</li>
<li>Extend it further to preserve capitalization, to keep <tt class="doctest"><span class="pre">qu</span></tt> together
(i.e. so that <tt class="doctest"><span class="pre">quiet</span></tt> becomes <tt class="doctest"><span class="pre">ietquay</span></tt>), and to detect when <tt class="doctest"><span class="pre">y</span></tt>
is used as a consonant (e.g. <tt class="doctest"><span class="pre">yellow</span></tt>) vs a vowel (e.g. <tt class="doctest"><span class="pre">style</span></tt>).</li>
</ol>
</li>
<li><p class="first">&#9681; Download some text from a language that has vowel harmony (e.g. Hungarian),
extract the vowel sequences of words, and create a vowel bigram table.</p>
</li>
<li><p class="first">&#9681; Python's <tt class="doctest"><span class="pre">random</span></tt> module includes a function <tt class="doctest"><span class="pre">choice()</span></tt> which
randomly chooses an item from a sequence, e.g. <tt class="doctest"><span class="pre">choice("aehh ")</span></tt> will
produce one of four possible characters, with the letter <tt class="doctest"><span class="pre">h</span></tt> being
twice as frequent as the others.  Write a generator expression
that produces a sequence of 500 randomly chosen letters drawn from the
string <tt class="doctest"><span class="pre">"aehh "</span></tt>, and put this
expression inside a call to the <tt class="doctest"><span class="pre">''.join()</span></tt> function, to concatenate
them into one long string.  You should get a result that looks like
uncontrolled sneezing or maniacal laughter: <tt class="doctest"><span class="pre">he  haha ee  heheeh eha</span></tt>.
Use <tt class="doctest"><span class="pre">split()</span></tt> and <tt class="doctest"><span class="pre">join()</span></tt> again to normalize the whitespace in
this string.</p>
</li>
<li><p class="first">&#9681; Consider the numeric expressions in the following sentence from
the MedLine Corpus: <span class="example">The corresponding free cortisol fractions in these
sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively.</span>
Should we say that the numeric expression <span class="example">4.53 +/- 0.15%</span> is three
words?  Or should we say that it's a single compound word?  Or should
we say that it is actually <em>nine</em> words, since it's read &quot;four point
five three, plus or minus zero point fifteen percent&quot;?  Or should we say that
it's not a &quot;real&quot; word at all, since it wouldn't appear in any dictionary?
Discuss these different possibilities.  Can you think of application domains
that motivate at least two of these answers?</p>
</li>
<li><p class="first">&#9681; Readability measures are used to score the reading difficulty of a
text, for the purposes of selecting texts of appropriate difficulty
for language learners.  Let us define
&#956;<sub>w</sub> to be the average number of letters per word, and
&#956;<sub>s</sub> to be the average number of words per sentence, in
a given text.  The Automated Readability Index (ARI) of the text
is defined to be:
<tt class="doctest"><span class="pre">4.71</span></tt> &#956;<sub>w</sub> <tt class="doctest"><span class="pre">+ 0.5</span></tt> &#956;<sub>s</sub> <tt class="doctest"><span class="pre">- 21.43</span></tt>.
Compute the ARI score for various sections of the Brown Corpus, including
section <tt class="doctest"><span class="pre">f</span></tt> (lore) and <tt class="doctest"><span class="pre">j</span></tt> (learned).  Make use of the fact that
<tt class="doctest"><span class="pre">nltk.corpus.brown.words()</span></tt> produces a sequence of words, while
<tt class="doctest"><span class="pre">nltk.corpus.brown.sents()</span></tt> produces a sequence of sentences.</p>
</li>
<li><p class="first">&#9681; Use the Porter Stemmer to normalize some tokenized text, calling
the stemmer on each word.  Do the same thing with the Lancaster Stemmer
and see if you observe any differences.</p>
</li>
<li><p class="first">&#9681; Define the variable <tt class="doctest"><span class="pre">saying</span></tt> to contain the list
<tt class="doctest"><span class="pre">['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',
'is', 'said', 'than', 'done', '.']</span></tt>.  Process this list using a <tt class="doctest"><span class="pre">for</span></tt> loop, and store the
length of each word in a new list <tt class="doctest"><span class="pre">lengths</span></tt>.  Hint: begin by assigning the
empty list to <tt class="doctest"><span class="pre">lengths</span></tt>, using <tt class="doctest"><span class="pre">lengths = []</span></tt>. Then each time
through the loop, use <tt class="doctest"><span class="pre">append()</span></tt> to add another length value to
the list. Now do the same thing using a list comprehension.</p>
</li>
<li><p class="first">&#9681; Define a variable <tt class="doctest"><span class="pre">silly</span></tt> to contain the string:
<tt class="doctest"><span class="pre">'newly formed bland ideas are inexpressible in an infuriating
way'</span></tt>.  (This happens to be the legitimate interpretation that
bilingual English-Spanish speakers can assign to Chomsky's
famous nonsense phrase, <span class="example">colorless green ideas sleep furiously</span>
according to Wikipedia).  Now write code to perform the following tasks:</p>
<ol class="loweralpha simple">
<li>Split <tt class="doctest"><span class="pre">silly</span></tt> into a list of strings, one per
word, using Python's <tt class="doctest"><span class="pre">split()</span></tt> operation, and save
this to a variable called <tt class="doctest"><span class="pre">bland</span></tt>.</li>
<li>Extract the second letter of each word in <tt class="doctest"><span class="pre">silly</span></tt> and join
them into a string, to get <tt class="doctest"><span class="pre">'eoldrnnnna'</span></tt>.</li>
<li>Combine the words in <tt class="doctest"><span class="pre">bland</span></tt> back into a single string, using <tt class="doctest"><span class="pre">join()</span></tt>.
Make sure the words in the resulting string are separated with
whitespace.</li>
<li>Print the words of <tt class="doctest"><span class="pre">silly</span></tt> in alphabetical order, one per line.</li>
</ol>
</li>
<li><p class="first">&#9681; The <tt class="doctest"><span class="pre">index()</span></tt> function can be used to look up items in sequences.
For example, <tt class="doctest"><span class="pre">'inexpressible'.index('e')</span></tt> tells us the index of the
first position of the letter <tt class="doctest"><span class="pre">e</span></tt>.</p>
<ol class="loweralpha simple">
<li>What happens when you look up a substring, e.g. <tt class="doctest"><span class="pre">'inexpressible'.index('re')</span></tt>?</li>
<li>Define a variable <tt class="doctest"><span class="pre">words</span></tt> containing a list of words.  Now use <tt class="doctest"><span class="pre">words.index()</span></tt>
to look up the position of an individual word.</li>
<li>Define a variable <tt class="doctest"><span class="pre">silly</span></tt> as in the exercise above.
Use the <tt class="doctest"><span class="pre">index()</span></tt> function in combination with list slicing to
build a list <tt class="doctest"><span class="pre">phrase</span></tt> consisting of all the words up to (but not
including) <tt class="doctest"><span class="pre">in</span></tt> in <tt class="doctest"><span class="pre">silly</span></tt>.</li>
</ol>
</li>
<li><p class="first">&#9681; Write code to convert nationality adjectives like <span class="example">Canadian</span> and
<span class="example">Australian</span> to their corresponding nouns <span class="example">Canada</span> and <span class="example">Australia</span>
(see <tt class="doctest"><span class="pre">http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names</span></tt>).</p>
</li>
<li><p class="first">&#9681; Read the LanguageLog post on phrases of the form <span class="example">as best as p can</span>
and <span class="example">as best p can</span>, where <span class="example">p</span> is a pronoun.   Investigate this
phenomenon with the help of a corpus and the <tt class="doctest"><span class="pre">findall()</span></tt> method
for searching tokenized text described in
<a class="reference internal" href="#sec-useful-applications-of-regular-expressions">3.5</a>.
<tt class="doctest"><span class="pre">http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html</span></tt></p>
</li>
<li><p class="first">&#9681; Study the <span class="emphasis">lolcat</span> version of the book of Genesis,
accessible as <tt class="doctest"><span class="pre">nltk.corpus.genesis.words('lolcat.txt')</span></tt>, and
the rules for converting text into <span class="emphasis">lolspeak</span> at
<tt class="doctest"><span class="pre">http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat</span></tt>.
Define regular expressions to convert English words into
corresponding lolspeak words.</p>
</li>
<li><p class="first">&#9681; Read about the <tt class="doctest"><span class="pre">re.sub()</span></tt> function for string substitution
using regular expressions, using <tt class="doctest"><span class="pre">help(re.sub)</span></tt> and by consulting
the further readings for this chapter.  Use <tt class="doctest"><span class="pre">re.sub</span></tt> in writing code
to remove HTML tags from an HTML file, and to normalize whitespace.</p>
</li>
<li><p class="first">&#9733; An interesting challenge for tokenization is words that have been
split across a line-break.  E.g. if <em>long-term</em> is split, then we
have the string <tt class="doctest"><span class="pre">long-\nterm</span></tt>.</p>
<ol class="loweralpha simple">
<li>Write a regular expression that identifies words that are
hyphenated at a line-break.  The expression will need to include the
<tt class="doctest"><span class="pre">\n</span></tt> character.</li>
<li>Use <tt class="doctest"><span class="pre">re.sub()</span></tt> to remove the <tt class="doctest"><span class="pre">\n</span></tt> character from these
words.</li>
<li>How might you identify words that should not remain hyphenated
once the newline is removed, e.g. <tt class="doctest"><span class="pre">'encyclo-\npedia'</span></tt>?x</li>
</ol>
</li>
<li><p class="first">&#9733; Read the Wikipedia entry on <em>Soundex</em>.  Implement this
algorithm in Python.</p>
</li>
<li><p class="first">&#9733; Obtain raw texts from two or more genres and compute their respective
reading difficulty scores as in the earlier exercise on reading difficulty.
E.g. compare ABC Rural News and ABC Science News (<tt class="doctest"><span class="pre">nltk.corpus.abc</span></tt>).
Use Punkt to perform sentence segmentation.</p>
</li>
<li><p class="first">&#9733; Rewrite the following nested loop as a nested list comprehension:</p>
<blockquote>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc">>>> words = ['attribution', 'confabulation', 'elocution',
...          'sequoia', 'tenacious', 'unidirectional']
>>> vsequences = set()
>>> for word in words:
...     vowels = []
...     for char in word:
...         if char in 'aeiou':
...             vowels.append(char)
...     vsequences.add(''.join(vowels))
>>> sorted(vsequences)
['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']</td>
</tr></table></td></tr>
</table></div>
<!-- sorted(set(''.join(c for c in word if c in 'aeiou') for word in words)) -->
</blockquote>
</li>
<li><p class="first">&#9733; Use WordNet to create a semantic index for a text collection.
Extend the concordance search program in <a class="reference internal" href="#code-stemmer-indexing">3.2</a>,
indexing each word using the offset of its first synset,
e.g. <tt class="doctest"><span class="pre">wn.synsets('dog')[0].offset</span></tt> (and optionally the
offset of some of its ancestors in the hypernym hierarchy).</p>
</li>
<li><p class="first">&#9733; With the help of a multilingual corpus such as the
Universal Declaration of Human Rights Corpus (<tt class="doctest"><span class="pre">nltk.corpus.udhr</span></tt>),
and NLTK's frequency distribution and rank correlation functionality
(<tt class="doctest"><span class="pre">nltk.FreqDist</span></tt>, <tt class="doctest"><span class="pre">nltk.spearman_correlation</span></tt>),
develop a system that guesses the language of a previously unseen text.
For simplicity, work with a single character encoding and just a few
languages.</p>
</li>
<li><p class="first">&#9733; Write a program that processes a text and discovers
cases where a word has been used with a novel sense.
For each word, compute the WordNet similarity
between all synsets of the word and all synsets of the
words in its context.  (Note that this is a crude
approach; doing it well is a difficult, open research problem.)</p>
</li>
<li><p class="first">&#9733; Read the article on normalization of non-standard words
<a href="#id52"><span class="problematic" id="id13">[Sproat2001NOR]_</span></a>, and implement a similar system for text normalization.</p>
</li>
</ol>
<!-- no longer works, need UserAgent spoofing
#. |soso| Define a function ``ghits()`` that takes a word as its argument and
   builds a Google query string of the form ``http://www.google.com/search?q=word``.
   Strip the |HTML| markup and normalize whitespace.  Search for a substring
   of the form ``Results 1 - 10 of about``, followed by some number
   `n`:math:,  and extract `n`:math:.
   Convert this to an integer and return it. -->
<!-- Footer to be used in all chapters -->
<div class="admonition admonition-about-this-document">
<p class="first admonition-title">About this document...</p>
<p>UPDATED FOR NLTK 3.0.
This is a chapter from <em>Natural Language Processing with Python</em>,
by <a class="reference external" href="http://estive.net/">Steven Bird</a>, <a class="reference external" href="http://homepages.inf.ed.ac.uk/ewan/">Ewan Klein</a> and <a class="reference external" href="http://ed.loper.org/">Edward Loper</a>,
Copyright &#169; 2014 the authors.
It is distributed with the <em>Natural Language Toolkit</em> [<tt class="doctest"><span class="pre">http://nltk.org/</span></tt>],
Version 3.0, under the terms of the
<em>Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License</em>
[<a class="reference external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/us/">http://creativecommons.org/licenses/by-nc-nd/3.0/us/</a>].</p>
<p class="last">This document was built on
Wed 20 Aug 2014 11:10:48 GET</p>
</div>
</div>
</div>
<div class="system-messages section">
<h1>Docutils System Messages</h1>
<div class="system-message" id="id14">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 106); <em><a href="#id15">backlink</a></em></p>
Unknown target name: &quot;chap-introduction&quot;.</div>
<div class="system-message" id="id16">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 127); <em><a href="#id17">backlink</a></em></p>
Unknown target name: &quot;chap-introduction&quot;.</div>
<div class="system-message" id="id18">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 441); <em><a href="#id19">backlink</a></em></p>
Unknown target name: &quot;chap-introduction&quot;.</div>
<div class="system-message" id="id20">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 651); <em><a href="#id21">backlink</a></em></p>
Unknown target name: &quot;sec-a-closer-look-at-python-texts-as-lists-of-words&quot;.</div>
<div class="system-message" id="id22">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 803); <em><a href="#id23">backlink</a></em></p>
Unknown target name: &quot;tab-word-tests&quot;.</div>
<div class="system-message" id="id24">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1111); <em><a href="#id25">backlink</a></em></p>
Unknown target name: &quot;tab-word-tests&quot;.</div>
<div class="system-message" id="id26">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1127); <em><a href="#id27">backlink</a></em></p>
Unknown target name: &quot;sec-lexical-resources&quot;.</div>
<div class="system-message" id="id28">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1556); <em><a href="#id29">backlink</a></em></p>
Unknown target name: &quot;sec-wordnet&quot;.</div>
<div class="system-message" id="id30">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1582); <em><a href="#id31">backlink</a></em></p>
Unknown target name: &quot;chap-data&quot;.</div>
<div class="system-message" id="id32">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1646); <em><a href="#id33">backlink</a></em></p>
Unknown target name: &quot;sec-sequences&quot;.</div>
<div class="system-message" id="id34">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1932); <em><a href="#id1">backlink</a></em></p>
Unknown target name: &quot;kissstrunk2006&quot;.</div>
<div class="system-message" id="id35">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1968); <em><a href="#id36">backlink</a></em></p>
Unknown target name: &quot;sec-further-examples-of-supervised-classification&quot;.</div>
<div class="system-message" id="id37">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 1994); <em><a href="#id38">backlink</a></em></p>
Unknown target name: &quot;chap-chunk&quot;.</div>
<div class="system-message" id="id39">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2039); <em><a href="#id2">backlink</a></em></p>
Unknown target name: &quot;brent1995&quot;.</div>
<div class="system-message" id="id40">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2332); <em><a href="#id41">backlink</a></em></p>
Unknown target name: &quot;sec-extracting-text-from-corpora&quot;.</div>
<div class="system-message" id="id42">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2499); <em><a href="#id3">backlink</a></em></p>
Unknown target name: &quot;jurafskymartin2008&quot;.</div>
<div class="system-message" id="id43">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2499); <em><a href="#id4">backlink</a></em></p>
Unknown target name: &quot;mertz2003tpp&quot;.</div>
<div class="system-message" id="id44">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2499); <em><a href="#id5">backlink</a></em></p>
Unknown target name: &quot;sproat2001nor&quot;.</div>
<div class="system-message" id="id45">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2506); <em><a href="#id6">backlink</a></em></p>
Unknown target name: &quot;friedl2002mre&quot;.</div>
<div class="system-message" id="id46">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2506); <em><a href="#id7">backlink</a></em></p>
Unknown target name: &quot;jurafskymartin2008&quot;.</div>
<div class="system-message" id="id47">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2506); <em><a href="#id8">backlink</a></em></p>
Unknown target name: &quot;mertz2003tpp&quot;.</div>
<div class="system-message" id="id48">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2531); <em><a href="#id9">backlink</a></em></p>
Unknown target name: &quot;brent1995&quot;.</div>
<div class="system-message" id="id49">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2531); <em><a href="#id10">backlink</a></em></p>
Unknown target name: &quot;niyogi2006&quot;.</div>
<div class="system-message" id="id50">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2537); <em><a href="#id11">backlink</a></em></p>
Unknown target name: &quot;baldwinkim2010&quot;.</div>
<div class="system-message" id="id51">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2548); <em><a href="#id12">backlink</a></em></p>
Unknown target name: &quot;hearst1992hyp&quot;.</div>
<div class="system-message" id="id52">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/home/user/.pr/nltk_book/book/ch03.rst</tt>, line 2890); <em><a href="#id13">backlink</a></em></p>
Unknown target name: &quot;sproat2001nor&quot;.</div>
</div>
</div>
</body>
</html>
